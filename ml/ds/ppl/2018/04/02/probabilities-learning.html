<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <title>Learning from probabilistic models | 
  sambaiga
</title>
  

  
  <meta name="description" content="
  
">
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Learning from probabilistic models | sambaiga</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Learning from probabilistic models" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The post introduce the principle of probabilistic modelling with focus on how to learn parameters of probabilistic model using maximum likehood, bayesian estimation and the maximum aposterior approximation." />
<meta property="og:description" content="The post introduce the principle of probabilistic modelling with focus on how to learn parameters of probabilistic model using maximum likehood, bayesian estimation and the maximum aposterior approximation." />
<link rel="canonical" href="https://sambaiga.github.io/ml/ds/ppl/2018/04/02/probabilities-learning.html" />
<meta property="og:url" content="https://sambaiga.github.io/ml/ds/ppl/2018/04/02/probabilities-learning.html" />
<meta property="og:site_name" content="sambaiga" />
<meta property="og:image" content="https://sambaiga.github.io/images/pmf.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-04-02T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"The post introduce the principle of probabilistic modelling with focus on how to learn parameters of probabilistic model using maximum likehood, bayesian estimation and the maximum aposterior approximation.","headline":"Learning from probabilistic models","dateModified":"2018-04-02T00:00:00-05:00","datePublished":"2018-04-02T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://sambaiga.github.io/ml/ds/ppl/2018/04/02/probabilities-learning.html"},"image":"https://sambaiga.github.io/images/pmf.png","url":"https://sambaiga.github.io/ml/ds/ppl/2018/04/02/probabilities-learning.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="https://sambaiga.github.io/ml/ds/ppl/2018/04/02/probabilities-learning.html"><link type="application/atom+xml" rel="alternate" href="https://sambaiga.github.io/feed.xml" title="sambaiga" />

  

  

  

  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Learning from probabilistic models | sambaiga</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Learning from probabilistic models" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The post introduce the principle of probabilistic modelling with focus on how to learn parameters of probabilistic model using maximum likehood, bayesian estimation and the maximum aposterior approximation." />
<meta property="og:description" content="The post introduce the principle of probabilistic modelling with focus on how to learn parameters of probabilistic model using maximum likehood, bayesian estimation and the maximum aposterior approximation." />
<link rel="canonical" href="https://sambaiga.github.io/ml/ds/ppl/2018/04/02/probabilities-learning.html" />
<meta property="og:url" content="https://sambaiga.github.io/ml/ds/ppl/2018/04/02/probabilities-learning.html" />
<meta property="og:site_name" content="sambaiga" />
<meta property="og:image" content="https://sambaiga.github.io/images/pmf.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-04-02T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"The post introduce the principle of probabilistic modelling with focus on how to learn parameters of probabilistic model using maximum likehood, bayesian estimation and the maximum aposterior approximation.","headline":"Learning from probabilistic models","dateModified":"2018-04-02T00:00:00-05:00","datePublished":"2018-04-02T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://sambaiga.github.io/ml/ds/ppl/2018/04/02/probabilities-learning.html"},"image":"https://sambaiga.github.io/images/pmf.png","url":"https://sambaiga.github.io/ml/ds/ppl/2018/04/02/probabilities-learning.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://sambaiga.github.io/feed.xml" title="sambaiga" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    // remove paragraph tags in rendered toc (happens from notebooks)
    var toctags = document.querySelectorAll(".toc-entry")
    toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">sambaiga</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/project/">Projects</a><a class="page-link" href="/resources.html">Resources</a><a class="page-link" href="/categories/">Tags</a><a class="page-link" href="/talks/">Talk</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <div class="wrapper">
    <h1 class="post-title p-name" itemprop="name headline">Learning from probabilistic models</h1><p class="page-description">The post introduce the principle of probabilistic modelling with focus on how to learn parameters of probabilistic model using maximum likehood, bayesian estimation and the maximum aposterior approximation.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2018-04-02T00:00:00-05:00" itemprop="datePublished">
        Apr 2, 2018
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#ML">ML</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#DS">DS</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#PPL">PPL</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/sambaiga/sambaiga/tree/master/_notebooks/2018-04-02-probabilities-learning.ipynb" role="button">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div><div class="px-2">
    <a href="https://colab.research.google.com/github/sambaiga/sambaiga/blob/master/_notebooks/2018-04-02-probabilities-learning.ipynb">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </div>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction">Introduction </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Like-hood-and-log-likehood-function">Like-hood and log-likehood function </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Maximum-Likelihood-Estimation">Maximum-Likelihood Estimation </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Limitation-of-MLE">Limitation of MLE </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Bayesian-Parameter-Estimation">Bayesian Parameter Estimation </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-04-02-probabilities-learning.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h2>
<p>Given some data $ \mathbf{x}=[x_1\ldots x_m]$  that come from some probability density function characterized by
an unknown parameter $ \theta$. How can we find $ \hat{\theta}$  that is the best estimator of $ \theta$. For example suppose we have flipped a particular coin $ 100$  times and landed head $ N_H = 55$  times and tails $ N_T = 45$  times. We are interested to know what is the probability that it will come-up head if we flip it again. In this case the behavior of the coin can be summerized with parameter  $ \theta$  the probability that a flip land head (H), which in this case is independent and identically distributed Bernoulli distribution. The key question is, how do we find parameter  $ \hat{\theta}$  of this distribution that fits the data. This is called parameter estimation, in which three approaches can be used:</p>
<ol>
<li>Maximum-Likehood estimation</li>
<li>Bayesian parameter estimation and </li>
<li>Maximum a-posterior approximation</li>
</ol>
<h3 id="Like-hood-and-log-likehood-function">
<a class="anchor" href="#Like-hood-and-log-likehood-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Like-hood and log-likehood function<a class="anchor-link" href="#Like-hood-and-log-likehood-function"> </a>
</h3>
<p>Before discussing the above learning approach, let firts define the <strong>like-hood function</strong> $L(\theta)$ which is the probability of the observed data as function of $\theta$ given as:</p>
$$
L(\theta) = P(x_1,\ldots x_m; \theta) = \prod_i^m P(x_i;\theta)
$$<p>The like-hood function indicates how likely each value of the parameter is to have generated the data. In the case of coin example above, the like-lihood is the probability of particular seqeuence of H and T generated:</p>
<p>$$
 L(\theta) = \theta ^{N_H}(1 - \theta ^{N_T})
 $$</p>
<p>We also define the <strong>log-likelihood function</strong> $\mathcal{L}(\theta)$ which is the log of the likelihood function $L(\theta)$.</p>
$$
\begin{aligned}
\mathcal{L}(\theta) &amp;= \log L(\theta) \\
 &amp; = \log \prod_i^m P(x_i;\theta) \\
  &amp; = \sum_i^M P(x_i;\theta)
\end{aligned}
$$<p>For the above coin example the log-likelihood is</p>
$$ 
\mathcal{L}(\theta)= N_H\log\theta + N_T\log(1-\theta)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Maximum-Likelihood-Estimation">
<a class="anchor" href="#Maximum-Likelihood-Estimation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Maximum-Likelihood Estimation<a class="anchor-link" href="#Maximum-Likelihood-Estimation"> </a>
</h2>
<p>The main objective of maximum likelihood estimation (MLE) is to determine the value of $\theta$ that is most likely to have generated the vector of observed data, $\mathbf{x}$ where $\theta$ is assumed to be  fixed point (point-estimation). MLE achieve this by finding the parameter that maximize the probability of the observed data. The parameter $\hat{\theta}$ is selected such that it maximize $  \mathcal{L}(\theta)$:</p>
<p>$
\hat{\theta}=\arg\max_{\theta} \mathcal{L}(\theta)
$</p>
<p>For the coin example the MLE is :</p>
$$
\begin{aligned}
\frac{\partial \mathcal{L}(\theta)}{\partial \theta} &amp; = \frac{\partial }{\partial \theta}(N_H\log\theta + N_T\log(1-\theta) \\
 &amp;= \frac{N_H}{\theta} - \frac{N_T}{1-\theta}
\end{aligned}
$$<p>Set $\frac{\partial \mathcal{L}(\theta)}{\partial \theta} = 0$  and  solve for $\theta$ we obtain the MLE:</p>
<p>$
\hat{\theta} =\frac{N_H}{N_H + N_T}
$</p>
<p>which is simply the fraction of flips that cameup head.</p>
<p>Now suppose we are observing power-meta data which can be modelled as gaussian ditribution with mean $$\mu$$ and standard deviation $\sigma$. We can use MLE to estimate $\hat{\mu}$ and $\hat{\sigma}$. The log-likehood for gausian distribution is given as</p>
$$
\begin{aligned}
\mathcal{L}(\theta) &amp;= \sum_{i=1}^M \log \left[ \frac{1}{\sqrt{2} \pi \sigma} \exp \frac{-(x_i - \mu)}{2\sigma ^2}\right] \\
 &amp; = -\frac{M}{2}\log 2\pi - M\log \sigma - \frac{1}{2\sigma^2} \sum_i^M (x_i - \mu)^2
\end{aligned}
$$<p>Let find $ \frac{\partial \mathcal{L}(\theta)}{\partial \mu} $ and $\frac{\partial \mathcal{L}(\theta)}{\partial \sigma} $ and set  equal to zero.</p>
$$
\begin{aligned}
\frac{\partial \mathcal{L}(\theta)}{\partial \mu} &amp;=  -\frac{1}{2\sigma^2} \sum_i^M \frac{\partial}{\partial \mu}(x_i - \mu)^2 \\
&amp; = \sum_i^M (x_i - \mu) = 0 \\
&amp;\Rightarrow \hat{\mu} = \frac{1}{M} \sum_{i=1}^M x_i
\end{aligned}
$$<p>which is the mean of the observed values. Similary:</p>
$$
\begin{aligned}
\frac{\partial \mathcal{L}(\theta)}{\partial \sigma} &amp;=  \frac{M}{\sigma} + \frac{1}{\sigma^3}\sum_i^M (x_i - \mu)^2 \\
&amp;\Rightarrow \hat{\sigma} = \sqrt{\frac{1}{M} \sum_{i=1}^M (x_i - \mu)^2}
\end{aligned}
$$<p>In the two examples above  we manged to obtain the exact maximum likelihood solution analytically. But this is not always the case, let’s consider how to compute the maximum likelihood estimate of the parameters of the gamma distribution, whose PDF is defined as:</p>
$$
P(x) = \frac{b^a}{\Gamma(a)}x^{x-1}\exp(-bx)
$$<p>where $\Gamma (a)$ is the gamma function which is the generalization of the factorial function to continous values given as:</p>
<p>$
\Gamma(t) = \int_0^{-\infty} x^{t-1}\exp(-x) \,dx
$</p>
<p>The model parameters for gamma distribution is $a$ and $b$ both of which are $\geq 0$. the log-likelihood is therefore:</p>
<p>$
\begin{aligned} \mathcal{L}( a, b) &amp; = \sum_{i=1}^M a\log b -\log \Gamma (a) + (a -1) \log x_i - bx_i \\
 &amp; = Ma\log b - M \log \Gamma (a) + (a - 1) \sum_{i=1}^M \log x_i - b \sum_{i=1}^M x_i
\end{aligned}
$</p>
<p>To get MLE we need employ gradient descent which consists of computing the derivatives:
$
\frac{\partial \mathcal{L}}{\partial a}
$ and $
\frac{\partial \mathcal{L}}{\partial b}
$ and then updating; $
a_{k+1}= a_k + \alpha \frac{\partial \mathcal{L}}{\partial a}
$
and 
$
b_{k+1}= b_k + \alpha \frac{\partial \mathcal{L}}{\partial b}
$</p>
<p>where $\alpha$ is the learning rate.</p>
<h3 id="Limitation-of-MLE">
<a class="anchor" href="#Limitation-of-MLE" aria-hidden="true"><span class="octicon octicon-link"></span></a>Limitation of MLE<a class="anchor-link" href="#Limitation-of-MLE"> </a>
</h3>
<p>Despite the fact that MLE is very powerful technique, it has a pitfall for little training data which can lead into seriously overfit. The most painful issue is when it assign a $0$ probability to items that were never seen in the training data but which still might actually happen. Take an example if we flipped  a coin twice and $N_H = 2$, the MLE of $\theta$, the probability of H would be $1$. This imply that we are considering it impossible for the coin to come up T. This problem is knowas <em>data sparsity</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bayesian-Parameter-Estimation">
<a class="anchor" href="#Bayesian-Parameter-Estimation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bayesian Parameter Estimation<a class="anchor-link" href="#Bayesian-Parameter-Estimation"> </a>
</h2>
<p>Unlike MLE which treat only the observation $\mathbf{x}$ as random variable and the parameter $\theta$ as a fixed point, the bayesian approach treat the parameter $\theta $ as random varibale as well with some known prior distribution. Let define the model for joint distribution $$p(\theta, \mathcal{D})$$ over parameter  $\theta$ and data $\mathcal{D}$. To further define this joint distribution we aslo need the following two distribution:</p>
<ul>
<li>
<p>A distribution of $P(\theta)$ knowas <strong>prior distribution</strong> which is the probability of paratemeter $\theta$ availabe beforehand, and before making any additional observations. It account for everything you believed about the parameter $\theta$ before observing the data. In practise choose prior that is computational convinient.</p>
</li>
<li>
<p>The <strong>likelihood</strong> $P(\mathcal{D}\mid \theta)$ which is the probability of data given the parameter like in maximum likelihood.</p>
</li>
</ul>
<p>With this two distributions, we can compute the posterior distribution and the posterior predictive distribution. The posterior distribution $P(\theta \mid \mathcal{D})$ which correspond to uncertainty about $\theta$ after observing the data given by:</p>
$$
\begin{aligned}
P(\theta  \mid  \mathcal{D}) &amp;= \frac{P(\theta)p(\mathcal{D}  \mid  \theta)}{P(\mathcal{D})} &amp;= \frac{P(\theta)P(\mathcal{D} \mid  \theta)}{ \displaystyle \int P(\theta ^ {\prime} ) P(\mathcal{D} \mid  \theta ^{\prime})}
\end{aligned} 
$$<p>The denominator is usually considered as a normalizing constant and thus the posterior distribution become:</p>
$$
P(\theta  \mid  \mathcal{D}) \propto P(\theta)P(\mathcal{D}  \mid \theta)
$$<p>On the other hand the posterior predictive distribution $P(\mathcal{D}^{\prime}\mid)\mathcal{D}$ is the distribution of future observation given past observation defined by:</p>
$$
P(\mathcal{D}^{\prime} \mid \mathcal{D} )= \int P(\theta\mid \mathcal{D}) P(\mathcal{D}^{\prime} \mid \theta)
$$<p>Generaly the Bayesian approach to parameter estimation works as follows:</p>
<ol>
<li>First we need to formulate our knowledge about a situation by defining a distribution model which expresses qualitative aspects of our knowledge about the situation and then specify a prior probability distribution which expresses our subjective
beliefs and subjective uncertainty about the unknown parameters, before seeing the data.</li>
<li>Gather data</li>
<li>Obtain posterior knowledge that updates our beliefs by computing the posterior probability distribution which estimates the unknown
parameters. </li>
</ol>
<p>Let apply the bayesian estimation to the coin example in which we have specified the likelihood equal to $\theta^{N_H}(1-\theta)^{N_T}$. We only required to specify the prior in which several approches can be used. One of the approach is relay upon lifetime experince of flipping coins in which most coins tend to be fair which implies $p(\theta) = 0.5$. We can also use various distribution to specify prior density but in practise a most useful distribution is the <strong>beta distribution</strong> parameterized by $a , b &gt; 0$ and defined as:</p>
$$
p(\theta; a, b) = \frac{\Gamma (a + b)}{\Gamma(a) \Gamma (b)} \theta ^{a-1}(1-\theta ^{b - 1})
$$<p>From the above eqution it is clear that the first term (with all $\Gamma$)is just a normalizing constant and thus we can rewrite the beta distribution as:</p>
$$
p(\theta; a, b) \propto \theta ^{a-1}(1-\theta) ^{b - 1}
$$<p>Note the beta distribution has the following properties</p>
<ul>
<li>It is centered around $\frac{a}{a + b}$ and it can be shown that if $\theta \sim \text{Beta}(a,b)$ then $\mathbb{E}(\theta)=\frac{a}{a + b}$.</li>
<li>It becomes more peaked for larger values of $a$ and $b$</li>
<li>It become normal distribution when $a = b = 1$</li>
</ul>
<p>Now let compute the posterior and posterior predictive distribution</p>
$$
\begin{aligned}
p(\theta | \mathcal{D}) &amp; \propto p(\theta)p(\mathcal{D} |\theta) \\
&amp; \propto \theta^{N_H}(1-\theta)^{N_T}\theta ^{a-1}(1-\theta) ^{b - 1} \\
&amp; = \theta ^{a-1+N_H}(1-\theta) ^{b - 1 + N_T}
\end{aligned}
$$
</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sambaiga/sambaiga"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/ml/ds/ppl/2018/04/02/probabilities-learning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer">

  <div class="wrapper">
<div class="wrapper">
     Copyright 2020 
    
    
    : sambaiga@gmail.com
  </div>

  </div>
</footer>



<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>
<script src="/assets/js/common.js"></script>

<script>
  $("script[type='math/tex']").replaceWith(function() {
      var tex = $(this).text();
      return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: false});
  });

  $("script[type='math/tex; mode=display']").replaceWith(function() {
      var tex = $(this).html();
      return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
  });
</script>


</body>

</html>
