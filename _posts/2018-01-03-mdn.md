---
title: "Mixture Density Networks."
description: This post review Mixture Density Networks and their applications in deep neural networks.
toc: false
comments: false
layout: post
show_tags: true
categories: [Machine learning]
image: images/post/deep-1.jpg
author: Anthony Faustine
---

## Introduction 
Deep Learning models are widely used in prediction problem which involves learning the mapping from a set of inputs variables $$\mathbf{x}=\{x_1, \ldots, x_d\}$$ to a set of output variables $$\mathbf{y}=\{y_1, \ldots,y_c\}$$.  In this setting, $$d$$ is the size of input features, and $$c$$ is the dimension of the output feature or target. In this case, usually the network is trained using minimization of the sum of squares errors or cross-entropy error function over a set of training data $$\{\mathbf{x}_{1:N},\mathbf{y}_{1:N}\}$$ of the form 

$$
\mathcal{L} = (\mathbf{y}-\hat\mathbf{y})^2 \text{ where } \hat\mathbf{y}_{1:c}=f(\mathbf{x}_{1:d}, \mathbf{w, b})
$$ 

With this approach it is explicitly assumed that there is a deterministic $$1-to-1$$ mapping between a given input variables $$\mathbf{x}=\{x_1, \ldots, x_d\}$$ and target variable $$\mathbf{y}=\{y_1, \ldots,y_c\}$$ without any uncertainty. As the result, the output of the network trained by this approach approximates the conditional mean of the output in the training data conditioned on the input vector. For classification problems with a well-chosen target coding scheme, these averages represent the posterior probability of class membership and thus be regarded as optimal. For a problem involving the prediction of a continuous variable, especially the conditional averages is not usually a good description of data and don't have power to the modal distribution of output with complex. One way to solve this problem is to model the complete conditional probability density instead. This is the approach used by Mixture Density Networks (MDN).


## Mixture Density Network
An MDN, as proposed by Bishop, is a flexible framework for modeling an arbitrary conditional probability distribution $$p(\mathbf{y}|\mathbf{x})$$ as a mixture of distributions. It combines a mixture model with DNN in which a DNN is used to parametrize a mixture model consisting of some predefined distributions. Considering gaussian distribution, DNN is used to map a set of input features $$ \mathbf{x}_{1:d} $$ to the parameters of a GMM i.e mixture weights $$\pi_k(\mathbf{x})$$, mean $$\mu _k(\mathbf{x})$$ and the covariance matrices $$\sigma_k^2(\mathbf{x})$$ which in turn gives a full probability density function of an output feature $$\mathbf{y}$$ conditioned on the input features. 

$$
p(\mathbf{y}|\mathbf{x})=\sum_{k=1}^M \pi_k(\mathbf{x}) \mathcal{N}(\mathbf{y}; \mu_k(\mathbf{x}), \sigma_k^2(\mathbf{x}))
$$

where $$M$$ is the number of components in the mixture and 

$$
\mathcal{N}(\mathbf{y}; \mu_k(\mathbf{x}), \sigma_k^2(\mathbf{x})) = \frac{1}{(2\sigma_k^2(\mathbf{x}))^{c/2}}\exp\left[\frac{||\mathbf{y}-\mu_k(\mathbf{x})||^2}{2\sigma_k^2(\mathbf{x})}\right]
$$

The mixture weights $$\pi_k(\mathbf{x})$$ represents the relative amounts by of each mixture components, which can be interpreted as the probabilities of the $$k-$$ components for a given observation $$\mathbf{x}$$.If we introduce a latent variable $$\mathbf{z}$$ with $$k$$ possible states, then $$\pi_k(\mathbf{x})$$ will represents the probability distribution of these states $$p(\mathbf{z})$$. Specifically, the MDN converts the input vector using DNN with an output layer $$\mathbf{z}$$ of linear units to obtain the output
 $$
 \hat{\mathbf{z}} = f(\mathbf{x}, \mathbf{\theta})
 $$

The total number of networks outputs i.e the dimension of $$\hat{\mathbf{z}} \text{ is } (c+2)\cdot M$$ compared to the usual $$c$$ outputs for a network used in the conventional manner. In order to guarantee that $$p(\mathbf{y}|\mathbf{x})$$ is a probability distribution, the outputs of the networks need to be constrained such that the variance should remain positive and the mixing coefficients lie between zero and one and sum to one. To achieve these constraints:
- The mean of the $$k-th$$ kernel is modeled directly as the network outputs: 

$$
\mu_{k}^i(\mathbf{x})=z_{k}^{\mu i} \text{ where } i = 1,\ldots, c
$$

- The variances of $$\sigma_k $ is represented by an exponential activation function of the corresponding network output.

$$
\sigma_k(\mathbf{x}) = \exp(z_k^{\sigma})
$$

- The mixing coefficient $$\pi _k(\mathbf{x})$$ is modeled as the softmax transformation of the corresponding output.

$$
\pi_k = \frac{\exp(z_k^{\pi})}{\sum_{j=1}^M \exp(z_j^{\pi})}
$$

## Training MDN
As the generative model, an MDN model can be trained using the backpropagation algorithm under the maximum likelihood criterion. Suppose $$\theta$$ is the vector of the trainable parameter, and we can redefine our model as a function of $$\mathbf{x}$$ parameterized by $$\theta$$

$$
p(\mathbf{y}|\mathbf{x}, \mathbf{\theta})=\sum_{k=1}^M \pi_k(\mathbf{x}, \mathbf{\theta}) \mathcal{N}(\mathbf{y}; \mu_k(\mathbf{x}, \mathbf{\theta}), \sigma_k^2(\mathbf{x}, \mathbf{\theta}))
$$

Considering a data set $$\mathcal{D}= \{ \mathbf{x}_{1:N},\mathbf{y}_{1:N}\}$$ 
we want to maximize 

$$
p(\mathbf{\theta}|\mathcal{D}) = p(\mathbf{\theta}|\mathbf{Y},\mathbf{X})
$$ 

By Bayes's theorem, this is equivalent to

$$
p(\mathbf{\theta}|\mathbf{Y},\mathbf{X})p(\mathbf{Y}) = p(\mathbf{Y},\mathbf{\theta} |\mathbf{X}) = p(\mathbf{Y}|\mathbf{X},\mathbf{\theta})p(\mathbf{\theta})
$$ 

which leads to

$$
p(\mathbf{\theta}|\mathbf{Y},\mathbf{X}) = \frac{p(\mathbf{Y}|\mathbf{X},\mathbf{\theta})p(\mathbf{\theta})}{p(\mathbf{Y})} \propto p(\mathbf{Y}|\mathbf{X},\mathbf{\theta})p(\mathbf{\theta})
$$
where 
$$
p(\mathbf{Y}|\mathbf{X},\mathbf{\theta})=\prod_{n=1}^N p(\mathbf{y}_n|\mathbf{x}_n, \mathbf{\theta})
$$ 
which is simply the product of the conditional densities for each pattern.

To define an error function, the standard approach is the maximum likelihood method, which requires maximisation of the log-likelihood function or, equivalently, minimisation of the negative logarithm of the likelihood. Therefore, the error function for the Mixture Density Network is:

$$
\begin{aligned}
E(\theta, \mathcal{D})&=-\log p(\mathbf{\theta}|\mathbf{Y},\mathbf{X})= -\log p(\mathbf{Y}|\mathbf{X},\mathbf{\theta})p(\mathbf{\theta})\\
&= -\left(\log \prod_{n=1}^N p(\mathbf{y}_n|\mathbf{x}_n, \mathbf{\theta}) + \log p(\mathbf{\theta})\right)\\
&=-\left(\sum_{n=1}^N \log \sum_{k=1}^M \pi_k(\mathbf{x}) \mathcal{N}(\mathbf{y}; \mu_k(\mathbf{x}), \sigma_k^2(\mathbf{x})) + \log p(\mathbf{\theta})\right)\\
\end{aligned}
$$

If we assume a non-informative prior of $$p(\mathbf{\theta})=1$$ the error function simplify to

$$
E(\theta, \mathcal{D}) = -\sum_{n=1}^N \log \sum_{k=1}^M \pi_k(\mathbf{x}) \mathcal{N}(\mathbf{y}; \mu_k(\mathbf{x}), \sigma_k^2(\mathbf{x}))
$$
