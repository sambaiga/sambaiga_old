<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Hidden markov model | sambaiga</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Hidden markov model" />
<meta name="author" content="Anthony Faustine" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/hmm.html" />
<meta property="og:url" content="https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/hmm.html" />
<meta property="og:site_name" content="sambaiga" />
<meta property="og:image" content="https://sambaiga.github.io/sambaiga/images/svi.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-02T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Anthony Faustine"},"headline":"Hidden markov model","dateModified":"2019-05-02T00:00:00-05:00","datePublished":"2019-05-02T00:00:00-05:00","@type":"BlogPosting","image":"https://sambaiga.github.io/sambaiga/images/svi.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/hmm.html"},"url":"https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/hmm.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/sambaiga/assets/css/style.css">
  <link rel="stylesheet" href="/sambaiga/assets/css/main.css"><link type="application/atom+xml" rel="alternate" href="https://sambaiga.github.io/sambaiga/feed.xml" title="sambaiga" /><link rel="shortcut icon" type="image/x-icon" href="/sambaiga/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Hidden markov model | sambaiga</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Hidden markov model" />
<meta name="author" content="Anthony Faustine" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/hmm.html" />
<meta property="og:url" content="https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/hmm.html" />
<meta property="og:site_name" content="sambaiga" />
<meta property="og:image" content="https://sambaiga.github.io/sambaiga/images/svi.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-02T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Anthony Faustine"},"headline":"Hidden markov model","dateModified":"2019-05-02T00:00:00-05:00","datePublished":"2019-05-02T00:00:00-05:00","@type":"BlogPosting","image":"https://sambaiga.github.io/sambaiga/images/svi.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/hmm.html"},"url":"https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/hmm.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://sambaiga.github.io/sambaiga/feed.xml" title="sambaiga" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    // remove paragraph tags in rendered toc (happens from notebooks)
    var toctags = document.querySelectorAll(".toc-entry")
    toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/sambaiga/">sambaiga</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/sambaiga/about/">About</a><a class="page-link" href="/sambaiga/resources/">Resources</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Hidden markov model</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-05-02T00:00:00-05:00" itemprop="datePublished">
        May 2, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/sambaiga/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/sambaiga/sambaiga/tree/master/_notebooks/2019-05-02-hmm.ipynb" role="button">
<img class="notebook-badge-image" src="/sambaiga/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div><div class="px-2">
    <a href="https://colab.research.google.com/github/sambaiga/sambaiga/blob/master/_notebooks/2019-05-02-hmm.ipynb">
        <img class="notebook-badge-image" src="/sambaiga/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Three-main-problems-in-HMMs">Three main problems in HMMs </a></li>
<li class="toc-entry toc-h2"><a href="#Solution-to-Problem-1">Solution to Problem 1 </a>
<ul>
<li class="toc-entry toc-h3"><a href="#The-forward-backward-Algorithm">The forward-backward Algorithm </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Forward-Algorithm">Forward-Algorithm </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Backward-Algorithm">Backward Algorithm </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-05-02-hmm.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>HMM is a Markov model whose states are not directly observed; instead, each state is characterized by a probability distribution function modeling the observation corresponding to that state. HMM has been extensively used in temporal pattern recognition such as speech, handwriting, gesture recognition, robotics, biological sequences, and recently in energy disaggregation. This tutorial will introduce the basic concept of HMM.</p>
<p>There are two variables in HMM: observed variables and hidden variables where the sequences of hidden variables form a Markov process, as shown in the figure below.</p>
<p><img src="/sambaiga/images/copied_from_nb/my_icons/hmm.png" alt=""></p>
<p>In the context of NILM, the hidden variables are used to model states(ON, OFF, standby etc.) of individual appliances, and the observed variables are used to model the electric usage. HMMs have been widely used in most of the recently proposed NILM approach because it represents well the individual appliance internal states which are not directly observed in the targeted energy consumption.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A typical HMM is characterised by the following:</p>
<ul>
<li>The finite set of hidden states $S $ (e.g ON, stand-by, OFF, etc.) of an appliance, $S = \{s_1, s_2....,s_N\} $. </li>
<li>The finite set of $M $ observable symbol $Y $ per states (power consumption) observed in each state, $Y = \{y_1, y_2....,y_M\} $. The observable symbol $Y $ can be discrete or a continuous set. </li>
<li>The transition matrix $\mathbf{A}=\{a_{ij},1\leq i,j \geq N\} $ represents the probability of moving from state $s_{t-1}=i $ to $s_t =j $such that: $a_{ij} = P(s_{t} =j \mid s_{t-1}=i) $, with $a_{ij} \leq 0 $ and where $s_t $ denotes the state occupied by the system at time $t $. The matrix $\mathbf{A} $ is $ N x N $.</li>
<li>The emission matrix $\mathbf{B} =\{b_j(k)\} $ representing the probability of emission of symbol $k\in  Y $ when system state is $s_t=j $ such that: $b_j(k) = p(y_t = k \mid s_t=j)$ . The matrix $\mathbf{B} $ is an $N x M $. The emission probability can be discrete or continous distribution. If the emission is descrete a multinomial distribution is used and multivariate Gaussian distribution is usually used for continous emission.</li>
<li>And the initial state probability distribution $\mathbf{\pi} = \{\pi_i \ldots \pi_N\} $ indicating the probability of each state of the hidden variable at $t = 1 $ such that, $\pi _i = P(q_1 = s_i), 1 \leq i \geq N $.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The complete HMM specification requires;</p>
<ol>
<li>Finite set of hidden states $N $ and observation symbols $M$</li>
<li>Length of observation seqences $T$ and</li>
<li>Specification of three probability measures $ \mathbf{A}, \mathbf{B}$ and $\mathbf{\pi} $
The set of all HMM model parameters is represented by $\mathbf{\lambda} =(\pi, A, B)$. Since $S$ is not observed, the likelihood functions $Y$ is given by the joint distribution of $Y$ and $S$ over all possible states. 
$$
P(Y \mid \lambda) = \sum P(Y, S \mid \lambda)
$$
where 
$$
P(Y,S \mid \lambda) = P(Y \mid S,\lambda)P(S \mid \lambda)
$$
Note that $y_t$ is independent and identically distributed given state sequence $S = \{s_1, \ldots s_N\}$. Also each state at time $t$ depend on the state at its previous time $t-1$. Then
$$
P(Y \mid S, \lambda) = \prod_{t=1}^T P(y_t \mid s_t)
$$
Similarly
$$
P(S \mid \lambda) = \pi _{s_1} \prod _{t=2}^T a_{ij}
$$
The joint probability is therefore:
$$
P(Y \mid \lambda) = \pi _{s_1}P(y_1 \mid s_1) \sum \prod_{t=2}^T a_{ij} P(y_t \mid s_t)
$$</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Three-main-problems-in-HMMs">
<a class="anchor" href="#Three-main-problems-in-HMMs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Three main problems in HMMs<a class="anchor-link" href="#Three-main-problems-in-HMMs"> </a>
</h2>
<p>When applying HMM to a real-world problem, three essential issues must be solved.</p>
<ol>
<li>Evaluation Problem: Given HMM parameters $\lambda$ and the observation sequence $Y = \{Y_1, Y_2....,Y_M\}$, find $P(Y \mid \lambda)$ the likelihood of the observation sequence $Y$ given the model $\lambda$. This problem gives a score on how well a given model matches a given observation and thus allows you to choose the model that best matches the observation.</li>
<li>Decoding Problem: Given HMM parameters $\lambda$ and the observation sequence $Y = \{Y_1, Y_2....,Y_M\}$, find an optimal state sequence $S = \{S_1, S_2....,S_N\}$ which best explain the observation.This problem attempts to cover the hidden part of the model.</li>
<li>Learning Problem: Given the observation sequence $Y = \{Y_1, Y_2....,Y_M\}$, find the model parameters $\lambda$ that maximize $P(Y \mid \lambda)$.This problem attempts to optimize the model parameters to describe the model.</li>
</ol>
<p>The first and the second problem can be solved by the dynamic programming algorithms known as the Viterbi algorithm and the Forward-Backward algorithm, respectively. The last one can be solved by an iterative Expectation-Maximization (EM) algorithm, known as the Baum-Welch algorithm. We will discuss the first and the second problem in this post.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Solution-to-Problem-1">
<a class="anchor" href="#Solution-to-Problem-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Solution to Problem 1<a class="anchor-link" href="#Solution-to-Problem-1"> </a>
</h2>
<p>A straight forward way to solve this problem is to find $P(Y \mid S, \lambda)$ for fixed state sequences $S = \{s_1,...s_T \}$ and then sum up over all possible states. This is generally infeasible since it requires about $2TN^T$ multiplications. However, the problem can be efficiently solved by using the forward algorithm as follows:</p>
<h3 id="The-forward-backward-Algorithm">
<a class="anchor" href="#The-forward-backward-Algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>The forward-backward Algorithm<a class="anchor-link" href="#The-forward-backward-Algorithm"> </a>
</h3>
<p>Let us define the <strong>forward variable</strong> 
$$
\alpha _t(i)=P(y_1,\ldots y_t, s_t=i \mid \lambda)
$$
the probability of the partial observation sequences $y_1 \ldots y_t$ up to time $t$ and the state $s_t =i$ at time $t$ given the model ${\lambda}$. We also define an emission probability given HMM state $i$ at time $t$ as $b_i(y_t)$.</p>
<h4 id="Forward-Algorithm">
<a class="anchor" href="#Forward-Algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Forward-Algorithm<a class="anchor-link" href="#Forward-Algorithm"> </a>
</h4>
<ul>
<li>
<p><strong>Initilization</strong></p>
<p>Let
  $$
  \begin{aligned}
  \alpha _1(i)&amp;=P(y_1, s_1=i \mid \lambda) \\
   &amp; = P(y_1 \mid s_1=i,\lambda)P(s_1=i \mid \lambda)\\
   &amp;= \pi _i b_i(y_1) \text{ for } 1\leq i \geq N
  \end{aligned}
  $$</p>
</li>
<li>
<p><strong>Induction</strong></p>
<p>For $t=2,3...T$ and $1\leq i \geq N$, compute:</p>
<p>\begin{align}
  \alpha _{t}(i) &amp; = P(y_1 \ldots y_t, s_t=i \mid \lambda)\\
   &amp;= \displaystyle \sum_{j=1}^{N} P(y_1 \ldots y_{t}, s_{t-1}=j,s_t=i \mid \lambda) \\
   &amp;= \displaystyle \sum_{j=1}^{N} P(y_t \mid s_t=i, y_1,\ldots y_{t-1}, s_{t-1}=j, \lambda) \\
   &amp; \times P(s_t=i \mid y_1 \ldots y_{t-1} \ldots , s_{t-1}=j, \lambda) \\
   &amp; \times P(y_1 \ldots y_{t-1}, s_{t-1}=j,\lambda) \\
   &amp; = P(y_t \mid s_t=i,\lambda)\displaystyle \sum_{j=1}^{N} P(s_t=i \mid s_{t-1}=j)\cdot P(y_1, \ldots y_{t-1}, s_{t-1}) \\
  &amp; = b_i(y_{t})\displaystyle \sum_{j=1}^{N} \alpha _{t-1}(i)a_{ij} 
  \end{align}</p>
</li>
<li>
<p><strong>Termination</strong></p>
<p>From $\alpha _t(i)=P(y_1,...y_t, s_t=i \mid \lambda)$, it cear that:
  $$ 
  P(Y \mid \lambda) = \displaystyle \sum_{i=1}^{N} P(y_1,\ldots y_T, s_T = i \mid \lambda) = \displaystyle \sum_{i=1}^{N}\alpha _T(i) 
  $$</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Backward-Algorithm">
<a class="anchor" href="#Backward-Algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Backward Algorithm<a class="anchor-link" href="#Backward-Algorithm"> </a>
</h3>
<p>This is the same as the forward algorithm discussed in the previous sectionexcept that it start at the end and works backward toward the beginning. We first define the <strong>backward variable</strong> $\beta_t(i)=P(y_{t+1},y_{t+2} \ldots y_{T} \mid s_t=i, {\lambda})$: probability of the partial observed sequence from $$t+1 $$ to the end at $$T$$ given state $$i$$ at time $t$ and the model $\lambda$.</p>
<p>Then $\beta_t(i)$ can be recursively computed as follows.</p>
<ul>
<li>
<strong>Initialization</strong>
Let $\beta_{T}(i)= 1$, for $1 \leq i\geq N$</li>
<li>
<strong>Induction</strong>
For $t =T-1, T-2,\ldots1$ for $1 \leq i\geq N$ and by using the sum and product rules, we can rewrite $\beta_t(j)$ as:
\begin{aligned}
\beta_t(i)&amp;=P(y_{t+1},\ldots y_{T} \mid s_t=j, {\lambda}) \\
&amp;= \displaystyle \sum_{i=1}^{N} P(y_{t+1} \ldots y_T, s_{t+1}=i \mid s_t=j, \lambda) \\
&amp; = \displaystyle \sum_{i=1}^{N} P(y_{t+1} \ldots y_T, s_{t+1}=i, s_t=j, \lambda)\cdot P(s_{t+1}=i \mid s_t=j) \\
&amp;= \displaystyle \sum_{i=1}^{N} P(y_{t+2} \ldots y_T, s_{t+1}=i, \lambda)\cdot P(y_{t+1} \mid s_{t + 1}=i, \lambda)\cdot P(s_{t+1}=i \mid s_t=j) \\
&amp; = \displaystyle \sum_{i=1}^{N} a_{ij}b_i(y_{t+1})\beta _{t+1}(i)
\end{aligned}</li>
<li>
<strong>Termination</strong>
\begin{aligned}
\beta_{0} &amp; = P(Y \mid \lambda) \\
&amp; = \displaystyle \sum_{i=1}^{N} P(y_1,\ldots y_T, s_1=i) \\
&amp;= \displaystyle \sum_{i=1}^{N} P(y_1,\ldots y_T \mid s_1=i)\cdot P(s_1=i) \\
&amp; = \displaystyle \sum_{i=1}^{N} P(y_1 \mid s_1=i)\cdot P(y_2,\ldots y_T \mid s_1=i)\cdot P(s_1=i) \\
&amp; = \displaystyle \sum_{i=1}^{N} \pi _i b_i(y_1)\beta _1(i)
\end{aligned}</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pyro</span>
<span class="kn">import</span> <span class="nn">pyro.distributions</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="n">num_categories</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_supervised_data</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_data</span> <span class="o">=</span> <span class="mi">80</span>

<span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>  <span class="c1">#initial probability </span>
<span class="n">transition_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="n">emission_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="n">transition_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="n">transition_prior</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">num_categories</span><span class="p">]))</span>
<span class="n">emission_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="n">emission_prior</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">num_categories</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">equilibrium</span><span class="p">(</span><span class="n">mc_matrix</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">mc_matrix</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">mc_matrix</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">inverse</span><span class="p">()</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>

<span class="n">start_prob</span> <span class="o">=</span> <span class="n">equilibrium</span><span class="p">(</span><span class="n">transition_prob</span><span class="p">)</span>

<span class="c1"># simulate data</span>
<span class="n">categories</span><span class="p">,</span> <span class="n">words</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_data</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">t</span> <span class="o">==</span> <span class="n">num_supervised_data</span><span class="p">:</span>
        <span class="n">category</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">start_prob</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">category</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">transition_prob</span><span class="p">[</span><span class="n">category</span><span class="p">])</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">emission_prob</span><span class="p">[</span><span class="n">category</span><span class="p">])</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">categories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">category</span><span class="p">)</span>
    <span class="n">words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="n">categories</span><span class="p">,</span> <span class="n">words</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">categories</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">states</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'Rainy'</span><span class="p">,</span> <span class="s1">'Sunny'</span><span class="p">)</span>
<span class="n">observations</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'walk'</span><span class="p">,</span> <span class="s1">'shop'</span><span class="p">,</span> <span class="s1">'clean'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">obs_seq</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">observations</span><span class="p">[</span><span class="n">y</span><span class="p">],</span> <span class="n">words</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
<span class="n">state_seq</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="n">y</span><span class="p">],</span> <span class="n">categories</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">transition_prob</span> <span class="o">=</span> <span class="n">pyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">"transition_prob"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="n">transition_prior</span><span class="p">))</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">transition_prob</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 2])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bob_says</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Bob says:"</span><span class="p">,</span> <span class="s2">", "</span><span class="p">,</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">observations</span><span class="p">[</span><span class="n">y</span><span class="p">],</span> <span class="n">bob_says</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Bob says: ,  ['walk', 'clean', 'shop', 'shop', 'clean', 'walk']
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">## Reference</span>

<span class="mf">1.</span> <span class="p">[[</span><span class="n">Cheng</span> <span class="n">Zhang</span><span class="p">,(</span><span class="mi">2017</span><span class="p">)]](</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">arxiv</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="nb">abs</span><span class="o">/</span><span class="mf">1711.05597</span><span class="p">):</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="sambaiga/sambaiga"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/sambaiga/jupyter/2019/05/02/hmm.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/sambaiga/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/sambaiga/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/sambaiga/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/sambaiga" title="sambaiga"><svg class="svg-icon grey"><use xlink:href="/sambaiga/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/sambaiga" title="sambaiga"><svg class="svg-icon grey"><use xlink:href="/sambaiga/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
