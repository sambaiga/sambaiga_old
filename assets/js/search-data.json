{
  
    
        "post0": {
            "title": "Vectorization and Distribution shapes in Pyro",
            "content": "Introduction . In the previous post we introduced pyro and its building blocks such as schotastic function, primitive sample and param primitive statement, model and guide. We also defined pyro model and use it to generate data, learn from data and predict future observations. . In this section, we will learn in details about inference in Pyro, how to use Pyro primitives and the effect handling library (pyro.poutine) to build custom tools for analysis. . Consider a previous poison regression model . import torch import pyro import pyro.distributions as dist from torch.distributions import constraints import matplotlib.pyplot as plt import seaborn as sns import numpy as np pyro.set_rng_seed(101) torch.manual_seed(101) %matplotlib inline . def model_(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) for t in range(len(y)): rate = torch.exp(intercept + slope * t) y[t] = pyro.sample(&quot;count_{}&quot;.format(t), dist.Poisson(rate), obs=y[t]) . Plate statement . From the given model above , pyro.param designate model parameters that we would like to optimize. Observations are denoted by the obs= keyword argument to pyro.sample. This specifies the likelihood function. Instead of log transforming the data, we use a LogNormal distribution. The observations are conditionally independent given the latent random variable slope and intercept. To explicitly mark this in Pyro, plate statement is used to construct conditionally independent sequences of variables. . with pyro.plate(&quot;name&quot;, size, subsample_size, device) as ind: # ...do conditionally independent stuff with ind... . However compared to range() each invocation of plate requires the user to provide a unique name. The plate statement can be used either sequentially as a generator or in parallel as a context manager. Sequential plate is similar to range()in that it generates a sequence of values. . # This version declares sequential independence and subsamples data: for i in plate(&#39;data&#39;, 100, subsample_size=10): if z[i]: # Control flow in this example prevents vectorization. obs = sample(&#39;obs_{}&#39;.format(i), dist.Normal(loc, scale), obs=data[i]) . Vectorized plate is similar to torch.arange() in that it yields an array of indices by which other tensors can be indexed. However, unlike torch.arange() plate also informs inference algorithms that the variables being indexed are conditionally independent. . # This version declares vectorized independence: with plate(&#39;data&#39;): obs = sample(&#39;obs&#39;, dist.Normal(loc, scale), obs=data) . Additionally, plate can take advantage of the conditional independence assumptions by subsampling the indices and informing inference algorithms to scale various computed values. This is typically used to subsample minibatches of data: . with plate(&quot;data&quot;, len(data), subsample_size=100) as ind: batch = data[ind] assert len(batch) == 100 . You can additionally nest plates, e.g. if you have per-pixel independence: . with pyro.plate(&quot;x_axis&quot;, 320): # within this context, batch dimension -1 is independent with pyro.plate(&quot;y_axis&quot;, 200): # within this context, batch dimensions -2 and -1 are independent . Finaly you can declare multiple plates and use them as reusable context managers. For example if you want to mix and match plates for e.g. noise that depends only on x, some noise that depends only on y, and some noise that depends on both . x_axis = pyro.plate(&quot;x_axis&quot;, 3, dim=-2) y_axis = pyro.plate(&quot;y_axis&quot;, 2, dim=-3) with x_axis: # within this context, batch dimension -2 is independent with y_axis: # within this context, batch dimension -3 is independent with x_axis, y_axis: # within this context, batch dimensions -3 and -2 are independent . def model_(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) with pyro.plate(&#39;N&#39;, len(y)) as t: log_y_hat = slope * t.type(torch.float) + intercept y=pyro.sample(&#39;y&#39;, dist.LogNormal(log_y_hat, 1.), obs=y) . Distribution shapes . Unlike PyTorch Tensors which have a single .shape attribute, pyro Distributions have two shape batch_shape and event_shape. These two combine to define the total shape of a sample. The batch_shape denote conditionally independent random variables, whereas .event_shape denote dependent random variables (ie one draw from a distribution). Because the dependent random variables define probability together, the .log_prob() method only produces a single number for each event of shape .event_shape. . d = dist.Bernoulli(0.5) print(d.batch_shape) print(d.event_shape) . torch.Size([]) torch.Size([]) . x = d.sample() x.shape . torch.Size([]) . Distributions can be batched by passing in batched parameters. . d = dist.Bernoulli(0.5*torch.ones(50)) print(d.batch_shape) print(d.event_shape) . torch.Size([50]) torch.Size([]) . x = d.sample() x.shape . torch.Size([50]) . From the two examples above, we observe that univariate distributions have empty event shape (because each number is an independent event). Let also consider multivariate distribution. . md = dist.MultivariateNormal(torch.zeros(3), torch.eye(3)) print(md.batch_shape) print(md.event_shape) . torch.Size([]) torch.Size([3]) . y = md.sample() y.shape . torch.Size([3]) . We can also create batched multivariate distribution as follows. . md = dist.MultivariateNormal(torch.zeros(3), torch.eye(3)).expand([50]) print(md.batch_shape) print(md.event_shape) . torch.Size([50]) torch.Size([3]) . y = md.sample() y.shape . torch.Size([50, 3]) . Because Multivariate distributions have nonempty .event_shape, the shapes of .sample() and .log_prob(x) differ: . md.log_prob(y).shape . torch.Size([50]) . The Distribution.sample() method also takes a sample_shape parameter that indexes over independent identically distributed (iid) random varables, such that: . sample.shape == sample_shape + batch_shape + event_shape . y_sample =md.sample([10]) y_sample.shape . torch.Size([10, 50, 3]) . Reshaping distributions . You can treat a univariate distribution as multivariate by calling the .to_event(n) property where n is the number of batch dimensions (from the right) to declare as dependent. . d = dist.Bernoulli(0.5*torch.ones(50, 3)).to_event(1) print(d.batch_shape) print(d.event_shape) . torch.Size([50]) torch.Size([3]) . While working with distributions in pyro it is essential to note that: . Samples have shape batch_shape + event_shape, | .log_prob(x) values have shape batch_shape. | You’ll need to ensure that batch_shape is carefully controlled by either trimming it down with .to_event(n) or by declaring dimensions as independent via pyro.plate. | Often in Pyro we’ll declare some dimensions as dependent even though they are in fact independent. This allows us to easily swap in a MultivariateNormal distribution later, but aslo it simplifies the code as we don’t need a plate. Consider the following two codes . x = pyro.sample(&quot;x&quot;, dist.Normal(0, 1).expand([10]).to_event(1)) . x.shape . torch.Size([10]) . with pyro.plate(&quot;y_plate&quot;, 10): y = pyro.sample(&quot;y&quot;, dist.Normal(0, 1)) # .expand([10]) is automatic . y.shape . torch.Size([10]) . From the two code examples, the second version with plate informs Pyro that it can make use of conditional independence information when estimating gradients, whereas in the first version Pyro must assume they are dependent (even though the normals are in fact conditionally independent). .",
            "url": "https://sambaiga.github.io/sambaiga/ppl/pyro/statistical%20inference/2020/03/04/ppl-pyro-two.html",
            "relUrl": "/ppl/pyro/statistical%20inference/2020/03/04/ppl-pyro-two.html",
            "date": " • Mar 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Probabilistic Programming with Pyro",
            "content": "Intro to Pyro . Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. It enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling. . Models and Probability distributions . Models are the basic unit of probabilistic programs in pyro, they represent simplified or abstract descriptions of a process by which data are generated. Models in pyro are expressed as stochastic functions which implies that models can be composed, reused, imported, and serialized just like regular Python callables. Probability distributions (pimitive stochastic functions) are important class of models (stochastic functions) used explicitly to compute the probability of the outputs given the inputs. Pyro uses PyTorch’s distribution library which contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. Each probability distributions are equipped with several methods such as: . prob(): $ log p( mathbf{x} mid theta ^{*})$ | mean: $ mathbb{E}_{p( mathbf{x} mid theta ^{*})}[ mathbf{x}]$ | sample: $ mathbf{x}^{*} sim {p( mathbf{x} mid theta ^{*})}$ | . You can also create custom distributions using transforms. . Example 1: Let define the unit normal distribution $ mathcal{N}(0,1)$, draw sample $x$ and compute the log probability according to the distribution. . import torch import pyro import pyro.distributions as dist from torch.distributions import constraints import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pyro.set_rng_seed(101) torch.manual_seed(101) torch.set_printoptions(precision=3) %matplotlib inline . mu = 0 sigma = 1 normal=dist.Normal(mu, sigma) x = normal.rsample() # draw a sample from N(1,1) print(&quot;sample&quot;, x.item()) #To compute the log probability according to the distribution print(&quot;prob&quot;, torch.exp(normal.log_prob(x)).item()) # score the sample from N(1,1) . sample -1.3905061483383179 prob 0.15172401070594788 . Sample and Param statements . Pyro simplifies the process of sampling from distributions with the use of pyro.sample statement. The pyro.sample statement call stochastic functions or models with a unique name as identifier. Pyro’s backend uses these names to uniquely identify sample statements and change their behavior at runtime depending on how the enclosing stochastic function is being used. Using pyro.sample statement, Pyro can implement various manipulations that underlie inference algorithms. . x = pyro.sample(&quot;name&quot;, fn, obs) &quot;&quot;&quot; name – name of sample fn – distribution class or function obs – observed datum (optional; should only be used in context of inference) optionally specified in kwargs &quot;&quot;&quot; . Example 2: Let sample from previous normal distribution created in example 1. . mu = 0 sigma = 1 x = pyro.sample(&quot;my_sample&quot;, dist.Normal(mu, sigma)) print(x) . tensor(-0.815) . The above code generate a random value and records it in the Pyro runtime. . data=2 x = pyro.sample(&quot;my_sample&quot;, dist.Normal(mu, sigma), obs=data) print(x) . 2 . /opt/miniconda3/lib/python3.7/site-packages/pyro/primitives.py:86: RuntimeWarning: trying to observe a value outside of inference at my_sample RuntimeWarning) . The above code conditions a stochatsic function on observed data. This should run on inference. . Pyro use pyro.param statement to saves the variable as a parameter in the param store. To interact with the param store. The pyro.param statement is used by pyro to declares a learnable parameter. . x = pyro.param(&quot;name&quot;, init_value, constraints) &quot;&quot;&quot; name – name of param init_value – initial value constraint – torch constraint &quot;&quot;&quot; . Example 3: Let create theta parameter . theta = pyro.param(&quot;theta&quot;, torch.tensor(1.0), constraint=dist.constraints.positive) . Simple PPL model . Consider the following Poison Regression model begin{align} y(t) &amp; sim lambda exp(- lambda) lambda &amp; sim exp(c + m(t)) c &amp; sim mathcal{N}(1, 1) m &amp; sim mathcal{N}(0, 1) end{align} . def model(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) for t in range(len(y)): rate = torch.exp(intercept + slope * t) y[t] = pyro.sample(&quot;count_{}&quot;.format(t), dist.Poisson(rate), obs=y[t]) return slope, intercept, y . Given a pyro model. We can . Generate data from model | Learn parameters of the model from data | Use the model to predict future observation. | Generate data from model . Running a Pyro model will generate a sample from the prior. . pyro.set_rng_seed(0) # We pass counts = [None, ..., None] to indicate time duration. true_slope, true_intercept, true_counts = model([None] * 50) fig, ax = plt.subplots(figsize=(6,4)) ax = sns.lineplot(x=np.arange(len(true_counts)),y=[c.item() for c in true_counts]) . Learn parameters of the model from data . To learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data. Inference algorithms in Pyro us arbitrary stochastic functions as approximate posterior distributions. that s. These functions are called guide functions or guides and contains pyro.sample and pyro.param statement. It is a stochastic function that represents a probability distribution over the latent (unobserved) variables. The guide can be arbitrary python code just like the model, but with a few requirements: . All unobserved sample statements that appear in the model appear in the guide. | The guide has the same input signature as the model (i.e. takes the same arguments). | There are no pyro.sample statements with the obs keyword in the guide. These are exclusive to the model. | There are pyro.param statements, which are exclusive to the guide. These provide differentiation for the inputs to the pay_probs sample in the guide vs. the model. | For example if the model contains a random variable z_1 . def model(): pyro.sample(&quot;z_1&quot;, ...) . then the guide needs to have a matching sample statement . def guide(): pyro.sample(&quot;z_1&quot;, ...) . Once a guide has been specified, we can then perform learning and inference which is an optimization problem of maximizing the evidence lower bound (ELBO). The ELBO, is a function of both $ theta$ and $ phi$, defined as an expectation w.r.t. to samples from the guide: . $${ rm ELBO} equiv mathbb{E}_{q_{ phi}({ bf z})} left [ log p_{ theta}({ bf x}, { bf z}) - log q_{ phi}({ bf z}) right]$$The SVI class is unified interface for stochastic variational inference in Pyro. To use this class you need to provide: . the model, | the guide, and an | optimizer which is a wrapper a for a PyTorch optimizer as discusseced in below | . from pyro.infer import SVI, Trace_ELBO svi = SVI(model, guide, optimizer, loss=Trace_ELBO()) . The SVI object provides two methods, step() and evaluate_loss(), . The method step() takes a single gradient step and returns an estimate of the loss (i.e. minus the ELBO). | The method evaluate_loss() returns an estimate of the loss without taking a gradient step. | . Both of these methods accept an optional argument: num_particles, which denotes the number of samples used to compute the loss and gradient. . The module pyro.optim provides support for optimization in Pyro. In particular it provides PyroOptim, which is used to wrap PyTorch optimizers and manage optimizers for dynamically generated parameters. PyroOptim takes two arguments: . a constructor for PyTorch optimizers optim_constructor and | a specification of the optimizer arguments optim_args | . from pyro.optim import Adam adam_params = {&quot;lr&quot;: 0.005, &quot;betas&quot;: (0.95, 0.999)} optimizer = Adam(adam_params) . Thus to learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data (here true_counts). . For the above example we will use Autoguide pyro inference algorithm: . AutoLaplaceApproximation:Laplace approximation (quadratic approximation) approximates the posterior log𝑝(𝑧|𝑥) by a multivariate normal distribution in the unconstrained space. | Autodelta: This implementation of AutoGuide uses Delta distributions to construct a MAP guide over the entire latent space. | . from pyro.infer.autoguide import AutoDelta from pyro.infer import SVI, Trace_ELBO from pyro.optim import Adam guide = AutoDelta(model) svi = SVI(model, guide, Adam({&quot;lr&quot;: 0.1}), Trace_ELBO()) for i in range(101): loss = svi.step(true_counts) # true_counts is passed as argument to model() if i % 10 == 0: print(&quot;loss = {}&quot;.format(loss)) . loss = 87295.88946688175 loss = 64525.8595520854 loss = 80838.8460238576 loss = 33014.93229973316 loss = 13704.865498423576 loss = 6232.828522503376 loss = 2017.9879159331322 loss = 631.3558134436607 loss = 170.10323333740234 loss = 198.57187271118164 loss = 207.4590385556221 . print(&quot;true_slope = {}&quot;.format(true_slope)) print(&quot;true_intercept = {}&quot;.format(true_intercept)) guess = guide() print(&quot;guess = {}&quot;.format(guess)) . true_slope = 0.15409961342811584 true_intercept = -0.293428897857666 guess = {&#39;slope&#39;: tensor(0.147, grad_fn=&lt;ExpandBackward&gt;), &#39;intercept&#39;: tensor(-0.054, grad_fn=&lt;ExpandBackward&gt;)} . Use model to predict future observation . A third way to use a Pyro model is to predict new observed data by guiding the model. This uses two of Pyro&#39;s effects: . trace records guesses made by the guide, and | replay conditions the model on those guesses, allowing the model to generate conditional samples. | . Traces are directed graphs whose nodes represent primitive calls or input/output, and whose edges represent conditional dependence relationships between those primitive calls. It return a handler that records the inputs and outputs of primitive calls and their dependencies. . We can record its execution using trace and use the resulting data structure to compute the log-joint probability of all of the sample sites in the execution or extract all parameters. . trace = pyro.poutine.trace(model).get_trace([]) pprint({ name: { &#39;value&#39;: props[&#39;value&#39;], &#39;prob&#39;: props[&#39;fn&#39;].log_prob(props[&#39;value&#39;]).exp() } for (name, props) in trace.nodes.items() if props[&#39;type&#39;] == &#39;sample&#39; }) . {&#39;intercept&#39;: {&#39;prob&#39;: tensor(0.250), &#39;value&#39;: tensor(-0.966)}, &#39;slope&#39;: {&#39;prob&#39;: tensor(2.818), &#39;value&#39;: tensor(0.083)}} . print(trace.log_prob_sum().exp()) . tensor(0.705) . Here, the trace feature will collect values every time they are sampled with sample and store them with the corresponding string name (that’s why we give each sample a name). With a little cleanup, we can print out the value and probability of each random variable’s value, along with the joint probability of the entire trace. . Replay return a callable that runs the original, reusing the values at sites in trace at those sites in the new trace. makes sample statements behave as if they had sampled the values at the corresponding sites in the trace . from pyro import poutine def forecast(forecast_steps=10): counts = true_counts + [None] * forecast_steps # observed data + blanks to fill in guide_trace = poutine.trace(guide).get_trace(counts) _, _, counts = poutine.replay(model, guide_trace)(counts) return counts . We can now call forecast() multiple times to generate samples. . for _ in range(1): full_counts = forecast(10) forecast_counts = full_counts[len(true_counts):] plt.plot([c.item() for c in full_counts], &quot;r&quot;, label=None if _ else &quot;forecast&quot;, alpha=0.3) plt.plot([c.item() for c in true_counts], &quot;k-&quot;, label=&quot;truth&quot;) plt.legend(); . References . Pyro-ducomentation | PPL models for timeseries forecasting |",
            "url": "https://sambaiga.github.io/sambaiga/ppl/pyro/statistical%20inference/2020/03/01/ppl-pyro-intro.html",
            "relUrl": "/ppl/pyro/statistical%20inference/2020/03/01/ppl-pyro-intro.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Stochastic Variational Inference (SVI)",
            "content": "The previous post introduced the basic principle of Variational Inference (VI) as one of the approach used to approximate difficult probability distribution, derived the ELBO function and discussed about Mean Field Variational Inference (MFVI) and the Coordinate Ascent Variational Inference (CAVI) algorithms. This post introduce another stochastic gradient based algorithm (SVI) used in practise to do VI under mean filed assumptions. It also present two important tricks re-parametrization trick and amortized inference that are useful when using SVI in solving problems. . Stochastic Variational Inference (SVI) . Consider the graphical model of the observations $ mathbf{x}$ and latent variable $ mathbf{z}= { theta, z }$ in figure 1 where $ theta$ is the global variable and $z = {z_1, ldots z_n }$ is the local (per-data-point) variable such that: . . $$ p( mathbf{x}, mathbf{z}) = p( theta| alpha) prod_{i=1}^N p(x_i|z_i, theta) cdot p(z_i| alpha) $$ . Similarly the variational parameters are given by $ lambda = { gamma, phi } $ where the variational parameter $ gamma$ correspond to latent variable and $ phi$ denote set of local variational parameters. The variational distribution $q( mathbf{z} mid phi)$ is given by . $$ q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i| phi_i, alpha) $$which also depend on hyper-parameter $ alpha$. The ELBO of this graphical model $ mathcal{L}_{VI}(q) = mathbb{E}_q[ log p( mathbf{x}, mathbf{z}, alpha) - log q( mathbf{z}, gamma)]$ has the following form: . $$ begin{split} mathcal{L}_{VI}(q) &amp;= mathbb{E}_q[ log p( theta| alpha)- log q( theta| gamma)] &amp;+ sum_{i=1}^{N} mathbb{E}_q[ log p(z_i| theta) + log p(x_i|z_i, theta)- log q(z_i| phi_i)] end{split} $$ The equation above could be optimized by CAVI algorithm discussed in previous post which is expensive for large data sets. The CAVI algorithm scales with $N$ as it require to optimize the local variational parameters for each data point before re-estimating the global variational parameters. . Unlike CAI, SVI uses stochastic optimization to fit the global variational parameters by repeatedly sub-sample the data to form stochastic estimate of ELBO. In every iteration one randomly selects mini-batches of size $b_{sz}$ to obtain a stochastic estimate of ELBO. . $$ begin{split} mathcal{L}_{VI}(q) &amp;= mathbb{E}_q[ log p( theta| alpha)- log q( theta| gamma)] &amp;+ frac{N}{b_{sz}} sum_{s=1}^{b_{sz}} mathbb{E}_q[ log p(z_{i_s}| theta) + log p(x_{i_s}|z_{i_s}, theta)- log q(z_{i_s}| phi_{i_s})] end{split} $$SVI algorithms follow noisy estimates of the gradient with a decreasing step size which is often cheaper to compute than the true gradient. Following such noisy estimates allows SVI to escape shallow local optima of complex objective functions. . Natural Gradient for SVI . To solve the optimization problem standard gradient-based methods such as SGD, Adam or Adagrad can be used. However, for SVI these gradient based methods cause slow convergence or converge to inferior local models. This is because, gradient based methods use the following update . $$ theta^{t+1}= theta^t + alpha frac{ partial mathcal{L}_{VI}(q)}{ partial theta} $$where . $$ frac{ partial mathcal{L}_{VI}(q)}{ partial theta} = frac{ partial mathcal{L}_{VI}(q)}{ partial theta_1}, ldots frac{ partial mathcal{L}_{VI}(q)}{ partial theta_k} $$ . is the the gradient vector which point in the direction where the function increases most quickly while the changes in the function are measured with respect to euclidean distance. As the result, if the euclidean distance between the variational parameter being optimized is not good measure of variation in objective function then gradient descent will move suboptimal through the parameter value. . Consider the following two set of gausian distributions $$ {d_{(1)}= mathcal{N}(-2, 3), d_{(2)}= mathcal{N}(2, 3) }$$ and $$ {d_{(1)}= mathcal{N}(-2, 1), d_{(2)}= mathcal{N}(2, 1) }$$. . The euclidean distance between the two distributions $d_{}= sqrt{( mu_1- mu_2)^2+ ( sigma^2_1- sigma^2_2)^2}=4$ It clear that, considering only the euclidean distance the two images are the same. However, when we consider the shape of the distribution, the distance is different in the first and second image. In the first image, the KL-divergence should be lower as there is more overlap between between the two distribution unlike the second image where their support barely overlap. The reason for this difference is that probability distribution do not naturally fit in euclidean space rather it fit on a statistical manifold also called Riemannian manifold. . Statistical manifold give a natural way of measuring distances between distribution that euclidean distance use in SGD. A common Riemannian metric for statistical manifold is the fisher information matrix defined by . $$ F_{ lambda} = mathbb{E}_{p(x; lambda)}[ nabla log p(x; lambda) ( nabla log p(x; theta))^T ] $$ It can be showed that the fisher information matrix $F_{ lambda}$ is the second derivative of the KL divergence between two distributions. . $$ F_{ theta} = nabla^2_{ theta} KL(q(x; lambda)||p(x; theta)) $$Thus for SVI, the standard gradients descent techniques can be replaced with the natural gradient as follows: . $$ tilde{ nabla_{q}} mathcal{L}(q) = F^{-1} nabla{q} mathcal{L}_{VI}(q) $$The update procedure for natural gradient can be summarized as follows: . Compute the loss $ mathcal{L}_{VI}(q)$ | Compute the gradient of the loss $ nabla{q} mathcal{L}_{VI}(q)$ | Compute the Fisher Information Matrix F. | Compute the natural gradient $ tilde{ nabla_{q}} mathcal{L}_{VI}(q)$ | Update the parameter $q^{t+1} =q^t - alpha tilde{ nabla_{ theta}} mathcal{L}_{VI}(q)$ | Using natural gradient instead of standard gradients simplify SVI gradient update. However the same conditions for convergence as standard SDG have to be fulfilled. First, the mini-batch indices must be drawn uniformly at random size where the size $b_{sz}$ of the mini-batch must satisfies $1 leq b_{sz} leq N$ The learning rate $ alpha$ needs to decrease with iterations $t$ satisying the Robbins Monro conditions $ sum_{t=1}^{ infty} alpha_t = infty$ and $ sum_{t=1}^{ infty} alpha_t^2 &lt; infty$ This guarantee that every point in the parameter space can be reached while the gradient noise decreases quickly enough to ensure convergence. . The next section presents two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems. . Re-parametrization trick . Consider the graphical model presented in figure 1, where gradient based stochastic optimization is used to learn the variational parameter $ phi$. For example; for Gaussian distribution $q_{ phi}(z|x)= mathcal{N}( mu_{ phi}(x), Sigma_{ phi}(x))$ . To maximize the likelihood of the data, we need to back propagate the loss to the parameter $ phi$ across the distribution of $z$ or across sample $z sim q_ phi(z mid x) $ However, it is difficulty to back-propagate through random variable. To address this problem, the re-parametrization trick is used. . First let consider the Law of the Unconscious Statistician (LOTUS), that is used to calculate the expected value of a function $g( epsilon)$ of a random variable $ epsilon$ when only the probability distribution $p( epsilon)$ of $ epsilon$ is known. The Law state that: . To compute the expectation of a measurable function $g(.)$ of a random variable $ epsilon$, we have to integrate $g( epsilon)$ with respect to the distribution function of $ epsilon$, that is: $$ mathbb{E}(g( epsilon)) = int g( epsilon)dF_{ epsilon}( epsilon) $$ . In other words, to compute the expectation of $z =g( epsilon)$ we only need to know $g(.)$ and the distribution of $ epsilon$. We do not need to explicitly know the distribution of $z$. Thus the above equation can be expression in the convenient alternative notation: . $$ mathbb{E}_{ epsilon sim p( epsilon)}(g( epsilon)) = mathbb{E}_{z sim p(z)} (z) $$Therefore the reparameteriztaion trick states that: . A random variable $z$ with distribution $q_{ phi}(z, phi)$ which is independent to $ phi$ can be expressed as transformation of random variable $ epsilon sim p( epsilon)$ that come from noise distribution such as uniform or gaussian such that $z = g( phi, epsilon)$ . For instance for Gaussian variable $z$ in the above example . $$ z = mu( phi) + sigma^2( phi) cdot epsilon $$where $ epsilon sim mathcal{N}(0, 1)$. Since $p( epsilon)$ is independent of the parameter of $q_{ phi}(z, phi)$, we can apply the change of variables in integral theory to compute any expectation over $z$ or any expectation over $ phi$. The SDG estimator can therefore be estimated by pulling the gradient into expectations and approximating it by samples from the noise distribution such that for any measurable function $f_{ theta}(.)$: $$ Delta_{ phi} mathbb{E}_{z sim p_{ phi}(z)} = frac{1}{M} sum_{i=1}^M Delta f(g( phi, epsilon_i)) $$ . where $ epsilon_i sim p( epsilon)$ , $f_{ theta}(.)$ must be differentiable w.r.t its input $z$ and $g( phi, epsilon_i)$ must exist and be differentiable with respect to $ phi$. . Amortized Variational Inference . Consider the graphical model presented in figure 1 where ecah data point $x_i$ is governed by its latent variable $z_i$ with variational parameter $phi_i$ such that . $$ q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i| phi_i, alpha) $$Using traditional SVI make it necessary to optimize $ phi_i$ for each data point $x_i$. As the results the number parameters to be optimized will grows with the number of observations $x$. This is not ideal for larger datasets. Apart from that, it requires one to re-run the optimization procedure in case of new observation or when we have to perform inference. To address these problem amortized VI introduce a parametrized function that maps from observation space to the parameter of the approximate posterior distribution. . Amortized VI try to learn from past inference/pre-computation so that future inferences run faster. Instead of approximating separate variables for each data point $x_i$, amortized VI assume that the local variational parameter $ phi$ can be predicted by a parametrized function $f_{ phi}(.)$ of data whose parameters are shared across all data points. Thus instead of introducing local variational parameter, we learn a single parametric function and work with a variational distribution that has the form . $$ q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i|f_{ phi}(.)) $$where $f_{ phi}(.)$ is the deep neural net function of $z$ . Deep neural network used in this context are called inference networks. Therefore amortized inference with inference networks combines probabilistic modelling with representation power of deep learning. Using amortized VI instead of traditional VI, has two important advantages. First the number of variational parameters remain constant with respect to the data size. We only need to specify the parameter of the neural networks which is independent to the number of observations. Second, for new observation or during inference all we need to do is to call the inference network. As the result, we can invest time upfront optimizing the inference network and during inference we use the trained network for fast inference. . Example . Consider a binary regression problem where we are intrested on predicting whether or not a customer will subscribe a term deposit after the marketing campaign the bank performed. . import pyro import pyro.distributions as dist import pyro.optim as optim import torch import torch.nn.functional as F from pyro.infer import Trace_ELBO, SVI import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, LabelBinarizer from sklearn.model_selection import StratifiedShuffleSplit %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) . def label_binarize(df): bin_enc = LabelBinarizer() return bin_enc.fit_transform(df) def standardize(df): std_enc = StandardScaler() return std_enc.fit_transform(df) . Load data . data = pd.read_csv(&quot;data/bank.csv&quot;) ## Encode target variable LE=LabelEncoder() target=LE.fit_transform(data.deposit.values) features = data.drop(columns=&quot;deposit&quot;) # Separate both dataframes into numeric_data = data.select_dtypes(exclude=&quot;object&quot;) categ_data = data.drop(columns=numeric_data) #categ_data = pd.get_dummies(categ_data.values) . for column in categ_data.columns: categ_data[column] = label_binarize(categ_data[column].values) columns_int = [&quot;campaign&quot;, &quot;pdays&quot;, &quot;previous&quot;] for i, column in enumerate(columns_int): numeric_data[column] = label_binarize(numeric_data[column].values) numeric_column = [&quot;age&quot;, &quot;balance&quot;, &quot;day&quot;, &quot;duration&quot;] for i, column in enumerate(numeric_column): numeric_data[column] = standardize(numeric_data[[column]].values) . features = pd.concat([numeric_data, categ_data], 1) features = torch.tensor(features.values).float() target = torch.tensor(target).long() . sss = StratifiedShuffleSplit(n_splits=10, test_size=0.15, random_state=0) train, test = next(sss.split(features.numpy(), target.numpy())) . X_train, X_test = features[train], features[test] y_train, y_test = target[train], target[test] . Model . from pyro.nn import PyroSample from torch import nn from pyro.nn import PyroModule assert issubclass(PyroModule[nn.Linear], nn.Linear) assert issubclass(PyroModule[nn.Linear], PyroModule) class BayesianRegression(PyroModule): def __init__(self, in_features, out_features=1): super().__init__() self.linear = PyroModule[nn.Linear](in_features, out_features) self.linear.weight = PyroSample(dist.Normal(0., 1.).expand([out_features, in_features]).to_event(2)) self.linear.bias = PyroSample(dist.Normal(0., 10.).expand([out_features]).to_event(1)) def forward(self, x, y=None): sigma = pyro.sample(&quot;sigma&quot;, dist.Uniform(0., 10.)) mean = torch.sigmoid(self.linear(x).squeeze(-1)) with pyro.plate(&quot;data&quot;, x.shape[0]): obs = pyro.sample(&quot;obs&quot;, dist.Bernoulli(mean), obs=y) return mean . Guide . from pyro.infer.autoguide import AutoDiagonalNormal model = BayesianRegression(features.size(-1), 1) guide = AutoDiagonalNormal(model) . from pyro.infer import SVI, Trace_ELBO adam = pyro.optim.Adam({&quot;lr&quot;: 0.03}) svi = SVI(model, guide, adam, loss=Trace_ELBO()) . for j in range(5000): # calculate the loss and take a gradient step loss = svi.step(X_train, y_train.float()) if j % 100 == 0: print(&quot;[iteration %04d] loss: %.4f&quot; % (j + 1, loss / len(X_train))) . [iteration 0001] loss: 0.0208 [iteration 0101] loss: 0.0188 [iteration 0201] loss: 0.0167 [iteration 0301] loss: 0.0160 [iteration 0401] loss: 0.0151 [iteration 0501] loss: 0.0146 [iteration 0601] loss: 0.0133 [iteration 0701] loss: 0.0136 [iteration 0801] loss: 0.0135 [iteration 0901] loss: 0.0135 [iteration 1001] loss: 0.0129 [iteration 1101] loss: 0.0137 [iteration 1201] loss: 0.0124 [iteration 1301] loss: 0.0123 [iteration 1401] loss: 0.0125 [iteration 1501] loss: 0.0132 [iteration 1601] loss: 0.0122 [iteration 1701] loss: 0.0120 [iteration 1801] loss: 0.0120 [iteration 1901] loss: 0.0131 [iteration 2001] loss: 0.0121 [iteration 2101] loss: 0.0120 [iteration 2201] loss: 0.0119 [iteration 2301] loss: 0.0118 [iteration 2401] loss: 0.0118 [iteration 2501] loss: 0.0118 [iteration 2601] loss: 0.0114 [iteration 2701] loss: 0.0118 [iteration 2801] loss: 0.0117 [iteration 2901] loss: 0.0119 [iteration 3001] loss: 0.0122 [iteration 3101] loss: 0.0119 [iteration 3201] loss: 0.0118 [iteration 3301] loss: 0.0122 [iteration 3401] loss: 0.0121 [iteration 3501] loss: 0.0117 [iteration 3601] loss: 0.0119 [iteration 3701] loss: 0.0118 [iteration 3801] loss: 0.0120 [iteration 3901] loss: 0.0117 [iteration 4001] loss: 0.0119 [iteration 4101] loss: 0.0129 [iteration 4201] loss: 0.0117 [iteration 4301] loss: 0.0118 [iteration 4401] loss: 0.0117 [iteration 4501] loss: 0.0119 [iteration 4601] loss: 0.0118 [iteration 4701] loss: 0.0122 [iteration 4801] loss: 0.0122 [iteration 4901] loss: 0.0120 . guide.requires_grad_(False) for name, value in pyro.get_param_store().items(): print(name, pyro.param(name)) . w_loc tensor([-0.0071, 0.0229, -0.0480, 0.8757, 0.0749, -0.9454, -0.9454, 0.0240, -0.1021, -0.3675, -0.1923, -0.9378, -0.4416, -0.0203, 0.2147, -1.3860, 6.3369], requires_grad=True) w_scale tensor([-1.9725, -2.2263, -1.8859, -1.4443, -1.4062, -1.3978, -1.3952, -1.3166, -1.2706, -1.2814, -0.7051, -1.3490, -1.2376, -1.4575, -1.2055, -1.3631, -0.8033], requires_grad=True) b_loc tensor(-0.9499, requires_grad=True) b_scale tensor(-1.4293, requires_grad=True) sigma_loc tensor(0.4712, requires_grad=True) sigma_scale tensor(1.0128, requires_grad=True) AutoDiagonalNormal.loc Parameter containing: tensor([ 2.0622e-01, -3.7485e-03, -3.0043e-02, -8.0583e-02, 6.4904e-01, 1.8374e-01, -3.6323e-01, -3.5512e-01, -2.5773e-02, 5.6965e-02, -1.7738e-01, 4.0707e-02, -5.3365e-01, -2.6170e-01, 3.2216e-01, 4.4906e-02, -2.5154e-01, 1.2037e+01, -5.6873e+00]) AutoDiagonalNormal.scale tensor([1.8662, 0.1736, 0.1103, 0.1989, 0.2169, 0.2756, 0.2084, 0.2840, 0.4834, 0.5076, 0.3287, 0.8293, 0.2835, 0.3674, 0.2236, 0.5573, 0.4754, 0.2786, 0.2006]) . guide.quantiles([0.25, 0.5, 0.75]) . {&#39;sigma&#39;: [tensor(2.5875), tensor(5.5137), tensor(8.1229)], &#39;linear.weight&#39;: [tensor([[-1.2086e-01, -1.0444e-01, -2.1471e-01, 5.0276e-01, -2.1774e-03, -5.0379e-01, -5.4666e-01, -3.5181e-01, -2.8543e-01, -3.9910e-01, -5.1865e-01, -7.2488e-01, -5.0951e-01, 1.7136e-01, -3.3100e-01, -5.7217e-01, 1.1849e+01]]), tensor([[-3.7485e-03, -3.0043e-02, -8.0583e-02, 6.4904e-01, 1.8374e-01, -3.6323e-01, -3.5512e-01, -2.5773e-02, 5.6965e-02, -1.7738e-01, 4.0707e-02, -5.3365e-01, -2.6170e-01, 3.2216e-01, 4.4906e-02, -2.5154e-01, 1.2037e+01]]), tensor([[ 0.1134, 0.0444, 0.0535, 0.7953, 0.3697, -0.2227, -0.1636, 0.3003, 0.3994, 0.0443, 0.6001, -0.3424, -0.0139, 0.4730, 0.4208, 0.0691, 12.2247]])], &#39;linear.bias&#39;: [tensor([-5.8226]), tensor([-5.6873]), tensor([-5.5520])]} . from pyro.infer import Predictive def summary(samples): site_stats = {} for k, v in samples.items(): site_stats[k] = { &quot;mean&quot;: torch.mean(v, 0), &quot;std&quot;: torch.std(v, 0), &quot;5%&quot;: v.kthvalue(int(len(v) * 0.05), dim=0)[0], &quot;95%&quot;: v.kthvalue(int(len(v) * 0.95), dim=0)[0], } return site_stats predictive = Predictive(model, guide=guide, num_samples=800, return_sites=(&quot;linear.weight&quot;, &quot;obs&quot;, &quot;_RETURN&quot;)) samples = predictive(X_test) pred_summary = summary(samples) . mu = pred_summary[&quot;_RETURN&quot;] y = pred_summary[&quot;obs&quot;] predictions = pd.DataFrame({ &quot;age&quot;: X_test[:, 0], &quot;rugged&quot;: X_test[:, 1], &quot;mu_mean&quot;: mu[&quot;mean&quot;], &quot;mu_perc_5&quot;: mu[&quot;5%&quot;], &quot;mu_perc_95&quot;: mu[&quot;95%&quot;], &quot;y_mean&quot;: y[&quot;mean&quot;], &quot;y_perc_5&quot;: y[&quot;5%&quot;], &quot;y_perc_95&quot;: y[&quot;95%&quot;], &quot;true_gdp&quot;: y_test, }) . y[&#39;mean&#39;] . tensor([0.0000, 0.9962, 0.0012, ..., 0.9987, 0.9950, 0.9975]) . plt.plot(y[&#39;mean&#39;], y_test) . [&lt;matplotlib.lines.Line2D at 0x1a28bc04d0&gt;] . ## Reference 1. [[Cheng Zhang,(2017)]](https://arxiv.org/abs/1711.05597): Advances in Variational Inference. 2. [[Daniel Ritchie,(2016)]](https://arxiv.org/abs/1610.05735):Deep Amortized Inference for Probabilistic Programs. 3. [[Andrew Miller,(2016)]](https://arxiv.org/abs/1610.05735):Natural Gradients and Stochastic Variational Inference. 4. [Shakir Mohamed](https://www.shakirm.com/papers/VITutorial.pdf):Variational Inference for Machine Learning. 5. [DS3 workshop](https://emtiyaz.github.io/teaching/ds3_2018/ds3.html):Approximate Bayesian Inference: Old and New. 6. [Variational Inference and Deep Generative Models](https://github.com/philschulz/VITutorial):Variational Inference for NLP audiences .",
            "url": "https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html",
            "relUrl": "/jupyter/2019/05/02/svi.html",
            "date": " • May 2, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Variational Inference (VI)",
            "content": "Bayesian method offer a different paradigm for doing statistical analysis. It is practical method for making inferences from data using probability models. Unlike other statistical approach, bayesian models are easy to interpret and incorporate uncertainty. In bayesian method we start with a belief which is also called a prior. We then update our belief after observing some data. The outcome is called a posterior. The process repeats as we keep on observing more data where the old posterior becomes a new prior. The process employs the Bayes rule. . Consider the Bayesian theorem, which allows us to use some knowledge or belief that we already have. Given data point $ mathcal{D} = {x in mathbb{R}^{N times d}, y in mathbb{R}^{N times c} }$. The Bayesian approach treat the latent variable or parameter $z$ as random variable with some prior distribution $p(z)$. This is the probability of parameters $z$ before hand. . $$ p(z | mathcal{D} ) = frac{p( mathcal{D} | z ) cdot p(z)}{p( mathcal{D})} $$where . $$ p( mathcal{D}) = int p( mathcal{D} |z ) cdot p(z) dz $$From the bayesian theorem above, $z$ is the hypothesis about the world, and $$ mathcal{D}$$ is the data or evidence. The probability $p( mathcal{D} mid z)$ is called likeli-hood; the probability of data given the latent variable and $p( mathcal{D})$ is the marginal-likelihood and $p(z mid mathcal{D} )$ is the posterior. . Bayesian Inference . Given data set $ mathcal{D}$ and latent variable $z$ that relate $x$ and $y$ such that: $$ y = f_{z}(x:z) $$ The first step in bayesian inference is to identify the parameter $z$ and express our lack of knowledge about this parameter in term of probability distribution $p(z)$. This is the prior knowledge about the parameter $z$. After that we express a likelihood $p( mathcal{D} mid z)$ which tell us how the data $ mathcal{D}$ interact with parameter $z$. Together the prior and the likelihood make our model (generative model). It tell us how we can simulate from our data. . In training stage we apply Bayesian theorem to get posterior distribution: . $$ p(z| mathcal{D}) = frac{p( mathcal{D}|, z)}{p( mathcal{D})} $$ . In testing stage we find predictive-distribution . $$ p( hat{y}| x, mathcal{D}) = int p( hat{y} | x, z) cdot p(z | x, y) dz $$ $y= f_{ theta}(x: theta) + in $ where $ in$ is the noise due to measurement and $f_{ theta}(X: theta)$ is the hypothesis function given; $ f_{ theta}(X: theta) = b + sum_{i=1}^N w_i phi (x_i) = theta^T cdot phi(X) $ . where $ phi(X)$ is the basis function and $ theta$ is the model parameters such that $ theta_{0} =b$$ and $$ phi_0=1 $ . The output of this model is the single point estimate for the best model parameter. The Bayesian modelling approach to this problem offer a systematic framework for learning distribution over values of the parameters and not a single estimate. The bayesian linear regression model $y= f_{ theta}(x: theta) + in $ as a Gaussian distribution such that: $ p(y|x, theta) = mathcal{N}(y|f_{ theta}(x: theta), beta^{-1}) $ . Assuming the data point are drawn independently and identically distributed the likelihood is expressed as: . $$ p(Y| X, theta) = prod_{i=1}^N mathcal{N}(y_i|f_{ theta}(x_i: theta_i), beta^{-1}) $$Let choose a prior that is conjugate to the likelihood . $$ p( theta|X) = mathcal{N}( theta|0, alpha^{-1}) $$Thus the posterior is given as: . $$ p( theta|Y, X) propto mathcal{N}( theta|0, alpha^{-1}) cdot prod_{i=1}^N mathcal{N}(y_i|f_{ theta}(x_i: theta_i), beta^{-1}) $$ Variational Inference (VI) . In the previous section we show that inference in probabilist model is often intractable and introduced several approach used to approximate the inference. Variational Inference (VI) is one of the approach used to approximate difficult probability distribution by turning the calculation about model into optimization. . Consider a probabilistic model which is joint distribution $p(x,z)$ of the latent variable $$z$$ observed variables $x$. To draw inference on the latent variable $z$ we compute the posterior . $$ p(z|x) = frac{p(x,z)}{p(x)} = frac{p(x|z) cdot p(z)}{p(x)} $$where $ p(x)= int p(x|z) cdot p(z) dz $ To approximate $p(z mid x)$ we first choose an approximating family of distribution $q(z)$ over latent variable $z$. Then we find set of parameters that makes $q(z)$ close to posterior distribution $p(z mid { bf x})$. Thus VI approximate $p(z mid x)$ with new distribution $q(z)$ such that $q(z)$ is close to $p(z mid x)$. To achieve this we minimize KL divergence between $q(z)$ and $p(z mid x)$ such that: . $$ q^*(z) = arg min D_{KL}(q(z)||p(z|x)) $$where $ D_{KL}(q(z)||p(z|x)) = int q(z) log frac{q(z)}{p(z|x)} $ . It clear that we can not minimize KL divergence since it is directly depend on posterior $p(z mid x)$. However we can minimize a function that is equal to KL divergence plus constant. This function is called Evidence Lower Bound(ELBO) $ mathcal{L}_{VI}(q)$. . Evidence Lower Bound (ELBO) . To derive the ELBO we first consider the Jensen&#39;s inequality which relates the value of a convex function of an integral to the integral of the convex function such that $f( mathbb{E}[x]) geq mathbb{E}[f(x)]$ where $f(.)$ is the concave function. Since logarithmic are strictly concave function it is clear that . $$ log int p(x)g(x) dx geq int p(x) log g(x) $$Let us consider a log of marginal evidence. . $$ begin{aligned} log p(x) &amp; = log int_z p(x,z) dz &amp; = log int_z p(x,z) cdot frac{q(z)}{q(z)} dz &amp; = log int_z q(z) frac{p(x,z)}{q(z)} dz &amp; = log left( mathbb{E}_q[ frac{p(x,z)}{q(z)}] right) &amp; geq mathbb{E}_q[ log p(x,z)] - mathbb{E}_q[ log q(z)] end{aligned} $$The final line is the ELBO which is the lower bound for the evidence. Thus the evidence lower bound for probability model $$p(x,z)$$ and approximation $$q(z)$$ to the posterior is . $$ mathcal{L}_{VI}(q) = mathbb{E}_q[ log p(x,z)] - mathbb{E}_q[ log q(z)] $$We can now show that KL divergence to the posterior is equal to the negative ELBO plus constant. . $$ begin{aligned} D_{KL}(q(z)||p(z|x)) &amp;= int q(z) log frac{q(z)}{p(z|x)} &amp;= mathbb{E}_q[ log q(z)] - mathbb{E}_q[ log p(x,z)] + mathbb{E}_q[ log p(x)] &amp;=- left( mathbb{E}_q[ log p(x,z)] - mathbb{E}_q[ log q(z)] right) + log p(x) &amp;= - mathcal{L}_{VI}(q) + log p(x) mathcal{L}_{VI}(q) &amp;= log p(x) + D_{KL}(q(z)||p(z|x)) end{aligned} $$From the equation above it clear that minimizing the KL divergence is equivalent to maximizing the ELBO. Recall that we want to find $$q(z)$$ such that KL divergence is small, the variational objective function becomes . $$ q^*(z) = arg min D_{KL}(q(z)||p(z|x)) = arg max mathcal{L}_{VI}(q) $$Mean Field Variational Inference . One of the important question on VI, is how to construct the family of variational distributions from which we want to draw $q(z)$ from. The simplest family is where each latent parameter $z_i$ has its own independent distribution. This means that we can easily factorize the variational distributions into groups: . $$ q(z_1, ldots, z_m) = prod_{i=1}^m q(z_i) $$This family of distribuion are known as Mean-Field Variational Family that make use of mean field theory. Inference using this factorization is known as Mean-Field Variational Inference (MFVI). . It possible to further parameterize the approximating distributions $q(z)$ with variational parameters $ lambda$ such that the approximating distribution become $q(z_i ; lambda_i)$. For example if we set our family of approximating distributions as a set of independent gauasian distributions $ mathcal{N}( mu_i, sigma^2_i)$ and parameterize this distributions with the mean and variance where $ lambda_i = ( mu_i, sigma^2_i)$ is the set of variational parameters. . The common algorithms used in practise to do VI under mean filed assumptions are coordinate ascent optimization (CAVI) and stochastic gradient based method. . Coordinate Ascent Variational Inference (CAVI) . The CAVI algorithm derive variational updates by hand and perform coordinate ascent (iteratively updating each latent variable $z_i$) on the latent until convergence of the ELBO. A common procedure to conduct CAVI is: . Choose variational distributions $q(z)$ | Compute ELBO; | Optimize individual $q(z_i)$ ’s by taking the gradient for each latent variable; | Repeat until ELBO converges. | . The coordinate ascent update for a latent variable can be derived by maximizing the ELBO function above. First, recall ELBO . $$ mathcal{L}_{VI}(q) = mathbb{E}_q[ log p(x,z)] - mathbb{E}_q[ log q(z)] $$ Applying chain rule we can decomopse the joint $p(x,z)$ as; $$ p(x_{1:n}, z_{1:m}) = p(x_{1:n}) prod_{i=1}^m p(z_i|z_{1:(i-1)}, x_{1:n}) $$ Using mean field approximation, we can decompose the entropy term of the ELBO as $$ mathbb{E}_q[ log q(z)] = sum_{i=1}^m mathbb{E}_q[ log q(z_i)] $$ Under the above assumption the ELBO become: $$ mathcal{L}_{ELBO}(q) = log p(x_{1:n}) + sum_{i=1}^m mathbb{E}_q[ log p(z_i|z_{1:(i-1)}, x_{1:n}) ] - mathbb{E}_q[ log q(z_i) $$ . To find this $ arg max_{q(z_i)} mathcal{L}_{ELBO}(q)$ we take derivative of ELBO with respect to $q(z_i)$ using Lagrange multipliers and set the derivative to zero. It can be shown that the coordinate ascent update rule is equal to . $$ q^*(z_i) propto { mathbb{E}_{q-i}[ log q(z_i,z_{ neg},x)] } $$where the notation $ neg$ denotes all indices other than the $i^{th}$ . Despite being very fast method for some models only work with conditionally conjugate models. . Reference . ICML 2018 tutorial:Variational Bayes and Beyond: Bayesian Inference for Big Data. | Shakir Mohamed:Variational Inference for Machine Learning. | DS3 workshop:Approximate Bayesian Inference: Old and New. | Variational Inference and Deep Generative Models:Variational Inference for NLP audiences |",
            "url": "https://sambaiga.github.io/sambaiga/ppl/ml/statistical%20inference/2019/03/02/bayes-vi.html",
            "relUrl": "/ppl/ml/statistical%20inference/2019/03/02/bayes-vi.html",
            "date": " • Mar 2, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Learning from probabilistic models",
            "content": "Introduction . Given some data $ mathbf{x}=[x_1 ldots x_m]$ that come from some probability density function characterized by an unknown parameter $ theta$. How can we find $ hat{ theta}$ that is the best estimator of $ theta$. For example suppose we have flipped a particular coin $ 100$ times and landed head $ N_H = 55$ times and tails $ N_T = 45$ times. We are interested to know what is the probability that it will come-up head if we flip it again. In this case the behavior of the coin can be summerized with parameter $ theta$ the probability that a flip land head (H), which in this case is independent and identically distributed Bernoulli distribution. The key question is, how do we find parameter $ hat{ theta}$ of this distribution that fits the data. This is called parameter estimation, in which three approaches can be used: . Maximum-Likehood estimation | Bayesian parameter estimation and | Maximum a-posterior approximation | Like-hood and log-likehood function . Before discussing the above learning approach, let firts define the like-hood function $L( theta)$ which is the probability of the observed data as function of $ theta$ given as: . $$ L( theta) = P(x_1, ldots x_m; theta) = prod_i^m P(x_i; theta) $$The like-hood function indicates how likely each value of the parameter is to have generated the data. In the case of coin example above, the like-lihood is the probability of particular seqeuence of H and T generated: . $$ L( theta) = theta ^{N_H}(1 - theta ^{N_T}) $$ . We also define the log-likelihood function $ mathcal{L}( theta)$ which is the log of the likelihood function $L( theta)$. . $$ begin{aligned} mathcal{L}( theta) &amp;= log L( theta) &amp; = log prod_i^m P(x_i; theta) &amp; = sum_i^M P(x_i; theta) end{aligned} $$For the above coin example the log-likelihood is . $$ mathcal{L}( theta)= N_H log theta + N_T log(1- theta) $$ Maximum-Likelihood Estimation . The main objective of maximum likelihood estimation (MLE) is to determine the value of $ theta$ that is most likely to have generated the vector of observed data, $ mathbf{x}$ where $ theta$ is assumed to be fixed point (point-estimation). MLE achieve this by finding the parameter that maximize the probability of the observed data. The parameter $ hat{ theta}$ is selected such that it maximize $ mathcal{L}( theta)$: . $ hat{ theta}= arg max_{ theta} mathcal{L}( theta) $ . For the coin example the MLE is : . $$ begin{aligned} frac{ partial mathcal{L}( theta)}{ partial theta} &amp; = frac{ partial }{ partial theta}(N_H log theta + N_T log(1- theta) &amp;= frac{N_H}{ theta} - frac{N_T}{1- theta} end{aligned} $$Set $ frac{ partial mathcal{L}( theta)}{ partial theta} = 0$ and solve for $ theta$ we obtain the MLE: . $ hat{ theta} = frac{N_H}{N_H + N_T} $ . which is simply the fraction of flips that cameup head. . Now suppose we are observing power-meta data which can be modelled as gaussian ditribution with mean $$ mu$$ and standard deviation $ sigma$. We can use MLE to estimate $ hat{ mu}$ and $ hat{ sigma}$. The log-likehood for gausian distribution is given as . $$ begin{aligned} mathcal{L}( theta) &amp;= sum_{i=1}^M log left[ frac{1}{ sqrt{2} pi sigma} exp frac{-(x_i - mu)}{2 sigma ^2} right] &amp; = - frac{M}{2} log 2 pi - M log sigma - frac{1}{2 sigma^2} sum_i^M (x_i - mu)^2 end{aligned} $$Let find $ frac{ partial mathcal{L}( theta)}{ partial mu} $ and $ frac{ partial mathcal{L}( theta)}{ partial sigma} $ and set equal to zero. . $$ begin{aligned} frac{ partial mathcal{L}( theta)}{ partial mu} &amp;= - frac{1}{2 sigma^2} sum_i^M frac{ partial}{ partial mu}(x_i - mu)^2 &amp; = sum_i^M (x_i - mu) = 0 &amp; Rightarrow hat{ mu} = frac{1}{M} sum_{i=1}^M x_i end{aligned} $$which is the mean of the observed values. Similary: . $$ begin{aligned} frac{ partial mathcal{L}( theta)}{ partial sigma} &amp;= frac{M}{ sigma} + frac{1}{ sigma^3} sum_i^M (x_i - mu)^2 &amp; Rightarrow hat{ sigma} = sqrt{ frac{1}{M} sum_{i=1}^M (x_i - mu)^2} end{aligned} $$In the two examples above we manged to obtain the exact maximum likelihood solution analytically. But this is not always the case, let’s consider how to compute the maximum likelihood estimate of the parameters of the gamma distribution, whose PDF is defined as: . $$ P(x) = frac{b^a}{ Gamma(a)}x^{x-1} exp(-bx) $$where $ Gamma (a)$ is the gamma function which is the generalization of the factorial function to continous values given as: . $ Gamma(t) = int_0^{- infty} x^{t-1} exp(-x) ,dx $ . The model parameters for gamma distribution is $a$ and $b$ both of which are $ geq 0$. the log-likelihood is therefore: . $ begin{aligned} mathcal{L}( a, b) &amp; = sum_{i=1}^M a log b - log Gamma (a) + (a -1) log x_i - bx_i &amp; = Ma log b - M log Gamma (a) + (a - 1) sum_{i=1}^M log x_i - b sum_{i=1}^M x_i end{aligned} $ . To get MLE we need employ gradient descent which consists of computing the derivatives: $ frac{ partial mathcal{L}}{ partial a} $ and $ frac{ partial mathcal{L}}{ partial b} $ and then updating; $ a_{k+1}= a_k + alpha frac{ partial mathcal{L}}{ partial a} $ and $ b_{k+1}= b_k + alpha frac{ partial mathcal{L}}{ partial b} $ . where $ alpha$ is the learning rate. . Limitation of MLE . Despite the fact that MLE is very powerful technique, it has a pitfall for little training data which can lead into seriously overfit. The most painful issue is when it assign a $0$ probability to items that were never seen in the training data but which still might actually happen. Take an example if we flipped a coin twice and $N_H = 2$, the MLE of $ theta$, the probability of H would be $1$. This imply that we are considering it impossible for the coin to come up T. This problem is knowas data sparsity. . Bayesian Parameter Estimation . Unlike MLE which treat only the observation $ mathbf{x}$ as random variable and the parameter $ theta$ as a fixed point, the bayesian approach treat the parameter $ theta $ as random varibale as well with some known prior distribution. Let define the model for joint distribution $$p( theta, mathcal{D})$$ over parameter $ theta$ and data $ mathcal{D}$. To further define this joint distribution we aslo need the following two distribution: . A distribution of $P( theta)$ knowas prior distribution which is the probability of paratemeter $ theta$ availabe beforehand, and before making any additional observations. It account for everything you believed about the parameter $ theta$ before observing the data. In practise choose prior that is computational convinient. . | The likelihood $P( mathcal{D} mid theta)$ which is the probability of data given the parameter like in maximum likelihood. . | . With this two distributions, we can compute the posterior distribution and the posterior predictive distribution. The posterior distribution $P( theta mid mathcal{D})$ which correspond to uncertainty about $ theta$ after observing the data given by: . $$ begin{aligned} P( theta mid mathcal{D}) &amp;= frac{P( theta)p( mathcal{D} mid theta)}{P( mathcal{D})} &amp;= frac{P( theta)P( mathcal{D} mid theta)}{ displaystyle int P( theta ^ { prime} ) P( mathcal{D} mid theta ^{ prime})} end{aligned} $$The denominator is usually considered as a normalizing constant and thus the posterior distribution become: . $$ P( theta mid mathcal{D}) propto P( theta)P( mathcal{D} mid theta) $$On the other hand the posterior predictive distribution $P( mathcal{D}^{ prime} mid) mathcal{D}$ is the distribution of future observation given past observation defined by: . $$ P( mathcal{D}^{ prime} mid mathcal{D} )= int P( theta mid mathcal{D}) P( mathcal{D}^{ prime} mid theta) $$Generaly the Bayesian approach to parameter estimation works as follows: . First we need to formulate our knowledge about a situation by defining a distribution model which expresses qualitative aspects of our knowledge about the situation and then specify a prior probability distribution which expresses our subjective beliefs and subjective uncertainty about the unknown parameters, before seeing the data. | Gather data | Obtain posterior knowledge that updates our beliefs by computing the posterior probability distribution which estimates the unknown parameters. | Let apply the bayesian estimation to the coin example in which we have specified the likelihood equal to $ theta^{N_H}(1- theta)^{N_T}$. We only required to specify the prior in which several approches can be used. One of the approach is relay upon lifetime experince of flipping coins in which most coins tend to be fair which implies $p( theta) = 0.5$. We can also use various distribution to specify prior density but in practise a most useful distribution is the beta distribution parameterized by $a , b &gt; 0$ and defined as: . $$ p( theta; a, b) = frac{ Gamma (a + b)}{ Gamma(a) Gamma (b)} theta ^{a-1}(1- theta ^{b - 1}) $$From the above eqution it is clear that the first term (with all $ Gamma$)is just a normalizing constant and thus we can rewrite the beta distribution as: . $$ p( theta; a, b) propto theta ^{a-1}(1- theta) ^{b - 1} $$Note the beta distribution has the following properties . It is centered around $ frac{a}{a + b}$ and it can be shown that if $ theta sim text{Beta}(a,b)$ then $ mathbb{E}( theta)= frac{a}{a + b}$. | It becomes more peaked for larger values of $a$ and $b$ | It become normal distribution when $a = b = 1$ | . Now let compute the posterior and posterior predictive distribution . $$ begin{aligned} p( theta | mathcal{D}) &amp; propto p( theta)p( mathcal{D} | theta) &amp; propto theta^{N_H}(1- theta)^{N_T} theta ^{a-1}(1- theta) ^{b - 1} &amp; = theta ^{a-1+N_H}(1- theta) ^{b - 1 + N_T} end{aligned} $$",
            "url": "https://sambaiga.github.io/sambaiga/ml/ds/ppl/2018/04/02/probabilities-learning.html",
            "relUrl": "/ml/ds/ppl/2018/04/02/probabilities-learning.html",
            "date": " • Apr 2, 2018"
        }
        
    
  
    
        ,"post5": {
            "title": "Basics of Probability and Information Theory",
            "content": "Introduction . Probability and Information theory are important field that has made significant contribution to deep learning and AI. Probability theory allows us to make uncertain statements and to reason in the presence of uncertainty where information theory enables us to quantify the amount of uncertainty in a probability distribution. . Probability Theory . Probability is a mathematical framework for representing uncertainty. It is very applicable in Machine learning and Artificial Intelligence as it allows to make uncertain statements and reason in the presence of uncertainty. Probability theory allow us to design ML algorithms that take into consideration of uncertain and sometimes stochastic quantities. It further tell us tell us how ML systems should reason in the presence of uncertainty. This is necessary because most things in the world are uncertain, and thus ML systems should reason using probabilistic rules. Probability theory can also be used to analyse the behaviour of ML algorithms probabilistically. Consider evaluating ML classification algorithm using accuracy metric which is the probability that the model will give a correct prediction given an example. . Probability and Probability distribution . Probability is a measure of the likelihood that an event will occur in a random experiment. It is quantified as number between 0 and 1. The mathematical function that maps all possible outcome of a random experiment with its associated probability it is called probability distribution. It describe how likely a random variable or set of random variable is to take on each of its possible state. The probability distribution for discrete random variable is called probability mass function (PMF) which measures the probability $X$ takes on the value $x$, denoted denoted as $P(X=x)$. To be PMF on random variable $X$ a function $P(X)$ must satisfy: . Domain of $P$ equal to all possible states of $X$ | $ forall x in X, 0 leq P(X=x) leq 1$ | $ sum_{x in X} P(x) =1$ | . Popular and useful PMF includes poison, binomial, bernouli, and uniform. Let consider a poison distribution defined as: . $$ P(X=x) = frac{ lambda ^x e^{ - lambda}}{x!} $$$ lambda &gt;0$ is called a parameter of the distribution, and it controls the distribution&#39;s shape. By increasing $ lambda$ , we add more probability to larger values, and conversely by decreasing $ lambda$ we add more probability to smaller values as shown in figure below. . Instead of a PMF, a continuous random variable has a probability density function (pdf) denoted as $f_X(x)$. An example of continuous random variable is a random variable with exponential density. $$ f_X(x mid lambda) = lambda ^x e^{ - lambda} text{, } x geq 0 $$ . To be a probability density function $p(x)$ must satisfy . The domain of $p$ must be the set of all possible state | $ forall x in X, f_X(x) geq 0$ | $ int_{x in X} f_X(x)dx =1$ | . The pdf does not give the probability of a specific state directly. The probability that $x$ is between two point $a, b$ is . $ int_{a}^b f_X(x)dx$ . The probability of intersection of two or more random variables is called joint probability denoted as $ P(X, Y) $ . Suppose we have two random variable $X$ and $Y$ and we know the joint PMF or pdf distribution between these variable. The PMF or pdf corresponding to a single variable is called marginal probability distribution defined as $$ P(x) = sum_{y in Y} P(x, y) $$ . for discrete random variable and $$ p(x) = int p(x)dy $$ . Marginalization allows us to get the distribution of variable $$X$$ ignoring variable $Y$ from the joint distribution $P(X,Y)$. The probability that some event will occur given we know other events is called condition probability denoted as $P(X mid Y)$. The marginal, joint and conditional probability are linked by the following rule $ P(X|Y) = frac{P(X, Y)}{P(Y)} $ . Independence, Conditional Independence and Chain Rule . Two random variables are said to be independent of each other if the probability that one random variables occur in no way affect the probability of the other random variable occurring. $X$ and $Y$ are said to be independent if $P(X,Y) = P(X) cdot P(Y)$ On the other hand two random variable $X$ and $Y$ are conditionally independent given an event $Z$ with $P(Z)&gt;0$ if . $$ P(X,Y mid Z) = P(X mid Y) cdot P(Y mid Z) $$The good example of conditional independence can be found on this link. Any joint probability distribution over many random variables may be decomposed into conditional distributions using chain rule as follows: . $$ P(X_1,X_2, ldots, X_n ) = P(X_1) prod_{i=2}^n P(X_i mid X_i, ldots X_{i-1}) $$Expectation, Variance and Covariance . Expected value of some function $f(x)$ with respect to a probability distribution $P(X)$ is the average or mean value that $f(x)$ takes on when $x$ is drawn from $P$. . $$ mathbb{E}_{x sim P}[f(x)] = sum P(x).f(x) $$for discrete random variable and . $$ mathbb{E}_{x sim P}[f(x)] = int P(x).f(x)dx $$Expectation are linear such that $$ mathbb{E}_{x sim P}[ alpha cdot f(x) + beta cdot g(x)] = alpha mathbb{E}_{x sim P}[f(x)] + beta mathbb{E}_{x sim P}[g(x)] $$ . Variance is a measure of how much the value of a function of random variable $X$ vary as we sample different value of $x$ from its probability distribution. $$ Var(f(x)) = mathbb{E}([f(x)- mathbb{E}[f(x)]^2]) $$ The square root of the variance is know as standard deviation. On the other hand the covarince give some sense of how much two value are linearly related to each other as well as the scale of these value. . $$ Cov(f(x), g(y)) = mathbb{E}[(f(x)- mathbb{E}[f(x)])(g(y)- mathbb{E}[g(y)])] $$ Information theory . Information theory deals with quantification of how much information is present in a signal. In context of machine learning, information theory we apply information theory to: characterize probability distributions and quantify similarities between probability distributions. The following are the key information concepts and their application to machine learning. . Entropy, Cross Entropy and Mutual information . Entropy give measure of uncertainty in a random experiment. It help us quantify the amount of uncertainty in an entire probability distribution. The entropy of a probability distribution is the expected amount of information in an event drawn from that distribution defined as. . $$ H(X) = - mathbb{E}_{x sim P}[ log P(x)] = - sum_{i=1}^n P(x_i)l log P(x_i) $$Entropy is widely used in model selection based on principle of maximum entropy. On the other hand, cross entropy is used to compare two probability distribution. It tell how similar two distribution are. The cross entropy between two probability distribution $P$ and $$Q$ defined over same set of outcome is given by . $$ H(P,Q)= - sum P(x) log Q(x) $$ . Cross entropy loss function is widely used in machine learning for classification problem. The mutual information over two random variables help us gain insight about the information that one random variable carries about the other. . $$ begin{aligned} I(X, Y) &amp;= sum P(x, y) log frac{P(x,y)}{P(x).P(y)} &amp;=H(X)- H(X mid Y) = H(Y) - H(Y mid X) end{aligned} $$From above equation the mutual information give insight about how far $X$ and $Y$ from being independent from each other. Mutual information can be used in feature selection instead of correlation as it capture both linear and non linear dependency. . Kullback-leibler Divergence . Kullback-leibler Divergence measure how one probability distribution diverge from the other. Given two probability distribution $P(x)$ and $Q(X)$ where the former is the modelled/estimated distribution and the later is the actual/expected distribution. The KL divergence is defined as . $$ begin{aligned} D_{KL}(P||Q) &amp; = mathbb{E}_{x sim P} [ log frac{P(x)}{Q(x)}] &amp; = mathbb{E}_{x sim P}[ log P(x)] - mathbb{E}_{x sim P}[ log Q(x)] end{aligned} $$For discrete random distribution . $$ D_{KL}(P||Q) = sum_{i} P(x_i) log frac{P(x_i)}{Q(x_i)} $$And for continuous random variable . $$ D_{KL}(p||q) = int_{x} p(x) log frac{p(x)}{q(x)} $$KL divergence between $P$ and $Q$ tells how much information we lose when trying to approximate data given by $P$ with $Q$. It is non-negative $D_{KL}(P mid mid Q) geq 0$ and $0$ if $P$ and $Q$ are the same (distribution discrete) or equal almost anywhere in the case of continuous distribution. Apart from that KL divergence is not symmetric $D_{KL}(P mid mid Q) neq D_{KL}(P mid mid Q)$ because of this it is not a true distance measure. . Relation between KL divergence and Cross Entropy . $$ begin{aligned} D_{KL}(P||Q) &amp; = mathbb{E}_{x sim P} [ log frac{P(x)}{Q(x)}] &amp; = mathbb{E}_{x sim P}[ log P(x)] - mathbb{E}_{x sim P}[ log Q(x)] &amp; = H(P) - H(P, Q) end{aligned} $$where $ mathbb{E}_{x sim P}[ log P(x)] = H(P)$$ and $$ mathbb{E}_{x sim P}[ log Q(x)] = H(P, Q)$. Thus $H(P,Q) = H(P) - D_{KL}(P||Q)$. This implies that minimizing cross entropy with respect to $Q$ is equivalent to minimizing the KL divergence. KL divergence is used in unsupervised machine learning technique like variational auto-encoder. The KL divergence is also used as objective function in variational bayesian method to find optimal value for approximating distribution. .",
            "url": "https://sambaiga.github.io/sambaiga/ml/ppl/2018/02/01/probabilities.html",
            "relUrl": "/ml/ppl/2018/02/01/probabilities.html",
            "date": " • Feb 1, 2018"
        }
        
    
  
    
        ,"post6": {
            "title": "Introduction to Machine Learning",
            "content": "Machine Learning . Introduction . Machine learning is a set of algorithms that automatically detect patterns in data and use the uncovered pattern to make inferences or predictions. It is a subfield of artificial intelligence that aims to enable computers to learn on their own. Any machine learning algorithms involve the baisc three steps: first you identify pattern from data, then you build (train) model that best explain the pattern and the world (unseen data) and lastly use the model to predict or do inference. The process of training (building) a model can be seen as a learning process where the model is exposed to new, unfamiliar data step by step. . Machine learning is an exciting and fast-moving field of computer science with many recent applications. Important applications where machine learning algorithms are regularly deployed includes: . Computer vision: Object Classification in Photograph, image captioning. | Speech recognition, Automatic Machine Translation. | Detecting anomalies (e.g. Security, credit card fraud) | Speech recognition. | Communication systemsref | Robots learning complex behaviors | Recommendations services like in Amazo or Netflix where intelligent machine learning algorithms analyze your activity and compare it to the millions of other users to determine what you might like to buy or binge watch nextref. | . Machine learning algorithms that learn to recognise what they see and hear are at the heart of Apple, Google, Amazon, Facebook, Netflix, Microsoft, etc. . Why Machine learning . For many problems such as recognizing people and objects and understanding human speech it’s difcult to program the correct behavior by hand. However with machine learning these taks are easier. Other reasons we might want to use machine learning to solve a given problem: . A system might need to adapt to a changing environment. For instance, spammers are constantly trying to figure out ways to trick our e-mail spam classifers, so the classifcation algorithms will need to constantly adapt. | A learning algorithm might be able to perform better than its human programmers. Learning algorithms have become world champions at a variety of games, from checkers to chess to Go. This would be impossible if the programs were only doing what they were explicitly told to. | We may want an algorithm to behave autonomously for privacy or fairness reasons, such as with ranking search results or targeting ads. | . Types of Machine Learning . Machine learning is usually divide into three major types: Supervised Learning, Unspervised Learning and . Supervised Learning: Supervised learning is where you have input variables x and an output variable y and you use an algorithm to learn the mapping function from the input to the outputref. For instance, if we’re trying to train a machine leearning algorithm to distinguish cars and trucks, we would collect images of cars and trucks, and label each one as a car or a truck. Supervised learning problems can be further grouped into regression and classification problems. . A regression problem: is when the output variable is a real value, such as “dollars” or “weight” e.g Linear regression and Random forest. | Classification: A classification problem is when the output variable is a category, such as “red” or “blue” or “disease” and “no disease” e.g Support vector machines, Random forest and logistic regression. Some popular examples of supervised machine learning algorithms are: | . Unspervised Learning :Unsupervised learning is where you only have input data (X) and no corresponding output variables.We just have a bunch of data, and want to look for patterns in the data. For instance, maybe we have lots of examples of patients with autism, and want to identify different subtypes of the condition.The most important types of unsupervised learning includes: . Distribution modeling where one has an unlabeled dataset (such as a collection of images or sentences), and the goal is to learn a probability distribution which matches the dataset as closely as possible. | Clustering where the aim is to discover the inherent groupings in the data, such as grouping customers by purchasing behavior. | . Reiforcement Learning: is learning best actions based on reward or punishment. It involves learning what actions to take in a given situation, based on rewards and penalties. Example a robot takes a big step forward, then falls. The next time, it takes a smaller step and is able to hold its balance. The robot tries variations like this many times; eventually, it learns the right size of steps to take and walks steadily. It has succeeded. . There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. Action is what an agent can do in each state. When a robot takes an action in a state, it receives a reward, a feedback from the environment. A reward can be positive or negative (penalties). . Typical ML task: Linear Regression . In regression, we are interested in predicting a scalar-valued target, such as the price of a stock. By linear, we mean that the target must be predicted as a linear function of the inputs. This is a kind of supervised learning algorithm; recall that, in supervised learning, we have a collection of training examples labeled with the correct outputs. Example applications of linear regression include weather forecasting, house pricing prediction, student performance (GPA) prediction just to mention a few. . Linear Regression: Formulating a learning problem . In order to formulate a learning problem mathematically, we need to define two things: a model (hypothesis)** and a *loss function. After defining model and loss function we solve an optimisation problem with the aim to find the model parameters that best fit the data. . Model (Hypothesis): It is the set of allowable hypotheses, or functions that compute predictions from the inputs. In the case of linear regression, the model simply consists of linear functions given by: . y=∑jwjxj+by = sum_j w_jx_j + by=j∑​wj​xj​+b . where www is the weights, and bbb is an intercept term, which we’ll call the bias. These two terms are called model parameters denoted as θ thetaθ. . Loss function: It defines how well the model fit the data and thus show how far off the prediction yyy is from the target ttt and given as: . L(y,t)=12(y−t)2 mathcal{L(y,t)} = frac{1}{2}(y - t)^2L(y,t)=21​(y−t)2 . Since the loss function show how far off the prediction is from the target for one data point. We also need to define a cost function. The cost function is simply the loss, averaged over all the training examples. . J(w1…wD,b)=1N∑i=1NL(y(i),t(i))=12N∑i=1N(y(i)−t(i))2=12N∑i=1N(∑jwjxj(i)+b−t(i)) begin{aligned} J (w_1 ldots w_D,b) &amp; = frac{1}{N} sum_{i=1}^N mathcal{L}(y^{(i)},t^{(i)}) &amp; = frac{1}{2N} sum_{i=1}^N (y^{(i)} - t^{(i)})^2 &amp;= frac{1}{2N} sum_{i=1}^N left( sum_j w_jx_j^{(i)} + b -t^{(i)} right) end{aligned}J(w1​…wD​,b)​=N1​i=1∑N​L(y(i),t(i))=2N1​i=1∑N​(y(i)−t(i))2=2N1​i=1∑N​(j∑​wj​xj(i)​+b−t(i))​ . In vectorized form: . J=12N∥y−t∥2=12N(y−t)T(y−t)wherey=wTx mathbf{J} = frac{1}{2N} lVert mathbf{y-t} lVert^2 = frac{1}{2N} mathbf{(y - t)^T(y-t)} quad text{where} quad mathbf{y = w^Tx}J=2N1​∥y−t∥2=2N1​(y−t)T(y−t)wherey=wTx . The python implementation of the cost function (vectorized) is shown below. . def loss(x, w, t): N, D = np.shape(x) y = np.matmul(x,w.T) loss = (y - t) return loss def cost(x,w, t): &#39;&#39;&#39; Evaluate the cost function in a vectorized manner for inputs `x` and targets `t`, at weights `w1`, `w2` and `b`. &#39;&#39;&#39; N, D = np.shape(x) return (loss(x, w,t) **2).sum() / (2.0 * N) . Combine our model and loss function, we get an optimization problem, where we are trying to minimize a cost function with respect to the model parameters θ thetaθ (i.e. the weights and bias). . Solving the optimization problem . We now want to find the choice of model parameters θw1…wD,b theta _{w_1 ldots w_D,b}θw1​…wD​,b​ that minimizes J(w1…wD,b)J (w_1 ldots w_D,b)J(w1​…wD​,b) as given in the cost function above.There are two methods which we can use: direct solution and gradient descent. . Direct Solution . One way to compute the minimum of a function is to set the partial derivatives to zero.For simplicity, let’s assume the model doesn’t have a bias term as shown in the equation below. . Jθ=12N∑i=1N(∑jwjxj(i)−t(i))J_ theta = frac{1}{2N} sum_{i=1}^N left( sum_j w_jx_j^{(i)} -t^{(i)} right)Jθ​=2N1​i=1∑N​(j∑​wj​xj(i)​−t(i)) . In vectorized form . J=12N∥y−t∥212N(y−t)T(y−t)wherey=wx mathbf{J} = frac{1}{2N} lVert mathbf{y-t} rVert ^2 frac{1}{2N} mathbf{(y - t)^T(y-t)} quad text{where} quad mathbf{y = wx}J=2N1​∥y−t∥22N1​(y−t)T(y−t)wherey=wx . For matrix differentiation we need the following results: . ∂Ax∂x=AT∂(xTAx)∂x=2ATx begin{aligned} frac{ partial mathbf{Ax}}{ partial mathbf{x}} &amp; = mathbf{A}^T frac{ partial ( mathbf{x}^T mathbf{Ax})}{ partial mathbf{x}} &amp; = 2 mathbf{A}^T mathbf{x} end{aligned}∂x∂Ax​​=AT∂x∂(xTAx)​=2ATx​ . Setting the partial derivatives of cost function in vectorized form to zero we obtain: . ∂J∂w=12N∂(wTxTxw−2tTwx+tTt)∂w=12N(2xTxw−2xTt)w=(xTx)−1xTt begin{aligned} frac{ partial mathbf{J}}{ partial mathbf{w}} &amp; = frac{1}{2N} frac{ partial left( mathbf{w^Tx^Tx w} -2 mathbf{t^Twx} + mathbf{t^Tt} right)}{ partial mathbf{w}} &amp;= frac{1}{2N} left(2 mathbf{x}^T mathbf{xw} -2 mathbf{x}^T mathbf{t} right) mathbf{w} &amp;= ( mathbf{x^Tx})^{-1} mathbf{x^Tt} end{aligned}∂w∂J​w​=2N1​∂w∂(wTxTxw−2tTwx+tTt)​=2N1​(2xTxw−2xTt)=(xTx)−1xTt​ . In python this result can be implemented as follows: . def directMethod(x, t): &#39;&#39;&#39; Solve linear regression exactly. (fully vectorized) Given `x` - NxD matrix of inputs `t` - target outputs Returns the optimal weights as a D-dimensional vector &#39;&#39;&#39; N, D = np.shape(x) A = np.matmul(x.T, x) c = np.dot(x.T, t) return np.matmul(linalg.inv(A), c) . Gradient Descent . The optimization algorithm commonly used to train machine learning is the gradient descent algorithm. It works by taking the derivative of the cost function JJJ with respect to the parameters at a specific position on this cost function, and updates the parameters in the direction of the negative gradient. The entries of the gradient vector are simply the partial derivatives with respect to each of the variables: . ∂J∂w=(∂J∂w1⋮∂J∂wD) frac{ partial mathbf{J}}{ partial mathbf{w}} = begin{pmatrix} frac{ partial J}{ partial w_1} vdots frac{ partial J}{ partial w_D} end{pmatrix}∂w∂J​=⎝⎜⎜⎛​∂w1​∂J​⋮∂wD​∂J​​⎠⎟⎟⎞​ . The parameter w mathbf{w}w is iteratively updated by taking steps proportional to the negative of the gradient: . wt+1=wt−α∂J∂w=wt−αNxT(y−t) mathbf{w_{t+1}} = mathbf{ w_t }- alpha frac{ partial mathbf{J}}{ partial mathbf{w}} = mathbf{w_t} - mathbf{ frac{ alpha}{N}x^T(y-t)}wt+1​=wt​−α∂w∂J​=wt​−Nα​xT(y−t) . In coordinate systems this is equivalent to: . wt+1=wt−α1N∑i=1Nxt(y(i)−t(i))w_{t+1} = w_t - alpha frac{1}{N} sum_{i=1}^{N} x_t (y^{(i)}-t^{(i)})wt+1​=wt​−αN1​i=1∑N​xt​(y(i)−t(i)) . The python implementation of gradient descent is shown below: . def getGradient(x, w, t): N, D = np.shape(x) gradient = (1.0/ float(N)) * np.matmul(np.transpose(x), loss(x,w,t)) return gradient def gradientDescentMethod(x, t, alpha=0.1, tolerance=1e-2): N, D = np.shape(x) #w = np.random.randn(D) w = np.zeros([D]) # Perform Gradient Descent iterations = 1 w_cost = [(w, cost(x,w, t))] while True: dw = getGradient(x, w, t) w_k = w - alpha * dw w_cost.append((w, cost(x, w, t))) # Stopping Condition if np.sum(abs(w_k - w)) &lt; tolerance: print (&quot;Converged.&quot;) break if iterations % 100 == 0: print (&quot;Iteration: %d - cost: %.4f&quot; %(iterations, cost(x, w, t))) iterations += 1 w = w_k return w, w_cost . Generalization . The goal of a learning algorithm is not to only to make correct predictions on the training examples; but it should be generalized to examples not seen seen before. The average squared error on novel examples is known as the generalization error, and we’d like this to be as small as possible. In practice, we nor- mally tune model parameters by partitioning the dataset into three different subsets: . The training set is used to train the model. | The validation set is used to estimate the generalization error of each hyperparameter setting. | The test set is used at the very end, to estimate the generalization error of the final model, once all hyperparameters have been chosen. | .",
            "url": "https://sambaiga.github.io/sambaiga/ml/2017/04/12/ml-intro.html",
            "relUrl": "/ml/2017/04/12/ml-intro.html",
            "date": " • Apr 12, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Anthony Faustine is a Data Scientist at CeADAR (UCD), Dublin, Ireland with over four years of successful experience in data analytics and Artificial Intelligence techniques for multiple applications. He effectively researches techniques for novel approaches to problems and develops prototypes to assess their viability. Although Anthony is a person who takes the initiative, he has a strong team-work spirit with experience of working in a highly international environment. . At CeADER, Anthony is devising and implementing data analytics/AI technical solutions for multiple application domains. He is also involved in the research and development of the applicability of Artificial Intelligence for Earth Observation (AI4EO). . Mr Faustine, received the B.sc. Degree in Electronics Science and Communication from the University of Dar es Salaam, Tanzania, and the M.sc. Degree in Telecommunications Engineering from the University of Dodoma, Tanzania, in 2010. From 2010 to 2017, he worked as an assistant lecturer at the University of Dodoma, Tanzania, where he was involved in several research projects within the context of ICT4D. . In 2017, Anthony joined IDLab, imec research group of the University of Ghent, in Belgium as a Ph.D. Machine learning researcher advised by Tom Dhaene and Dirk Deschrijver. His research focused on machine learning techniques applied to energy smart-meter data. He develops methods to identify active appliances and extract their corresponding power consumption from aggregate power (energy-disaggregation) in residential and industrial buildings. . His research interests lie in the intersections between Computational Sustainability and Artificial Intelligence. He works towards bridging the gap between research and real-world applicability of machine learning for sustainable development. .",
          "url": "https://sambaiga.github.io/sambaiga/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
      ,"page4": {
          "title": "Projects",
          "content": "",
          "url": "https://sambaiga.github.io/sambaiga/project/",
          "relUrl": "/project/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Resources",
          "content": "",
          "url": "https://sambaiga.github.io/sambaiga/resources.html",
          "relUrl": "/resources.html",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
      ,"page10": {
          "title": "Talk",
          "content": "",
          "url": "https://sambaiga.github.io/sambaiga/talks/",
          "relUrl": "/talks/",
          "date": ""
      }
      
  

  
  

  
  

  

  
  

}