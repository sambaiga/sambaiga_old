{
  
    
        "post0": {
            "title": "Vectorization and Distribution shapes in Pyro",
            "content": "Introduction . In the previous post we introduced pyro and its building blocks such as schotastic function, primitive sample and param primitive statement, model and guide. We also defined pyro model and use it to generate data, learn from data and predict future observations. . In this section, we will learn in details about inference in Pyro, how to use Pyro primitives and the effect handling library (pyro.poutine) to build custom tools for analysis. . Consider a previous poison regression model . import torch import pyro import pyro.distributions as dist from torch.distributions import constraints import matplotlib.pyplot as plt import seaborn as sns import numpy as np pyro.set_rng_seed(101) torch.manual_seed(101) %matplotlib inline . def model_(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) for t in range(len(y)): rate = torch.exp(intercept + slope * t) y[t] = pyro.sample(&quot;count_{}&quot;.format(t), dist.Poisson(rate), obs=y[t]) . Plate statement . From the given model above , pyro.param designate model parameters that we would like to optimize. Observations are denoted by the obs= keyword argument to pyro.sample. This specifies the likelihood function. Instead of log transforming the data, we use a LogNormal distribution. The observations are conditionally independent given the latent random variable slope and intercept. To explicitly mark this in Pyro, plate statement is used to construct conditionally independent sequences of variables. . with pyro.plate(&quot;name&quot;, size, subsample_size, device) as ind: # ...do conditionally independent stuff with ind... . However compared to range() each invocation of plate requires the user to provide a unique name. The plate statement can be used either sequentially as a generator or in parallel as a context manager. Sequential plate is similar to range()in that it generates a sequence of values. . # This version declares sequential independence and subsamples data: for i in plate(&#39;data&#39;, 100, subsample_size=10): if z[i]: # Control flow in this example prevents vectorization. obs = sample(&#39;obs_{}&#39;.format(i), dist.Normal(loc, scale), obs=data[i]) . Vectorized plate is similar to torch.arange() in that it yields an array of indices by which other tensors can be indexed. However, unlike torch.arange() plate also informs inference algorithms that the variables being indexed are conditionally independent. . # This version declares vectorized independence: with plate(&#39;data&#39;): obs = sample(&#39;obs&#39;, dist.Normal(loc, scale), obs=data) . Additionally, plate can take advantage of the conditional independence assumptions by subsampling the indices and informing inference algorithms to scale various computed values. This is typically used to subsample minibatches of data: . with plate(&quot;data&quot;, len(data), subsample_size=100) as ind: batch = data[ind] assert len(batch) == 100 . You can additionally nest plates, e.g. if you have per-pixel independence: . with pyro.plate(&quot;x_axis&quot;, 320): # within this context, batch dimension -1 is independent with pyro.plate(&quot;y_axis&quot;, 200): # within this context, batch dimensions -2 and -1 are independent . Finaly you can declare multiple plates and use them as reusable context managers. For example if you want to mix and match plates for e.g. noise that depends only on x, some noise that depends only on y, and some noise that depends on both . x_axis = pyro.plate(&quot;x_axis&quot;, 3, dim=-2) y_axis = pyro.plate(&quot;y_axis&quot;, 2, dim=-3) with x_axis: # within this context, batch dimension -2 is independent with y_axis: # within this context, batch dimension -3 is independent with x_axis, y_axis: # within this context, batch dimensions -3 and -2 are independent . def model_(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) with pyro.plate(&#39;N&#39;, len(y)) as t: log_y_hat = slope * t.type(torch.float) + intercept y=pyro.sample(&#39;y&#39;, dist.LogNormal(log_y_hat, 1.), obs=y) . Distribution shapes . Unlike PyTorch Tensors which have a single .shape attribute, pyro Distributions have two shape batch_shape and event_shape. These two combine to define the total shape of a sample. The batch_shape denote conditionally independent random variables, whereas .event_shape denote dependent random variables (ie one draw from a distribution). Because the dependent random variables define probability together, the .log_prob() method only produces a single number for each event of shape .event_shape. . d = dist.Bernoulli(0.5) print(d.batch_shape) print(d.event_shape) . torch.Size([]) torch.Size([]) . x = d.sample() x.shape . torch.Size([]) . Distributions can be batched by passing in batched parameters. . d = dist.Bernoulli(0.5*torch.ones(50)) print(d.batch_shape) print(d.event_shape) . torch.Size([50]) torch.Size([]) . x = d.sample() x.shape . torch.Size([50]) . From the two examples above, we observe that univariate distributions have empty event shape (because each number is an independent event). Let also consider multivariate distribution. . md = dist.MultivariateNormal(torch.zeros(3), torch.eye(3)) print(md.batch_shape) print(md.event_shape) . torch.Size([]) torch.Size([3]) . y = md.sample() y.shape . torch.Size([3]) . We can also create batched multivariate distribution as follows. . md = dist.MultivariateNormal(torch.zeros(3), torch.eye(3)).expand([50]) print(md.batch_shape) print(md.event_shape) . torch.Size([50]) torch.Size([3]) . y = md.sample() y.shape . torch.Size([50, 3]) . Because Multivariate distributions have nonempty .event_shape, the shapes of .sample() and .log_prob(x) differ: . md.log_prob(y).shape . torch.Size([50]) . The Distribution.sample() method also takes a sample_shape parameter that indexes over independent identically distributed (iid) random varables, such that: . sample.shape == sample_shape + batch_shape + event_shape . y_sample =md.sample([10]) y_sample.shape . torch.Size([10, 50, 3]) . Reshaping distributions . You can treat a univariate distribution as multivariate by calling the .to_event(n) property where n is the number of batch dimensions (from the right) to declare as dependent. . d = dist.Bernoulli(0.5*torch.ones(50, 3)).to_event(1) print(d.batch_shape) print(d.event_shape) . torch.Size([50]) torch.Size([3]) . While working with distributions in pyro it is essential to note that: . Samples have shape batch_shape + event_shape, | .log_prob(x) values have shape batch_shape. | You’ll need to ensure that batch_shape is carefully controlled by either trimming it down with .to_event(n) or by declaring dimensions as independent via pyro.plate. | Often in Pyro we’ll declare some dimensions as dependent even though they are in fact independent. This allows us to easily swap in a MultivariateNormal distribution later, but aslo it simplifies the code as we don’t need a plate. Consider the following two codes . x = pyro.sample(&quot;x&quot;, dist.Normal(0, 1).expand([10]).to_event(1)) . x.shape . torch.Size([10]) . with pyro.plate(&quot;y_plate&quot;, 10): y = pyro.sample(&quot;y&quot;, dist.Normal(0, 1)) # .expand([10]) is automatic . y.shape . torch.Size([10]) . From the two code examples, the second version with plate informs Pyro that it can make use of conditional independence information when estimating gradients, whereas in the first version Pyro must assume they are dependent (even though the normals are in fact conditionally independent). .",
            "url": "https://sambaiga.github.io/sambaiga/jupyter/2020/03/04/ppl-pyro-two.html",
            "relUrl": "/jupyter/2020/03/04/ppl-pyro-two.html",
            "date": " • Mar 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Probabilistic Programming with Pyro",
            "content": "Intro to Pyro . Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. It enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling. . Models and Probability distributions . Models are the basic unit of probabilistic programs in pyro, they represent simplified or abstract descriptions of a process by which data are generated. Models in pyro are expressed as stochastic functions which implies that models can be composed, reused, imported, and serialized just like regular Python callables. Probability distributions (pimitive stochastic functions) are important class of models (stochastic functions) used explicitly to compute the probability of the outputs given the inputs. Pyro uses PyTorch’s distribution library which contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. Each probability distributions are equipped with several methods such as: . prob(): $ log p( mathbf{x} mid theta ^{*})$ | mean: $ mathbb{E}_{p( mathbf{x} mid theta ^{*})}[ mathbf{x}]$ | sample: $ mathbf{x}^{*} sim {p( mathbf{x} mid theta ^{*})}$ | . You can also create custom distributions using transforms. . Example 1: Let define the unit normal distribution $ mathcal{N}(0,1)$, draw sample $x$ and compute the log probability according to the distribution. . import torch import pyro import pyro.distributions as dist from torch.distributions import constraints import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pyro.set_rng_seed(101) torch.manual_seed(101) torch.set_printoptions(precision=3) %matplotlib inline . mu = 0 sigma = 1 normal=dist.Normal(mu, sigma) x = normal.rsample() # draw a sample from N(1,1) print(&quot;sample&quot;, x.item()) #To compute the log probability according to the distribution print(&quot;prob&quot;, torch.exp(normal.log_prob(x)).item()) # score the sample from N(1,1) . sample -1.3905061483383179 prob 0.15172401070594788 . Sample and Param statements . Pyro simplifies the process of sampling from distributions with the use of pyro.sample statement. The pyro.sample statement call stochastic functions or models with a unique name as identifier. Pyro’s backend uses these names to uniquely identify sample statements and change their behavior at runtime depending on how the enclosing stochastic function is being used. Using pyro.sample statement, Pyro can implement various manipulations that underlie inference algorithms. . x = pyro.sample(&quot;name&quot;, fn, obs) &quot;&quot;&quot; name – name of sample fn – distribution class or function obs – observed datum (optional; should only be used in context of inference) optionally specified in kwargs &quot;&quot;&quot; . Example 2: Let sample from previous normal distribution created in example 1. . mu = 0 sigma = 1 x = pyro.sample(&quot;my_sample&quot;, dist.Normal(mu, sigma)) print(x) . tensor(-0.815) . The above code generate a random value and records it in the Pyro runtime. . data=2 x = pyro.sample(&quot;my_sample&quot;, dist.Normal(mu, sigma), obs=data) print(x) . 2 . /opt/miniconda3/lib/python3.7/site-packages/pyro/primitives.py:86: RuntimeWarning: trying to observe a value outside of inference at my_sample RuntimeWarning) . The above code conditions a stochatsic function on observed data. This should run on inference. . Pyro use pyro.param statement to saves the variable as a parameter in the param store. To interact with the param store. The pyro.param statement is used by pyro to declares a learnable parameter. . x = pyro.param(&quot;name&quot;, init_value, constraints) &quot;&quot;&quot; name – name of param init_value – initial value constraint – torch constraint &quot;&quot;&quot; . Example 3: Let create theta parameter . theta = pyro.param(&quot;theta&quot;, torch.tensor(1.0), constraint=dist.constraints.positive) . Simple PPL model . Consider the following Poison Regression model begin{align} y(t) &amp; sim lambda exp(- lambda) lambda &amp; sim exp(c + m(t)) c &amp; sim mathcal{N}(1, 1) m &amp; sim mathcal{N}(0, 1) end{align} . def model(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) for t in range(len(y)): rate = torch.exp(intercept + slope * t) y[t] = pyro.sample(&quot;count_{}&quot;.format(t), dist.Poisson(rate), obs=y[t]) return slope, intercept, y . Given a pyro model. We can . Generate data from model | Learn parameters of the model from data | Use the model to predict future observation. | Generate data from model . Running a Pyro model will generate a sample from the prior. . pyro.set_rng_seed(0) # We pass counts = [None, ..., None] to indicate time duration. true_slope, true_intercept, true_counts = model([None] * 50) fig, ax = plt.subplots(figsize=(6,4)) ax = sns.lineplot(x=np.arange(len(true_counts)),y=[c.item() for c in true_counts]) . Learn parameters of the model from data . To learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data. Inference algorithms in Pyro us arbitrary stochastic functions as approximate posterior distributions. that s. These functions are called guide functions or guides and contains pyro.sample and pyro.param statement. It is a stochastic function that represents a probability distribution over the latent (unobserved) variables. The guide can be arbitrary python code just like the model, but with a few requirements: . All unobserved sample statements that appear in the model appear in the guide. | The guide has the same input signature as the model (i.e. takes the same arguments). | There are no pyro.sample statements with the obs keyword in the guide. These are exclusive to the model. | There are pyro.param statements, which are exclusive to the guide. These provide differentiation for the inputs to the pay_probs sample in the guide vs. the model. | For example if the model contains a random variable z_1 . def model(): pyro.sample(&quot;z_1&quot;, ...) . then the guide needs to have a matching sample statement . def guide(): pyro.sample(&quot;z_1&quot;, ...) . Once a guide has been specified, we can then perform learning and inference which is an optimization problem of maximizing the evidence lower bound (ELBO). The ELBO, is a function of both $ theta$ and $ phi$, defined as an expectation w.r.t. to samples from the guide: . $${ rm ELBO} equiv mathbb{E}_{q_{ phi}({ bf z})} left [ log p_{ theta}({ bf x}, { bf z}) - log q_{ phi}({ bf z}) right]$$The SVI class is unified interface for stochastic variational inference in Pyro. To use this class you need to provide: . the model, | the guide, and an | optimizer which is a wrapper a for a PyTorch optimizer as discusseced in below | . from pyro.infer import SVI, Trace_ELBO svi = SVI(model, guide, optimizer, loss=Trace_ELBO()) . The SVI object provides two methods, step() and evaluate_loss(), . The method step() takes a single gradient step and returns an estimate of the loss (i.e. minus the ELBO). | The method evaluate_loss() returns an estimate of the loss without taking a gradient step. | . Both of these methods accept an optional argument: num_particles, which denotes the number of samples used to compute the loss and gradient. . The module pyro.optim provides support for optimization in Pyro. In particular it provides PyroOptim, which is used to wrap PyTorch optimizers and manage optimizers for dynamically generated parameters. PyroOptim takes two arguments: . a constructor for PyTorch optimizers optim_constructor and | a specification of the optimizer arguments optim_args | . from pyro.optim import Adam adam_params = {&quot;lr&quot;: 0.005, &quot;betas&quot;: (0.95, 0.999)} optimizer = Adam(adam_params) . Thus to learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data (here true_counts). . For the above example we will use Autoguide pyro inference algorithm: . AutoLaplaceApproximation:Laplace approximation (quadratic approximation) approximates the posterior log𝑝(𝑧|𝑥) by a multivariate normal distribution in the unconstrained space. | Autodelta: This implementation of AutoGuide uses Delta distributions to construct a MAP guide over the entire latent space. | . from pyro.infer.autoguide import AutoDelta from pyro.infer import SVI, Trace_ELBO from pyro.optim import Adam guide = AutoDelta(model) svi = SVI(model, guide, Adam({&quot;lr&quot;: 0.1}), Trace_ELBO()) for i in range(101): loss = svi.step(true_counts) # true_counts is passed as argument to model() if i % 10 == 0: print(&quot;loss = {}&quot;.format(loss)) . loss = 87295.88946688175 loss = 64525.8595520854 loss = 80838.8460238576 loss = 33014.93229973316 loss = 13704.865498423576 loss = 6232.828522503376 loss = 2017.9879159331322 loss = 631.3558134436607 loss = 170.10323333740234 loss = 198.57187271118164 loss = 207.4590385556221 . print(&quot;true_slope = {}&quot;.format(true_slope)) print(&quot;true_intercept = {}&quot;.format(true_intercept)) guess = guide() print(&quot;guess = {}&quot;.format(guess)) . true_slope = 0.15409961342811584 true_intercept = -0.293428897857666 guess = {&#39;slope&#39;: tensor(0.147, grad_fn=&lt;ExpandBackward&gt;), &#39;intercept&#39;: tensor(-0.054, grad_fn=&lt;ExpandBackward&gt;)} . Use model to predict future observation . A third way to use a Pyro model is to predict new observed data by guiding the model. This uses two of Pyro&#39;s effects: . trace records guesses made by the guide, and | replay conditions the model on those guesses, allowing the model to generate conditional samples. | . Traces are directed graphs whose nodes represent primitive calls or input/output, and whose edges represent conditional dependence relationships between those primitive calls. It return a handler that records the inputs and outputs of primitive calls and their dependencies. . We can record its execution using trace and use the resulting data structure to compute the log-joint probability of all of the sample sites in the execution or extract all parameters. . trace = pyro.poutine.trace(model).get_trace([]) pprint({ name: { &#39;value&#39;: props[&#39;value&#39;], &#39;prob&#39;: props[&#39;fn&#39;].log_prob(props[&#39;value&#39;]).exp() } for (name, props) in trace.nodes.items() if props[&#39;type&#39;] == &#39;sample&#39; }) . {&#39;intercept&#39;: {&#39;prob&#39;: tensor(0.250), &#39;value&#39;: tensor(-0.966)}, &#39;slope&#39;: {&#39;prob&#39;: tensor(2.818), &#39;value&#39;: tensor(0.083)}} . print(trace.log_prob_sum().exp()) . tensor(0.705) . Here, the trace feature will collect values every time they are sampled with sample and store them with the corresponding string name (that’s why we give each sample a name). With a little cleanup, we can print out the value and probability of each random variable’s value, along with the joint probability of the entire trace. . Replay return a callable that runs the original, reusing the values at sites in trace at those sites in the new trace. makes sample statements behave as if they had sampled the values at the corresponding sites in the trace . from pyro import poutine def forecast(forecast_steps=10): counts = true_counts + [None] * forecast_steps # observed data + blanks to fill in guide_trace = poutine.trace(guide).get_trace(counts) _, _, counts = poutine.replay(model, guide_trace)(counts) return counts . We can now call forecast() multiple times to generate samples. . for _ in range(1): full_counts = forecast(10) forecast_counts = full_counts[len(true_counts):] plt.plot([c.item() for c in full_counts], &quot;r&quot;, label=None if _ else &quot;forecast&quot;, alpha=0.3) plt.plot([c.item() for c in true_counts], &quot;k-&quot;, label=&quot;truth&quot;) plt.legend(); . References . Pyro-ducomentation | PPL models for timeseries forecasting |",
            "url": "https://sambaiga.github.io/sambaiga/jupyter/2020/03/01/ppl-pyro-intro.html",
            "relUrl": "/jupyter/2020/03/01/ppl-pyro-intro.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Stochastic Variational Inference (SVI)",
            "content": "Introduction . The previous post introduced the basic principle of Variational Inference (VI) as one of the approach used to approximate difficult probability distribution, derived the ELBO function and discussed about Mean Field Variational Inference (MFVI) and the Coordinate Ascent Variational Inference (CAVI) algorithms. This post introduce another stochastic gradient based algorithm (SVI) used in practise to do VI under mean filed assumptions. It also present two important tricks re-parametrization trick and amortized inference that are useful when using SVI in solving problems. . Stochastic Variational Inference (SVI) . Consider the graphical model of the observations $ mathbf{x}$ and latent variable $ mathbf{z}={ theta, z}$ in figure 1 where $ theta$ is the global variable and $z = {z_1, ldots z_n}$ is the local (per-data-point) variable such that: . . p(x,z)=p(θ∣α)∏i=1Np(xi∣zi,θ)⋅p(zi∣α)p( mathbf{x}, mathbf{z}) = p( theta| alpha) prod_{i=1}^N p(x_i|z_i, theta) cdot p(z_i| alpha)p(x,z)=p(θ∣α)i=1∏N​p(xi​∣zi​,θ)⋅p(zi​∣α) . Similarly the variational parameters are given by λ={γ,ϕ} lambda = { gamma, phi }λ={γ,ϕ} where the variational parameter γ gammaγ correspond to latent variable and ϕ phiϕ denote set of local variational parameters. The variational distribution q(z∣ϕ)q( mathbf{z} mid phi)q(z∣ϕ) is given by . q(z∣ϕ)=q(θ∣γ)∏i=1Nq(zi∣ϕi,α)q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i| phi_i, alpha)q(z∣ϕ)=q(θ∣γ)i=1∏N​q(zi​∣ϕi​,α) . which also depend on hyper-parameter α alphaα. The ELBO of this graphical model LVI(q)=Eq[log⁡p(x,z,α)−log⁡q(z,γ)] mathcal{L}_{VI}(q) = mathbb{E}_q[ log p( mathbf{x}, mathbf{z}, alpha) - log q( mathbf{z}, gamma)]LVI​(q)=Eq​[logp(x,z,α)−logq(z,γ)] has the following form: . begin{split} mathcal{L}{VI}(q) &amp;= mathbb{E}_q[ log p( theta| alpha)- log q( theta| gamma)] &amp;+ sum{i=1}^{N} mathbb{E}_q[ log p(z_i| theta) . log p(x_i|z_i, theta)- log q(z_i| phi_i)] end{split}&lt;/span&gt; | . The equation above could be optimized by CAVI algorithm discussed in previous post which is expensive for large data sets. The CAVI algorithm scales with NNN as it require to optimize the local variational parameters for each data point before re-estimating the global variational parameters. . Unlike CAI, SVI uses stochastic optimization to fit the global variational parameters by repeatedly sub-sample the data to form stochastic estimate of ELBO. In every iteration one randomly selects mini-batches of size bszb_{sz}bsz​ to obtain a stochastic estimate of ELBO. . begin{split} mathcal{L}{VI}(q) &amp;= mathbb{E}_q[ log p( theta| alpha)- log q( theta| gamma)] &amp;+ frac{N}{b{sz}} sum_{s=1}^{b_{sz}} mathbb{E}q[ log p(z{i_s}| theta) + log p(x_{i_s}|z_{i_s}, theta)- log q(z_{i_s}| phi_{i_s})] end{split} . SVI algorithms follow noisy estimates of the gradient with a decreasing step size which is often cheaper to compute than the true gradient. Following such noisy estimates allows SVI to escape shallow local optima of complex objective functions. . Natural Gradient for SVI . To solve the optimization problem standard gradient-based methods such as SGD, Adam or Adagrad can be used. However, for SVI these gradient based methods cause slow convergence or converge to inferior local models. This is because, gradient based methods use the following update . θt+1=θt+α∂LVI(q)∂θ theta^{t+1}= theta^t + alpha frac{ partial mathcal{L}_{VI}(q)}{ partial theta}θt+1=θt+α∂θ∂LVI​(q)​ . where . ∂LVI(q)∂θ=∂LVI(q)∂θ1,…∂LVI(q)∂θk frac{ partial mathcal{L}_{VI}(q)}{ partial theta} = frac{ partial mathcal{L}_{VI}(q)}{ partial theta_1}, ldots frac{ partial mathcal{L}_{VI}(q)}{ partial theta_k}∂θ∂LVI​(q)​=∂θ1​∂LVI​(q)​,…∂θk​∂LVI​(q)​ . is the the gradient vector which point in the direction where the function increases most quickly while the changes in the function are measured with respect to euclidean distance. As the result, if the euclidean distance between the variational parameter being optimized is not good measure of variation in objective function then gradient descent will move suboptimal through the parameter value. . Consider the following two set of gausian distributions {d(1)=N(−2,3),d(2)=N(2,3)} {d_{(1)}= mathcal{N}(-2, 3), d_{(2)}= mathcal{N}(2, 3) }{d(1)​=N(−2,3),d(2)​=N(2,3)} and {d(1)=N(−2,1),d(2)=N(2,1)} {d_{(1)}= mathcal{N}(-2, 1), d_{(2)}= mathcal{N}(2, 1) }{d(1)​=N(−2,1),d(2)​=N(2,1)}. . . The euclidean distance between the two distributions d=(μ1−μ2)2+(σ12−σ22)2=4d_{}= sqrt{( mu_1- mu_2)^2+ ( sigma^2_1- sigma^2_2)^2}=4d​=(μ1​−μ2​)2+(σ12​−σ22​)2​=4. It clear that, considering only the euclidean distance the two images are the same. However, when we consider the shape of the distribution, the distance is different in the first and second image. In the first image, the KL-divergence should be lower as there is more overlap between between the two distribution unlike the second image where their support barely overlap. The reason for this difference is that probability distribution do not naturally fit in euclidean space rather it fit on a statistical manifold also called Riemannian manifold. . Statistical manifold give a natural way of measuring distances between distribution that euclidean distance use in SGD. A common Riemannian metric for statistical manifold is the fisher information matrix defined by . Fλ=Ep(x;λ)[∇log⁡p(x;λ)(∇log⁡p(x;θ))T]F_{ lambda} = mathbb{E}_{p(x; lambda)}[ nabla log p(x; lambda) ( nabla log p(x; theta))^T ]Fλ​=Ep(x;λ)​[∇logp(x;λ)(∇logp(x;θ))T] . It can be showed that the fisher information matrix FλF_{ lambda}Fλ​ is the second derivative of the KL divergence between two distributions. . Fθ=∇θ2KL(q(x;λ)∣∣p(x;θ))F_{ theta} = nabla^2_{ theta} KL(q(x; lambda)||p(x; theta))Fθ​=∇θ2​KL(q(x;λ)∣∣p(x;θ)) . Thus for SVI, the standard gradients descent techniques can be replaced with the natural gradient as follows: . ∇q~L(q)=F−1∇qLVI(q) tilde{ nabla_{q}} mathcal{L}(q) = F^{-1} nabla{q} mathcal{L}_{VI}(q)∇q​~​L(q)=F−1∇qLVI​(q) . The update procedure for natural gradient can be summarized as follows: . Compute the loss LVI(q) mathcal{L}_{VI}(q)LVI​(q) | Compute the gradient of the loss ∇qLVI(q) nabla{q} mathcal{L}_{VI}(q)∇qLVI​(q) | Compute the Fisher Information Matrix F. | Compute the natural gradient ∇q~LVI(q) tilde{ nabla_{q}} mathcal{L}_{VI}(q)∇q​~​LVI​(q) | Update the parameter qt+1=qt−α∇θ~LVI(q)q^{t+1} =q^t - alpha tilde{ nabla_{ theta}} mathcal{L}_{VI}(q)qt+1=qt−α∇θ​~​LVI​(q) | Using natural gradient instead of standard gradients simplify SVI gradient update. However the same conditions for convergence as standard SDG have to be fulfilled. First, the mini-batch indices must be drawn uniformly at random size where the size bszb_{sz}bsz​ of the mini-batch must satisfies 1≤bsz≤N1 leq b_{sz} leq N1≤bsz​≤N The learning rate α alphaα needs to decrease with iterations ttt satisying the Robbins Monro conditions ∑t=1∞αt=∞ sum_{t=1}^{ infty} alpha_t = infty∑t=1∞​αt​=∞ and ∑t=1∞αt2&lt;∞ sum_{t=1}^{ infty} alpha_t^2 &lt; infty∑t=1∞​αt2​&lt;∞ This guarantee that every point in the parameter space can be reached while the gradient noise decreases quickly enough to ensure convergence. . The next section presents two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems. . Re-parametrization trick . Consider the graphical model presented in figure 1, where gradient based stochastic optimization is used to learn the variational parameter ϕ phiϕ. For example; for Gaussian distribution . qϕ(z∣x)=N(μϕ(x),Σϕ(x))q_{ phi}(z|x)= mathcal{N}( mu_{ phi}(x), Sigma_{ phi}(x))qϕ​(z∣x)=N(μϕ​(x),Σϕ​(x)) . to maximize the likelihood of the data, we need to back propagate the loss to the parameter ϕ phiϕ across the distribution of zzz or across sample z∼qϕ(z∣x)z sim q_ phi(z mid x)z∼qϕ​(z∣x) However, it is difficulty to back-propagate through random variable. To address this problem, the re-parametrization trick is used.First let consider the Law of the Unconscious Statistician (LOTUS), used to calculate the expected value of a function g(ϵ)g( epsilon)g(ϵ) of a random variable ϵ epsilonϵ when only the probability distribution p(ϵ)p( epsilon)p(ϵ) of ϵ epsilonϵ is known. It state that: . To compute the expectation of a measurable function g(.)g(.)g(.) of a random variable ϵ epsilonϵ, we have to integrate g(ϵ)g( epsilon)g(ϵ) with respect to the distribution function of ϵ epsilonϵ, that is: . E(g(ϵ))=∫g(ϵ)dFϵ(ϵ) mathbb{E}(g( epsilon)) = int g( epsilon)dF_{ epsilon}( epsilon)E(g(ϵ))=∫g(ϵ)dFϵ​(ϵ) . In other words, to compute the expectation of z=g(ϵ)z =g( epsilon)z=g(ϵ) we only need to know g(.)g(.)g(.) and the distribution of ϵ epsilonϵ. We do not need to explicitly know the distribution of zzz. Thus the above equation can be expression in the convenient alternative notation: . Eϵ∼p(ϵ)(g(ϵ))=Ez∼p(z)(z) mathbb{E}_{ epsilon sim p( epsilon)}(g( epsilon)) = mathbb{E}_{z sim p(z)} (z)Eϵ∼p(ϵ)​(g(ϵ))=Ez∼p(z)​(z) . Therefore the reparameteriztaion trick states that: . A random variable zzz with distribution qϕ(z,ϕ)q_{ phi}(z, phi)qϕ​(z,ϕ) which is independent to ϕ phiϕ can be expressed as transformation of random variable ϵ∼p(ϵ) epsilon sim p( epsilon)ϵ∼p(ϵ) that come from noise distribution such as uniform or gaussian such that z=g(ϕ,ϵ)z = g( phi, epsilon)z=g(ϕ,ϵ) . For instance for Gaussian variable zzz in the above example z=μ(ϕ)+σ2(ϕ)⋅ϵz = mu( phi) + sigma^2( phi) cdot epsilonz=μ(ϕ)+σ2(ϕ)⋅ϵ where ϵ∼N(0,1) epsilon sim mathcal{N}(0, 1)ϵ∼N(0,1) . Since p(ϵ)p( epsilon)p(ϵ) is independent of the parameter of qϕ(z,ϕ)q_{ phi}(z, phi)qϕ​(z,ϕ), we can apply the change of variables in integral theory to compute any expectation over zzz or any expectation over ϕ phiϕ. The SDG estimator can therefore be estimated by pulling the gradient into expectations and approximating it by samples from the noise distribution such that for any measurable function fθ(.)f_{ theta}(.)fθ​(.): . ΔϕEz∼pϕ(z)=1M∑i=1MΔf(g(ϕ,ϵi)) Delta_{ phi} mathbb{E}_{z sim p_{ phi}(z)} = frac{1}{M} sum_{i=1}^M Delta f(g( phi, epsilon_i))Δϕ​Ez∼pϕ​(z)​=M1​i=1∑M​Δf(g(ϕ,ϵi​)) . where ϵi∼p(ϵ) epsilon_i sim p( epsilon)ϵi​∼p(ϵ) , fθ(.)f_{ theta}(.)fθ​(.) must be differentiable w.r.t its input zzz and g(ϕ,ϵi)g( phi, epsilon_i)g(ϕ,ϵi​) must exist and be differentiable with respect to ϕ phiϕ. . Amortized Variational Inference . Consider the graphical model presented in figure 1 where ecah data point xix_ixi​ is governed by its latent variable ziz_izi​ with variational parameter phiiphi_iphii​ such that . q(z∣ϕ)=q(θ∣γ)∏i=1Nq(zi∣ϕi,α)q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i| phi_i, alpha)q(z∣ϕ)=q(θ∣γ)i=1∏N​q(zi​∣ϕi​,α) . Using traditional SVI make it necessary to optimize ϕi phi_iϕi​ for each data point xix_ixi​. As the results the number parameters to be optimized will grows with the number of observations xxx. This is not ideal for larger datasets. Apart from that, it requires one to re-run the optimization procedure in case of new observation or when we have to perform inference. To address these problem amortized VI introduce a parametrized function that maps from observation space to the parameter of the approximate posterior distribution. . Amortized VI try to learn from past inference/pre-computation so that future inferences run faster. Instead of approximating separate variables for each data point xix_ixi​, amortized VI assume that the local variational parameter ϕ phiϕ can be predicted by a parametrized function fϕ(.)f_{ phi}(.)fϕ​(.) of data whose parameters are shared across all data points. Thus instead of introducing local variational parameter, we learn a single parametric function and work with a variational distribution that has the form . q(z∣ϕ)=q(θ∣γ)∏i=1Nq(zi∣fϕ(.))q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i|f_{ phi}(.))q(z∣ϕ)=q(θ∣γ)i=1∏N​q(zi​∣fϕ​(.)) . where fϕ(.)f_{ phi}(.)fϕ​(.) is the deep neural net function of zzz . Deep neural network used in this context are called inference networks. Therefore amortized inference with inference networks combines probabilistic modelling with representation power of deep learning. Using amortized VI instead of traditional VI, has two important advantages. First the number of variational parameters remain constant with respect to the data size. We only need to specify the parameter of the neural networks which is independent to the number of observations. Second, for new observation or during inference all we need to do is to call the inference network. As the result, we can invest time upfront optimizing the inference network and during inference we use the trained network for fast inference. . Reference . [Cheng Zhang,(2017)]: Advances in Variational Inference. | [Daniel Ritchie,(2016)]:Deep Amortized Inference for Probabilistic Programs. | [Andrew Miller,(2016)]:Natural Gradients and Stochastic Variational Inference. | Shakir Mohamed:Variational Inference for Machine Learning. | DS3 workshop:Approximate Bayesian Inference: Old and New. | Variational Inference and Deep Generative Models:Variational Inference for NLP audiences |",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/probabilistic%20model/2018/06/08/stochastic-vi.html",
            "relUrl": "/machine%20learning/probabilistic%20model/2018/06/08/stochastic-vi.html",
            "date": " • Jun 8, 2018"
        }
        
    
  
    
        ,"post3": {
            "title": "Learning from probabilistic models",
            "content": "Introduction . Given some data $ mathbf{x}=[x_1 ldots x_m]$ that come from some probability density function characterized by an unknown parameter $ theta$. How can we find $ hat{ theta}$ that is the best estimator of $ theta$. For example suppose we have flipped a particular coin $ 100$ times and landed head $ N_H = 55$ times and tails $ N_T = 45$ times. We are interested to know what is the probability that it will come-up head if we flip it again. In this case the behavior of the coin can be summerized with parameter $ theta$ the probability that a flip land head (H), which in this case is independent and identically distributed Bernoulli distribution. The key question is, how do we find parameter $ hat{ theta}$ of this distribution that fits the data. This is called parameter estimation, in which three approaches can be used: . Maximum-Likehood estimation | Bayesian parameter estimation and | Maximum a-posterior approximation | Like-hood and log-likehood function . Before discussing the above learning approach, let firts define the like-hood function $L( theta)$ which is the probability of the observed data as function of $ theta$ given as: . $$ L( theta) = P(x_1, ldots x_m; theta) = prod_i^m P(x_i; theta) $$The like-hood function indicates how likely each value of the parameter is to have generated the data. In the case of coin example above, the like-lihood is the probability of particular seqeuence of H and T generated: . $$ L( theta) = theta ^{N_H}(1 - theta ^{N_T}) $$ . We also define the log-likelihood function $ mathcal{L}( theta)$ which is the log of the likelihood function $L( theta)$. . $$ begin{aligned} mathcal{L}( theta) &amp;= log L( theta) &amp; = log prod_i^m P(x_i; theta) &amp; = sum_i^M P(x_i; theta) end{aligned} $$For the above coin example the log-likelihood is . $$ mathcal{L}( theta)= N_H log theta + N_T log(1- theta) $$ Maximum-Likelihood Estimation . The main objective of maximum likelihood estimation (MLE) is to determine the value of $ theta$ that is most likely to have generated the vector of observed data, $ mathbf{x}$ where $ theta$ is assumed to be fixed point (point-estimation). MLE achieve this by finding the parameter that maximize the probability of the observed data. The parameter $ hat{ theta}$ is selected such that it maximize $ mathcal{L}( theta)$: . $ hat{ theta}= arg max_{ theta} mathcal{L}( theta) $ . For the coin example the MLE is : . $$ begin{aligned} frac{ partial mathcal{L}( theta)}{ partial theta} &amp; = frac{ partial }{ partial theta}(N_H log theta + N_T log(1- theta) &amp;= frac{N_H}{ theta} - frac{N_T}{1- theta} end{aligned} $$Set $ frac{ partial mathcal{L}( theta)}{ partial theta} = 0$ and solve for $ theta$ we obtain the MLE: . $ hat{ theta} = frac{N_H}{N_H + N_T} $ . which is simply the fraction of flips that cameup head. . Now suppose we are observing power-meta data which can be modelled as gaussian ditribution with mean $$ mu$$ and standard deviation $ sigma$. We can use MLE to estimate $ hat{ mu}$ and $ hat{ sigma}$. The log-likehood for gausian distribution is given as . $$ begin{aligned} mathcal{L}( theta) &amp;= sum_{i=1}^M log left[ frac{1}{ sqrt{2} pi sigma} exp frac{-(x_i - mu)}{2 sigma ^2} right] &amp; = - frac{M}{2} log 2 pi - M log sigma - frac{1}{2 sigma^2} sum_i^M (x_i - mu)^2 end{aligned} $$Let find $ frac{ partial mathcal{L}( theta)}{ partial mu} $ and $ frac{ partial mathcal{L}( theta)}{ partial sigma} $ and set equal to zero. . $$ begin{aligned} frac{ partial mathcal{L}( theta)}{ partial mu} &amp;= - frac{1}{2 sigma^2} sum_i^M frac{ partial}{ partial mu}(x_i - mu)^2 &amp; = sum_i^M (x_i - mu) = 0 &amp; Rightarrow hat{ mu} = frac{1}{M} sum_{i=1}^M x_i end{aligned} $$which is the mean of the observed values. Similary: . $$ begin{aligned} frac{ partial mathcal{L}( theta)}{ partial sigma} &amp;= frac{M}{ sigma} + frac{1}{ sigma^3} sum_i^M (x_i - mu)^2 &amp; Rightarrow hat{ sigma} = sqrt{ frac{1}{M} sum_{i=1}^M (x_i - mu)^2} end{aligned} $$In the two examples above we manged to obtain the exact maximum likelihood solution analytically. But this is not always the case, let’s consider how to compute the maximum likelihood estimate of the parameters of the gamma distribution, whose PDF is defined as: . $$ P(x) = frac{b^a}{ Gamma(a)}x^{x-1} exp(-bx) $$where $ Gamma (a)$ is the gamma function which is the generalization of the factorial function to continous values given as: . $ Gamma(t) = int_0^{- infty} x^{t-1} exp(-x) ,dx $ . The model parameters for gamma distribution is $a$ and $b$ both of which are $ geq 0$. the log-likelihood is therefore: . $ begin{aligned} mathcal{L}( a, b) &amp; = sum_{i=1}^M a log b - log Gamma (a) + (a -1) log x_i - bx_i &amp; = Ma log b - M log Gamma (a) + (a - 1) sum_{i=1}^M log x_i - b sum_{i=1}^M x_i end{aligned} $ . To get MLE we need employ gradient descent which consists of computing the derivatives: $ frac{ partial mathcal{L}}{ partial a} $ and $ frac{ partial mathcal{L}}{ partial b} $ and then updating; $ a_{k+1}= a_k + alpha frac{ partial mathcal{L}}{ partial a} $ and $ b_{k+1}= b_k + alpha frac{ partial mathcal{L}}{ partial b} $ . where $ alpha$ is the learning rate. . Limitation of MLE . Despite the fact that MLE is very powerful technique, it has a pitfall for little training data which can lead into seriously overfit. The most painful issue is when it assign a $0$ probability to items that were never seen in the training data but which still might actually happen. Take an example if we flipped a coin twice and $N_H = 2$, the MLE of $ theta$, the probability of H would be $1$. This imply that we are considering it impossible for the coin to come up T. This problem is knowas data sparsity. . Bayesian Parameter Estimation . Unlike MLE which treat only the observation $ mathbf{x}$ as random variable and the parameter $ theta$ as a fixed point, the bayesian approach treat the parameter $ theta $ as random varibale as well with some known prior distribution. Let define the model for joint distribution $$p( theta, mathcal{D})$$ over parameter $ theta$ and data $ mathcal{D}$. To further define this joint distribution we aslo need the following two distribution: . A distribution of $P( theta)$ knowas prior distribution which is the probability of paratemeter $ theta$ availabe beforehand, and before making any additional observations. It account for everything you believed about the parameter $ theta$ before observing the data. In practise choose prior that is computational convinient. . | The likelihood $P( mathcal{D} mid theta)$ which is the probability of data given the parameter like in maximum likelihood. . | . With this two distributions, we can compute the posterior distribution and the posterior predictive distribution. The posterior distribution $P( theta mid mathcal{D})$ which correspond to uncertainty about $ theta$ after observing the data given by: . $$ begin{aligned} P( theta mid mathcal{D}) &amp;= frac{P( theta)p( mathcal{D} mid theta)}{P( mathcal{D})} &amp;= frac{P( theta)P( mathcal{D} mid theta)}{ displaystyle int P( theta ^ { prime} ) P( mathcal{D} mid theta ^{ prime})} end{aligned} $$The denominator is usually considered as a normalizing constant and thus the posterior distribution become: . $$ P( theta mid mathcal{D}) propto P( theta)P( mathcal{D} mid theta) $$On the other hand the posterior predictive distribution $P( mathcal{D}^{ prime} mid) mathcal{D}$ is the distribution of future observation given past observation defined by: . $$ P( mathcal{D}^{ prime} mid mathcal{D} )= int P( theta mid mathcal{D}) P( mathcal{D}^{ prime} mid theta) $$Generaly the Bayesian approach to parameter estimation works as follows: . First we need to formulate our knowledge about a situation by defining a distribution model which expresses qualitative aspects of our knowledge about the situation and then specify a prior probability distribution which expresses our subjective beliefs and subjective uncertainty about the unknown parameters, before seeing the data. | Gather data | Obtain posterior knowledge that updates our beliefs by computing the posterior probability distribution which estimates the unknown parameters. | Let apply the bayesian estimation to the coin example in which we have specified the likelihood equal to $ theta^{N_H}(1- theta)^{N_T}$. We only required to specify the prior in which several approches can be used. One of the approach is relay upon lifetime experince of flipping coins in which most coins tend to be fair which implies $p( theta) = 0.5$. We can also use various distribution to specify prior density but in practise a most useful distribution is the beta distribution parameterized by $a , b &gt; 0$ and defined as: . $$ p( theta; a, b) = frac{ Gamma (a + b)}{ Gamma(a) Gamma (b)} theta ^{a-1}(1- theta ^{b - 1}) $$From the above eqution it is clear that the first term (with all $ Gamma$)is just a normalizing constant and thus we can rewrite the beta distribution as: . $$ p( theta; a, b) propto theta ^{a-1}(1- theta) ^{b - 1} $$Note the beta distribution has the following properties . It is centered around $ frac{a}{a + b}$ and it can be shown that if $ theta sim text{Beta}(a,b)$ then $ mathbb{E}( theta)= frac{a}{a + b}$. | It becomes more peaked for larger values of $a$ and $b$ | It become normal distribution when $a = b = 1$ | . Now let compute the posterior and posterior predictive distribution . $$ begin{aligned} p( theta | mathcal{D}) &amp; propto p( theta)p( mathcal{D} | theta) &amp; propto theta^{N_H}(1- theta)^{N_T} theta ^{a-1}(1- theta) ^{b - 1} &amp; = theta ^{a-1+N_H}(1- theta) ^{b - 1 + N_T} end{aligned} $$",
            "url": "https://sambaiga.github.io/sambaiga/jupyter/2018/04/02/probabilities-learning.html",
            "relUrl": "/jupyter/2018/04/02/probabilities-learning.html",
            "date": " • Apr 2, 2018"
        }
        
    
  
    
        ,"post4": {
            "title": "Basics of Probability and Information Theory",
            "content": "Introduction . Probability and Information theory are important field that has made significant contribution to deep learning and AI. Probability theory allows us to make uncertain statements and to reason in the presence of uncertainty where information theory enables us to quantify the amount of uncertainty in a probability distribution. . Probability Theory . Probability is a mathematical framework for representing uncertainty. It is very applicable in Machine learning and Artificial Intelligence as it allows to make uncertain statements and reason in the presence of uncertainty. Probability theory allow us to design ML algorithms that take into consideration of uncertain and sometimes stochastic quantities. It further tell us tell us how ML systems should reason in the presence of uncertainty. This is necessary because most things in the world are uncertain, and thus ML systems should reason using probabilistic rules. Probability theory can also be used to analyse the behaviour of ML algorithms probabilistically. Consider evaluating ML classification algorithm using accuracy metric which is the probability that the model will give a correct prediction given an example. . Probability and Probability distribution . Probability is a measure of the likelihood that an event will occur in a random experiment. It is quantified as number between 0 and 1. The mathematical function that maps all possible outcome of a random experiment with its associated probability it is called probability distribution. It describe how likely a random variable or set of random variable is to take on each of its possible state. The probability distribution for discrete random variable is called probability mass function (PMF) which measures the probability $X$ takes on the value $x$, denoted denoted as $P(X=x)$. To be PMF on random variable $X$ a function $P(X)$ must satisfy: . Domain of $P$ equal to all possible states of $X$ | $ forall x in X, 0 leq P(X=x) leq 1$ | $ sum_{x in X} P(x) =1$ | . Popular and useful PMF includes poison, binomial, bernouli, and uniform. Let consider a poison distribution defined as: . $$ P(X=x) = frac{ lambda ^x e^{ - lambda}}{x!} $$$ lambda &gt;0$ is called a parameter of the distribution, and it controls the distribution&#39;s shape. By increasing $ lambda$ , we add more probability to larger values, and conversely by decreasing $ lambda$ we add more probability to smaller values as shown in figure below. . Instead of a PMF, a continuous random variable has a probability density function (pdf) denoted as $f_X(x)$. An example of continuous random variable is a random variable with exponential density. $$ f_X(x mid lambda) = lambda ^x e^{ - lambda} text{, } x geq 0 $$ . To be a probability density function $p(x)$ must satisfy . The domain of $p$ must be the set of all possible state | $ forall x in X, f_X(x) geq 0$ | $ int_{x in X} f_X(x)dx =1$ | . The pdf does not give the probability of a specific state directly. The probability that $x$ is between two point $a, b$ is . $ int_{a}^b f_X(x)dx$ . The probability of intersection of two or more random variables is called joint probability denoted as $ P(X, Y) $ . Suppose we have two random variable $X$ and $Y$ and we know the joint PMF or pdf distribution between these variable. The PMF or pdf corresponding to a single variable is called marginal probability distribution defined as $$ P(x) = sum_{y in Y} P(x, y) $$ . for discrete random variable and $$ p(x) = int p(x)dy $$ . Marginalization allows us to get the distribution of variable $$X$$ ignoring variable $Y$ from the joint distribution $P(X,Y)$. The probability that some event will occur given we know other events is called condition probability denoted as $P(X mid Y)$. The marginal, joint and conditional probability are linked by the following rule $ P(X|Y) = frac{P(X, Y)}{P(Y)} $ . Independence, Conditional Independence and Chain Rule . Two random variables are said to be independent of each other if the probability that one random variables occur in no way affect the probability of the other random variable occurring. $X$ and $Y$ are said to be independent if $P(X,Y) = P(X) cdot P(Y)$ On the other hand two random variable $X$ and $Y$ are conditionally independent given an event $Z$ with $P(Z)&gt;0$ if . $$ P(X,Y mid Z) = P(X mid Y) cdot P(Y mid Z) $$The good example of conditional independence can be found on this link. Any joint probability distribution over many random variables may be decomposed into conditional distributions using chain rule as follows: . $$ P(X_1,X_2, ldots, X_n ) = P(X_1) prod_{i=2}^n P(X_i mid X_i, ldots X_{i-1}) $$Expectation, Variance and Covariance . Expected value of some function $f(x)$ with respect to a probability distribution $P(X)$ is the average or mean value that $f(x)$ takes on when $x$ is drawn from $P$. . $$ mathbb{E}_{x sim P}[f(x)] = sum P(x).f(x) $$for discrete random variable and . $$ mathbb{E}_{x sim P}[f(x)] = int P(x).f(x)dx $$Expectation are linear such that $$ mathbb{E}_{x sim P}[ alpha cdot f(x) + beta cdot g(x)] = alpha mathbb{E}_{x sim P}[f(x)] + beta mathbb{E}_{x sim P}[g(x)] $$ . Variance is a measure of how much the value of a function of random variable $X$ vary as we sample different value of $x$ from its probability distribution. $$ Var(f(x)) = mathbb{E}([f(x)- mathbb{E}[f(x)]^2]) $$ The square root of the variance is know as standard deviation. On the other hand the covarince give some sense of how much two value are linearly related to each other as well as the scale of these value. . $$ Cov(f(x), g(y)) = mathbb{E}[(f(x)- mathbb{E}[f(x)])(g(y)- mathbb{E}[g(y)])] $$ Information theory . Information theory deals with quantification of how much information is present in a signal. In context of machine learning, information theory we apply information theory to: characterize probability distributions and quantify similarities between probability distributions. The following are the key information concepts and their application to machine learning. . Entropy, Cross Entropy and Mutual information . Entropy give measure of uncertainty in a random experiment. It help us quantify the amount of uncertainty in an entire probability distribution. The entropy of a probability distribution is the expected amount of information in an event drawn from that distribution defined as. . $$ H(X) = - mathbb{E}_{x sim P}[ log P(x)] = - sum_{i=1}^n P(x_i)l log P(x_i) $$Entropy is widely used in model selection based on principle of maximum entropy. On the other hand, cross entropy is used to compare two probability distribution. It tell how similar two distribution are. The cross entropy between two probability distribution $P$ and $$Q$ defined over same set of outcome is given by . $$ H(P,Q)= - sum P(x) log Q(x) $$ . Cross entropy loss function is widely used in machine learning for classification problem. The mutual information over two random variables help us gain insight about the information that one random variable carries about the other. . $$ begin{aligned} I(X, Y) &amp;= sum P(x, y) log frac{P(x,y)}{P(x).P(y)} &amp;=H(X)- H(X mid Y) = H(Y) - H(Y mid X) end{aligned} $$From above equation the mutual information give insight about how far $X$ and $Y$ from being independent from each other. Mutual information can be used in feature selection instead of correlation as it capture both linear and non linear dependency. . Kullback-leibler Divergence . Kullback-leibler Divergence measure how one probability distribution diverge from the other. Given two probability distribution $P(x)$ and $Q(X)$ where the former is the modelled/estimated distribution and the later is the actual/expected distribution. The KL divergence is defined as . $$ begin{aligned} D_{KL}(P||Q) &amp; = mathbb{E}_{x sim P} [ log frac{P(x)}{Q(x)}] &amp; = mathbb{E}_{x sim P}[ log P(x)] - mathbb{E}_{x sim P}[ log Q(x)] end{aligned} $$For discrete random distribution . $$ D_{KL}(P||Q) = sum_{i} P(x_i) log frac{P(x_i)}{Q(x_i)} $$And for continuous random variable . $$ D_{KL}(p||q) = int_{x} p(x) log frac{p(x)}{q(x)} $$KL divergence between $P$ and $Q$ tells how much information we lose when trying to approximate data given by $P$ with $Q$. It is non-negative $D_{KL}(P mid mid Q) geq 0$ and $0$ if $P$ and $Q$ are the same (distribution discrete) or equal almost anywhere in the case of continuous distribution. Apart from that KL divergence is not symmetric $D_{KL}(P mid mid Q) neq D_{KL}(P mid mid Q)$ because of this it is not a true distance measure. . Relation between KL divergence and Cross Entropy . $$ begin{aligned} D_{KL}(P||Q) &amp; = mathbb{E}_{x sim P} [ log frac{P(x)}{Q(x)}] &amp; = mathbb{E}_{x sim P}[ log P(x)] - mathbb{E}_{x sim P}[ log Q(x)] &amp; = H(P) - H(P, Q) end{aligned} $$where $ mathbb{E}_{x sim P}[ log P(x)] = H(P)$$ and $$ mathbb{E}_{x sim P}[ log Q(x)] = H(P, Q)$. Thus $H(P,Q) = H(P) - D_{KL}(P||Q)$. This implies that minimizing cross entropy with respect to $Q$ is equivalent to minimizing the KL divergence. KL divergence is used in unsupervised machine learning technique like variational auto-encoder. The KL divergence is also used as objective function in variational bayesian method to find optimal value for approximating distribution. .",
            "url": "https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html",
            "relUrl": "/jupyter/2018/02/01/probabilities.html",
            "date": " • Feb 1, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ". Anthony Faustine is a Data Scientist at CeADAR (UCD), Dublin, Ireland with over four years of successful experience in data analytics and Artificial Intelligence techniques for multiple applications. He effectively researches techniques for novel approaches to problems and develops prototypes to assess their viability. Although Anthony is a person who takes the initiative, he has a strong team-work spirit with experience of working in a highly international environment. . At CeADER, Anthony is devising and implementing data analytics/AI technical solutions for multiple application domains. He is also involved in the research and development of the applicability of Artificial Intelligence for Earth Observation (AI4EO). . Mr Faustine, received the B.sc. Degree in Electronics Science and Communication from the University of Dar es Salaam, Tanzania, and the M.sc. Degree in Telecommunications Engineering from the University of Dodoma, Tanzania, in 2010. From 2010 to 2017, he worked as an assistant lecturer at the University of Dodoma, Tanzania, where he was involved in several research projects within the context of ICT4D. . In 2017, Anthony joined IDLab, imec research group of the University of Ghent, in Belgium as a Ph.D. Machine learning researcher advised by Tom Dhaene and Dirk Deschrijver. His research focused on machine learning techniques applied to energy smart-meter data. He develops methods to identify active appliances and extract their corresponding power consumption from aggregate power (energy-disaggregation) in residential and industrial buildings. . His research interests lie in the intersections between Computational Sustainability and Artificial Intelligence. He works towards bridging the gap between laboratory and real-world applicability of machine learning for sustainable development. .",
          "url": "https://sambaiga.github.io/sambaiga/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}