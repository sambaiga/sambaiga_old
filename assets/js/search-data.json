{
  
    
        "post0": {
            "title": "AI4EO-An Opportunity for Computation Sustainability",
            "content": "Introduction . Computational Sustainability focuses on developing computational models, methods, and tools to help policymakers design more effective solutions and policies for sustainable development. The advancement of Information and Communication Technologies (ICT), particularly Earth Observation (EO) and Artificial intelligence (AI) offer prospects of addressing sustainability challenges. A more in-depth explanation about the above project can be viewed in this video: . Earth observations (EO) are data and information about the planet’s physical, chemical, and biological systems. It involves the collection, analysis, and presentation about the status of, and changes in, the natural and human-made environment. The most common sources of EO data include drones, land stations, and satellites. While drones capture high-resolution images on a small scale, satellites generate growing amounts of multi-resolution and multi-bands imagery and other data sources for the whole Earth. These data could be used to create all kinds of different products so that businesses, scientists, policymakers, and even everyday citizens can understand the past, present, and future trends in the Earth systems. Figure below shows multiband imagery from satellites by the electromagnetic. . On the other hand, AI is an area of computer science devoted to developing systems that can learn (from data) to make decisions and predictions within specific contexts. Indeed, AI technology can extract more in-depth insights from datasets than other techniques. Lately, AI has been used with success in solving complex problems in several domains such as machine translation, computer vision, autonomous cars, to mention a few. Machine learning and particularly computer vision models provide explicitly useful and practical approaches for analyzing and extracting relevant information from EO imagery data. Deep learning models and especially Convolution Neural Networks (CNNs) have proven effective in several computer vision tasks such as object detection, classification, and video processing, image generations, and image captioning, to mention a few. These models could be applied to detect and classify objects from complex EO imagery at a larger scale. Figure 2 presents AI capability for object detection and using computer vision techniques for multiband satellite images. This image has been taken from here). . Applying these techniques to EO data will make it easy to efficiently automate the recognition of known and unknown patterns at large-scale. This is likely to reveal useful insights and opportunities for addressing sustainability challenges. For example, AI models could be applied to perform automated change detection, crop mapping, and yield estimation from high-resolution imagery in a larger-scale. The fusion of EO data and other data sources such as geo-referenced, demographics, and social-network data can be used to facilitate the more targeted developmental challenge. For instance, it has been demonstrated that the AI model can be used to predict the poverty level by analyzing satellite imagery, night lights, and demographic data. . EO data sources . There are many EO data sources made available recently. These data sources offer virtual visualization of any location on earth with resolution ranging from 5 centimeters to 120 meters depending on the instruments of satellites, airbus, or drones. The data sources are published as public or commercial data sources. . Public EO data providers . The EO puplic data providers are public service framework that allows full, free and open access to all data collected. Copernicus and Landsat are the famous and largest public satellite data providers. Landsat s one of the world’s largest satellite image providers. It is a joint program of the National Aeronautics and Space Administration (NASA) and the United States Geological Survey (USGS). It provides access to satellites of the Landsat family, which have access over the archival of 50 years of earth data. Landsat satellites collect data on the forests, farms, urban areas, and water sources, generating the longest continuous record. The freely available information is used to understand environmental change better, manage agricultural practices, allocate scarce water resources, monitor the extent and health of forests and respond to natural disasters, and more. Data can be accessed using LandsLook Viewer, USGS GloVis, Earth Explorer, Free Web-Enabled Landsat Data (WELD). More information is available here. . Copernicus is managed by the Europe Unions EO program and collect data from a constellation of 6 families of satellites, known as Sentinels. Each Sentinel mission focuses on different but interrelated aspects of EO, including Atmospheric monitoring (Sentinels 4 and 5), Marine environment monitoring (Sentinel-3), Land monitoring (Sentinel-2), Climate Change and Emergency management. Currently Copernicus produces 12 terabytes per day of data for the 6 families of satellites, known as “Sentinels.” The data are open access and can be freely downloaded using [Copernicus Open Access Hub]. A summary of Copernicus program can found in this video . Commercial data providers . The commercial satellite imagery providers provide access to data with high resolution with 3 centimeters to 10 meters. These services are paid and have good archival imagery. The most popular commercial EO imagery providers include; Planet Labs, DigitalGlobe and Airbus. . Planet Labs provides access to a wide range of satellite data. It provides access to SkySAT families and RapidEye satellites. With 120+ satellites in orbit, Planet can image anywhere on Earth’s landmass daily, at 3 - 5-meter resolution. Planet processes and delivers imagery quickly and efficiently. Planet’s platform downloads and processes 11+ TB of data daily, enabling customers to build and run analytics at scale. Users can access Planet’s data, using the paid planet API. Nevertheless, university researchers, academics, and scientists apply for free access as decribed in this link. . The DigitalGlobe is similar to Planet Labs and provides data access to a full range constellation of satellites in orbit. It provides access to EarlyBird-1, IKONOS, QuickBird, GeoEye-1, a family of WorldView satellites. It offers a high resolution of up to 30cm, showing crisp details, satellite imagery, geospatial information, and location-based intelligence. Recently, DigitalGlobe has started providing 0.4m resolution imagery today, which is one of the best in the business. . On the other hand, the Airbus, with Pleiades and SPOT missions, provide very high-resolution multispectral twin satellites with 0.5 meters and 1.5-meter resolution, respectively. These imagery data are particularly suitable for emergency response and up-to daily change detection. . AI ready EO datasets . Building ML applications for EO requires access to both EO data and their ground truth. Creating such a data-set is time-consuming and costly. As a result different organisations provide ready-to-use EO dataset which allow ML and GIS researchers and other stakeholders to build and test their ML application specific to EO. Radiant MLHub and Spacenet are the two notable EO training data providers. Radiant MLHub is an open library for geospatial training data to advance machine learning applications on EO. It hosts open training datasets generated by Radiant Earth Foundation’s team as well as other training data catalogs contributed by Radiant Earth’s partners. The data provided by Radiant MLHub are stored using a SpatioTemporal Asset Catalog (STAC) compliant catalog and exposed through a standard API. These data are open to anyone to use. It also free stores, register and share your dataset. . The Spacenet, on the other hand, provides access to high-quality geospatial data for developers, researchers, and startups with a specific focus on the four open-source key pillars: data, challenges, algorithms, and tools. It also hosts challenges that focus on applying advanced machine learning techniques to solve difficult mapping challenges. The SpaceNet Dataset is hosted as an Amazon Web Services (AWS) Public Dataset, which is open for geospatial machine learning research. The dataset consists of well-annotated and very high-resolution satellite imagery with foundational mapping features such as building footprints or road networks. . Kaggle, a world’s largest data science community with powerful tools and resources, is another source of EO training datasets which host several machine learning challenges EO imagery. This challenges includes Dstl Satellite Imagery Feature Detection, Airbus Ship Detection Challenge and Draper Satellite Image Chronology to mention a few. . . API for accessing EO data. . Despite the availability of free and commercial satellite imagery, it is somehow challenging to directly download and use these data. Accessing these data requires one to have expertise in satellite imagery. Several API solutions that make it easy to access, download, and use satellite imagery from different sources have been developed to address these challenges. Sentinel Hub API is one of the easily available data API. . Sentinel Hub API makes satellite data from Sentinel, Landsat, and other Earth observation imagery easily accessible via easy-to-integrate web services. The API allows users to integrate satellite data either into their applications. To this end, Sentinel-hub offers several plugins such as sentinelhub-python, Sentinelhub-js, Sentinel Hub QGIS and EO Browser which is is a search tool for Sentinel-2 and Landsat 5,7,8 satellite imagery. Sentinelhub-js offer seamless integration of Sentinel Hub and other similar EO web services in web or node.js. This allows web developers to access remote sensing data quickly and to integrate it with their applications. On the other hand, the sentinel hub-python enables users to seamlessly make requests from Sentinel Hub OGC web services, download, and process images within their Python scripts. The Sentinel Hub QGIS plugin allows users to configure and harness Sentinel Hub services directly in QGIS. The Sentinel Hub API is the paid services but the also offer free access for research purpose. More details could found on this link. . Users can also use sentinelsat a python API for searching, downloading and retrieving the metadata of Sentinel satellite images from the Copernicus Open Access Hub. Compared to Sentinel-hub API, this is free services limited to only Copernicus satellites. . Opportunity and Challenges of AI4EO . Machine learning, precisely computer vision, can be applied to EO imagery data for multimodal semantic segmentation, detecting objects, detecting changes from a time series satellite image or image retrieval. The computer vision model can automatically generate semantic maps of a large area from EO data [Audebert2017a]. The resulting semantic maps can be used for the cartography of urban areas or to determine land use cover at a massive scale. In change detection, machine learning models could be used to extend the semantic analysis of EO data by incorporating the multi-temporal dimension. This enables us to track changes around the globe or monitor activity in high-revisit rate acquisitions. It also plays an essential role in the production of maps depicting the evolutions of land use, urban coverage, deforestation, and other multi-temporal type analysis. The image retrieval aims to retrieve images with similar visual contents with respect to the query image from a database. . . Even though AI provides a potential application to EO, several challenges need to be addressed to successfully exploits AI potentials. This is because compared to other types of Data, EO present several challenges for machine learning algorithms. The following video discuss some of the opportunities and challenges of machine learning for EO. . First, the EO data is multimodal and high dimensional. For instance, the EO satellite data come from a variety of sensors types such as passive sensors (RBGN), active sensors (Synthetic Aperture Radar (SAR)), near-infrared sensors. The data also contain additional geo-related data like weather, geo-physical or biochemical quantities and other derived products. This data variety raises the following fundamental challenges when applied to the machine learning model: *how to combine all these data types (data fusion) since all these sources provide complementary information that should be used jointly to maximize the model’s performance. As a result, there is a need to develop novel machine learning models that match EO data taken from different sources with different imaging modalities. Modifying existing vision-based deep networks to these data is not trivial, as this requires to work with new data structures that do not share the same underlying physical and numerical properties. Other exciting topics could be investigating the transferability of deep learning networks to [EO imaging modalities] (https://ieeexplore.ieee.org/document/8113128). Among the other challenges are the sheer number of pixels and the geographic extent per image. For example, a single DigitalGlobe satellite image encompasses &gt; 64 km2 and over 250 million pixels. Also, the objects of interest are microscopic, which complicates traditional computer vision techniques. . The size of EO data is increasing at an exponential rate demanding automation, massive computing, and machine learning algorithms that are fast enough and sufficiently transferrable to be applied for the whole earth’s surface. Besides, these data contain plenty of unlabelled data, making it challenging to use well-established supervised machine learning techniques. Yet this provides an opportunity to explore the recent progress in semi-supervised learning, self-supervised and active-learning methods for EO application. Furthermore, the existence of meta-data and other geo-referenced data such as the Open-street map (OSM) provides an opportunity for creating annotated satellite imagery data for machine learning algorithms. . It should be noted that EO data are time-variable dependent as satellite guarantees continuous data acquisition for decades. For example, the sentinel-1 images the entire earth every six days. Thus machine learning algorithms for EO imagery analysis should jointly exploit both the temporal, spectral, and spatial information of these data. . The interpretability of the machine learning model applicable to EO is another exciting research opportunity. Machine learning models are useful to estimate correlations, but what about causations. Exploiting graphical models and causal discovery to learns cause and effects relations from EO data is a unique opportunity for machine learning in EO. This can be useful for hypothesis testing, model-data comparison, and understanding the causes of extreme impacts. . Conclusion . The satellite has been in the orbit of the earth for many decades, but the access to the data and applications using satellite images has recently become prominent. In this blog, we have introduced the opportunity of using Earth Observation and Artificial Intelligence to address sustainability challenges. We introduced different sources for EO data sources that include public and private satellite providers. We also presented the prominent AI-ready EO data providers and introduced Sentinel-hub, which is an API for accessing EO data from different sources. Finally, the blog highlight opportunity and challenges of applying advanced machine learning techniques such as deep learning for EO imagery data. In the upcoming blog, we will discuss how to download, process, and use EO data for machine learning applications with a specif focus on computational sustainability. Stay tuned for more! . References . Working towards AI and Earth observation | .",
            "url": "http://localhost:4000/sambaiga/machine%20learning/deep%20learning/eearth%20observation/2020/06/20/eo-blog-one.html",
            "relUrl": "/machine%20learning/deep%20learning/eearth%20observation/2020/06/20/eo-blog-one.html",
            "date": " • Jun 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Mixture Density Networks.",
            "content": "Introduction . Deep Learning models are widely used in prediction problem which involves learning the mapping from a set of inputs variables x={x1,…,xd} mathbf{x}= {x_1, ldots, x_d }x={x1​,…,xd​} to a set of output variables y={y1,…,yc} mathbf{y}= {y_1, ldots,y_c }y={y1​,…,yc​}. In this setting, ddd is the size of input features, and ccc is the dimension of the output feature or target. In this case, usually the network is trained using minimization of the sum of squares errors or cross-entropy error function over a set of training data {x1:N,y1:N} { mathbf{x}_{1:N}, mathbf{y}_{1:N} }{x1:N​,y1:N​} of the form . L=(y−y^)2 where y^1:c=f(x1:d,w,b) mathcal{L} = ( mathbf{y}- hat mathbf{y})^2 text{ where } hat mathbf{y}_{1:c}=f( mathbf{x}_{1:d}, mathbf{w, b})L=(y−y^​)2 where y^​1:c​=f(x1:d​,w,b) . With this approach it is explicitly assumed that there is a deterministic 1−to−11-to-11−to−1 mapping between a given input variables x={x1,…,xd} mathbf{x}= {x_1, ldots, x_d }x={x1​,…,xd​} and target variable y={y1,…,yc} mathbf{y}= {y_1, ldots,y_c }y={y1​,…,yc​} without any uncertainty. As the result, the output of the network trained by this approach approximates the conditional mean of the output in the training data conditioned on the input vector. For classification problems with a well-chosen target coding scheme, these averages represent the posterior probability of class membership and thus be regarded as optimal. For a problem involving the prediction of a continuous variable, especially the conditional averages is not usually a good description of data and don’t have power to the modal distribution of output with complex. One way to solve this problem is to model the complete conditional probability density instead. This is the approach used by Mixture Density Networks (MDN). . Mixture Density Network . An MDN, as proposed by Bishop, is a flexible framework for modeling an arbitrary conditional probability distribution p(y∣x)p( mathbf{y}| mathbf{x})p(y∣x) as a mixture of distributions. It combines a mixture model with DNN in which a DNN is used to parametrize a mixture model consisting of some predefined distributions. Considering gaussian distribution, DNN is used to map a set of input features x1:d mathbf{x}_{1:d}x1:d​ to the parameters of a GMM i.e mixture weights πk(x) pi_k( mathbf{x})πk​(x), mean μk(x) mu _k( mathbf{x})μk​(x) and the covariance matrices σk2(x) sigma_k^2( mathbf{x})σk2​(x) which in turn gives a full probability density function of an output feature y mathbf{y}y conditioned on the input features. . p(y∣x)=∑k=1Mπk(x)N(y;μk(x),σk2(x))p( mathbf{y}| mathbf{x})= sum_{k=1}^M pi_k( mathbf{x}) mathcal{N}( mathbf{y}; mu_k( mathbf{x}), sigma_k^2( mathbf{x}))p(y∣x)=k=1∑M​πk​(x)N(y;μk​(x),σk2​(x)) . where MMM is the number of components in the mixture and . N(y;μk(x),σk2(x))=1(2σk2(x))c/2exp⁡[∣∣y−μk(x)∣∣22σk2(x)] mathcal{N}( mathbf{y}; mu_k( mathbf{x}), sigma_k^2( mathbf{x})) = frac{1}{(2 sigma_k^2( mathbf{x}))^{c/2}} exp left[ frac{|| mathbf{y}- mu_k( mathbf{x})||^2}{2 sigma_k^2( mathbf{x})} right]N(y;μk​(x),σk2​(x))=(2σk2​(x))c/21​exp[2σk2​(x)∣∣y−μk​(x)∣∣2​] . The mixture weights πk(x) pi_k( mathbf{x})πk​(x) represents the relative amounts by of each mixture components, which can be interpreted as the probabilities of the k−k-k− components for a given observation x mathbf{x}x.If we introduce a latent variable z mathbf{z}z with kkk possible states, then πk(x) pi_k( mathbf{x})πk​(x) will represents the probability distribution of these states p(z)p( mathbf{z})p(z). Specifically, the MDN converts the input vector using DNN with an output layer z mathbf{z}z of linear units to obtain the output z^=f(x,θ) hat{ mathbf{z}} = f( mathbf{x}, mathbf{ theta})z^=f(x,θ) . The total number of networks outputs i.e the dimension of z^ is (c+2)⋅M hat{ mathbf{z}} text{ is } (c+2) cdot Mz^ is (c+2)⋅M compared to the usual ccc outputs for a network used in the conventional manner. In order to guarantee that p(y∣x)p( mathbf{y}| mathbf{x})p(y∣x) is a probability distribution, the outputs of the networks need to be constrained such that the variance should remain positive and the mixing coefficients lie between zero and one and sum to one. To achieve these constraints: . The mean of the k−thk-thk−th kernel is modeled directly as the network outputs: | . μki(x)=zkμi where i=1,…,c mu_{k}^i( mathbf{x})=z_{k}^{ mu i} text{ where } i = 1, ldots, cμki​(x)=zkμi​ where i=1,…,c . The variances of $$ sigma_k $ is represented by an exponential activation function of the corresponding network output. | . σk(x)=exp⁡(zkσ) sigma_k( mathbf{x}) = exp(z_k^{ sigma})σk​(x)=exp(zkσ​) . The mixing coefficient πk(x) pi _k( mathbf{x})πk​(x) is modeled as the softmax transformation of the corresponding output. | . πk=exp⁡(zkπ)∑j=1Mexp⁡(zjπ) pi_k = frac{ exp(z_k^{ pi})}{ sum_{j=1}^M exp(z_j^{ pi})}πk​=∑j=1M​exp(zjπ​)exp(zkπ​)​ . Training MDN . As the generative model, an MDN model can be trained using the backpropagation algorithm under the maximum likelihood criterion. Suppose θ thetaθ is the vector of the trainable parameter, and we can redefine our model as a function of x mathbf{x}x parameterized by θ thetaθ . p(y∣x,θ)=∑k=1Mπk(x,θ)N(y;μk(x,θ),σk2(x,θ))p( mathbf{y}| mathbf{x}, mathbf{ theta})= sum_{k=1}^M pi_k( mathbf{x}, mathbf{ theta}) mathcal{N}( mathbf{y}; mu_k( mathbf{x}, mathbf{ theta}), sigma_k^2( mathbf{x}, mathbf{ theta}))p(y∣x,θ)=k=1∑M​πk​(x,θ)N(y;μk​(x,θ),σk2​(x,θ)) . Considering a data set D={x1:N,y1:N} mathcal{D}= { mathbf{x}_{1:N}, mathbf{y}_{1:N} }D={x1:N​,y1:N​} we want to maximize . p(θ∣D)=p(θ∣Y,X)p( mathbf{ theta}| mathcal{D}) = p( mathbf{ theta}| mathbf{Y}, mathbf{X})p(θ∣D)=p(θ∣Y,X) . By Bayes’s theorem, this is equivalent to . p(θ∣Y,X)p(Y)=p(Y,θ∣X)=p(Y∣X,θ)p(θ)p( mathbf{ theta}| mathbf{Y}, mathbf{X})p( mathbf{Y}) = p( mathbf{Y}, mathbf{ theta} | mathbf{X}) = p( mathbf{Y}| mathbf{X}, mathbf{ theta})p( mathbf{ theta})p(θ∣Y,X)p(Y)=p(Y,θ∣X)=p(Y∣X,θ)p(θ) . which leads to . p(θ∣Y,X)=p(Y∣X,θ)p(θ)p(Y)∝p(Y∣X,θ)p(θ)p( mathbf{ theta}| mathbf{Y}, mathbf{X}) = frac{p( mathbf{Y}| mathbf{X}, mathbf{ theta})p( mathbf{ theta})}{p( mathbf{Y})} propto p( mathbf{Y}| mathbf{X}, mathbf{ theta})p( mathbf{ theta})p(θ∣Y,X)=p(Y)p(Y∣X,θ)p(θ)​∝p(Y∣X,θ)p(θ) where p(Y∣X,θ)=∏n=1Np(yn∣xn,θ)p( mathbf{Y}| mathbf{X}, mathbf{ theta})= prod_{n=1}^N p( mathbf{y}_n| mathbf{x}_n, mathbf{ theta})p(Y∣X,θ)=∏n=1N​p(yn​∣xn​,θ) which is simply the product of the conditional densities for each pattern. . To define an error function, the standard approach is the maximum likelihood method, which requires maximisation of the log-likelihood function or, equivalently, minimisation of the negative logarithm of the likelihood. Therefore, the error function for the Mixture Density Network is: . E(θ,D)=−log⁡p(θ∣Y,X)=−log⁡p(Y∣X,θ)p(θ)=−(log⁡∏n=1Np(yn∣xn,θ)+log⁡p(θ))=−(∑n=1Nlog⁡∑k=1Mπk(x)N(y;μk(x),σk2(x))+log⁡p(θ)) begin{aligned} E( theta, mathcal{D})&amp;=- log p( mathbf{ theta}| mathbf{Y}, mathbf{X})= - log p( mathbf{Y}| mathbf{X}, mathbf{ theta})p( mathbf{ theta}) &amp;= - left( log prod_{n=1}^N p( mathbf{y}_n| mathbf{x}_n, mathbf{ theta}) + log p( mathbf{ theta}) right) &amp;=- left( sum_{n=1}^N log sum_{k=1}^M pi_k( mathbf{x}) mathcal{N}( mathbf{y}; mu_k( mathbf{x}), sigma_k^2( mathbf{x})) + log p( mathbf{ theta}) right) end{aligned}E(θ,D)​=−logp(θ∣Y,X)=−logp(Y∣X,θ)p(θ)=−(logn=1∏N​p(yn​∣xn​,θ)+logp(θ))=−(n=1∑N​logk=1∑M​πk​(x)N(y;μk​(x),σk2​(x))+logp(θ))​ . If we assume a non-informative prior of p(θ)=1p( mathbf{ theta})=1p(θ)=1 the error function simplify to . E(θ,D)=−∑n=1Nlog⁡∑k=1Mπk(x)N(y;μk(x),σk2(x))E( theta, mathcal{D}) = - sum_{n=1}^N log sum_{k=1}^M pi_k( mathbf{x}) mathcal{N}( mathbf{y}; mu_k( mathbf{x}), sigma_k^2( mathbf{x}))E(θ,D)=−n=1∑N​logk=1∑M​πk​(x)N(y;μk​(x),σk2​(x)) .",
            "url": "http://localhost:4000/sambaiga/machine%20learning/2018/01/03/mdn.html",
            "relUrl": "/machine%20learning/2018/01/03/mdn.html",
            "date": " • Jan 3, 2018"
        }
        
    
  
    
        ,"post2": {
            "title": "Introduction to Machine Learning - Classification.",
            "content": "Introduction . Previously we learned how to predict continuous-valued quantities as a linear function of input values. This post will describe a classification problem where the goal is to learn a mapping from inputs $x$ to target $t$ such that $t in {1 ldots C }$ with $C$ being the number of classes.If $C = 2$, this is called binary classification (in which case we often assume $y in {0, 1}$; if $C &gt; 2$, this is called multiclass classification. . We will first consider binary classification problem in which the target classes $t$ will be generated from 2 class distributions: blue ($t=1$) and red ($t=0$). Samples from both classes are sampled from their respective distributions. These samples are plotted in the figure below. . Note that $X$ is a $N times 2$ matrix of individual input samples $ mathbf{x}_i$, and that $ mathbf{t}$ is a corresponding $N times 1$ vector of target values $t_i$. . Logistic Regression . With logistic regression the goal is to predict the target class $t$ from the input values $x$. The network is defined as having an input $ mathbf{x} = [x_1, x_2]$ which gets transformed by the weights $ mathbf{w} = [w_1, w_2]$ to generate the probability that sample $ mathbf{x}$ belongs to class $t=1$$ This probability $P(t=1 mid mathbf{x}, mathbf{w})$ is represented by the output $y$ of the network computed as $y = sigma( mathbf{x} * mathbf{w}^T)$. $ sigma$ is the logistic function and is defined as: . σ(z)=11+e−z sigma(z) = frac{1}{1+e^{-z}}σ(z)=1+e−z1​ . which squashes the predictions to be between 0 and 1 such that: . P(t=1∣x,w)=y(σ(z))P(t=0∣x,w)=1−P(t=1∣x,w)=1−y(σ(z)) begin{aligned} P(t=1| mathbf{x}, mathbf{w}) &amp;= y( sigma(z))P(t=0 mid mathbf{x}, mathbf{w}) &amp;= 1 - P(t=1 mid mathbf{x}, mathbf{w}) = 1 - y( sigma(z)) end{aligned}P(t=1∣x,w)​=y(σ(z))P(t=0∣x,w)=1−P(t=1∣x,w)=1−y(σ(z))​ . The loss function for logistic function is called crossentropy and defined as: . LCE(y,t)={−log⁡yif t=1−log⁡(1−y)if t=0 mathcal{L}_{CE}(y,t)= begin{cases} - log y quad text{if } t = 1 - log (1-y) quad text{if } t = 0 end{cases}LCE​(y,t)={−logyif t=1−log(1−y)if t=0​ . The crossentropy can be written in other form as: . LCE(y,t)=−tlog⁡y−(1−t)log⁡(1−y) mathcal{L}_{CE}(y,t)= -t log y -(1-t) log(1-y)LCE​(y,t)=−tlogy−(1−t)log(1−y) . When we combine the logistic activation function with cross-entropy loss, we get logistic regression: . z=wTx+b y=σ(z) LCE(y,t)=−tlog⁡y−(1−t)log⁡(1−y) begin{aligned} z &amp; = mathbf{w^Tx + b} y &amp; = sigma(z) mathcal{L}_{CE}(y,t) &amp;= -t log y -(1-t) log(1-y) end{aligned}z y LCE​(y,t)​=wTx+b=σ(z)=−tlogy−(1−t)log(1−y)​ . The cost function with respect to the model parameters θ thetaθ (i.e. the weights and bias) is therefore: . εθ=1N∑i=1NLCE(y,t) =1N∑i=1N(−t(i)log⁡y(i)−(1−t(i))log⁡(1−y(i))) begin{aligned} varepsilon_{ theta} &amp; = frac{1}{N} sum_{i=1}^N mathcal{L}_{CE}(y,t) &amp; = frac{1}{N} sum_{i=1}^N left(-t^{(i)} log y^{(i)} -(1-t^{(i)}) log(1-y^{(i)}) right) end{aligned}εθ​ ​=N1​i=1∑N​LCE​(y,t)=N1​i=1∑N​(−t(i)logy(i)−(1−t(i))log(1−y(i)))​ . which can be implemented in python as follows: . # Define the cost function def cost(x, w, t): N, D = np.shape(x) z = z_value(x,w) y = y_value(z) result = np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))/float(N) return -result . Gradient Descent for Logistic Function . To derive the gradient descent updates, we’ll need the partial derivatives of the cost function. We’ll do this by applying the Chain Rule twice: first to compute ∂LCE∂z frac{ partial mathcal{L}_{CE}}{ partial z}∂z∂LCE​​ and then again to compute $ frac{ partial mathcal{L}_{CE}}{ partial w_j}$ But first, let’s find $ frac{ partial y}{ partial z}$. . ∂y∂z=e−z(1+e−z)2=y(1−y) frac{ partial y}{ partial z} = frac{e^{-z}}{(1 + e^{-z})^2}= y(1-y)∂z∂y​=(1+e−z)2e−z​=y(1−y) . Now for the Chain Rule: . ∂LCE∂z=∂LCE∂y∂y∂z =(−ty+1−t1−y)y(1−y) =y−t begin{aligned} frac{ partial mathcal{L}_{CE}}{ partial z} &amp; = frac{ partial mathcal{L}_{CE}}{ partial y} frac{ partial y}{ partial z} &amp; = left( frac{-t}{y} + frac{1-t}{1-y} right) y(1-y) &amp;= y - t end{aligned}∂z∂LCE​​  ​=∂y∂LCE​​∂z∂y​=(y−t​+1−y1−t​)y(1−y)=y−t​ . Similary: . ∂LCE∂wj=∂LCE∂z∂z∂wj =∂LCE∂zxj =(y−t)xj begin{aligned} frac{ partial mathcal{L}_{CE}}{ partial w_j} &amp; = frac{ partial mathcal{L}_{CE}}{ partial z} frac{ partial z}{ partial w_j} &amp; = frac{ partial mathcal{L}_{CE}}{ partial z} x_j &amp;= (y - t)x_j end{aligned}∂wj​∂LCE​​  ​=∂z∂LCE​​∂wj​∂z​=∂z∂LCE​​xj​=(y−t)xj​​ . We can also obtain ∂LCE∂b frac{ partial mathcal{L}_{CE}}{ partial b}∂b∂LCE​​ as follows: . ∂LCE∂b=∂LCE∂z∂z∂b=(y−t) begin{aligned} frac{ partial mathcal{L}_{CE}}{ partial b} &amp;= frac{ partial mathcal{L}_{CE}}{ partial z} frac{ partial z}{ partial b} &amp; = (y-t) end{aligned}∂b∂LCE​​​=∂z∂LCE​​∂b∂z​=(y−t)​ . The gradient descent algorithm works by taking the derivative of the cost function εθ varepsilon_{ theta}εθ​ with respect to the parameters, and updates the parameters in the direction of the negative gradient.The parameter w mathbf{w}w is iteratively updated by taking steps proportional to the negative of the gradient: . wk+1=wk−α∂ε∂w mathbf{w_{k+1}} = mathbf{ w_k }- alpha frac{ partial mathbf{ varepsilon}}{ partial mathbf{w}}wk+1​=wk​−α∂w∂ε​ . where: . ∂LCE∂ε=∂ε∂LCE⋅∂LCE∂w=1NxT(y−t) begin{aligned} frac{ partial mathcal{L}_{CE}}{ partial varepsilon} &amp;= frac{ partial varepsilon }{ partial mathcal{L}_{CE}} cdot frac{ partial mathcal{L}_{CE}}{ partial mathbf{w}} &amp;= frac{1}{N} mathbf{x^T(y - t)} end{aligned}∂ε∂LCE​​​=∂LCE​∂ε​⋅∂w∂LCE​​=N1​xT(y−t)​ . which can be implemented in python as follows: . #gradient def gradient(x, w, t): z = z_value(x,w) y = y_value(z) error = y-t dw = x.T.dot(error) return dw.T def solve_gradient(x, t, alpha=0.1, tolerance=1e-2): N, D = np.shape(x) w = np.zeros([D]) iterations = 1 w_cost = [(w, cost(x,w, t))] while True: dw = gradient(x, w, t) w_k = w - alpha * dw w_cost.append((w, cost(x, w, t))) # Stopping Condition if np.sum(abs(w_k - w)) &lt; tolerance: print (&quot;Converged.&quot;) break if iterations % 100 == 0: print (&quot;Iteration: %d - cost: %.4f&quot; %(iterations, cost(x, w, t))) iterations += 1 w = w_k return w . Let us apply the above concept in the following example. Consider the case we want to predict whether a student with a specific pass mark can be admitted or not. . # load dataset admission = pd.read_csv(&#39;data/admission.csv&#39;, names = [&quot;grade1&quot;, &quot;grade2&quot;, &quot;remark&quot;]) admission.head() . The data-preprosessing is done using the following python code: . features = [&#39;grade1&#39;, &#39;grade2&#39;] target = [&#39;remark&#39;] targetVal = admission[target] featureVal = admission[features] y = np.array(targetVal) # Standardize the features for i in range(2): featureVal.iloc[:,i] = (featureVal.iloc[:,i] / featureVal.iloc[:,i].max()) # Add bias term to feature data b = np.ones((featureVal.shape[0], 1)) X = np.hstack((b, featureVal)) # randomly separate data into training and test data from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=50) . We use the solve_gradient function defined before to find the parameter for logistic regression. . w_g = solve_gradient(X_train, y_train, alpha=0.05, tolerance = 1e-9) . Now that you learned the parameters of the model, you can use the model to predict whether a particular student will be admitted. . Let define the prediction function that only 1 or 0 depending on the predicted class. . def predict(x,w): z = z_value(x,w) y = y_value(z) return np.around(y) . To find the accuracy of the model: . p_test = predict(X_test, w_g) p_train = predict(X_train, w_g) print (&#39;Test Accuracy: %f&#39; % ((y_test[np.where(p_test == y_test)].size / float(y_test.size)) * 100.0)) print (&#39;Train Accuracy: %f&#39; % ((y_train[np.where(p_train == y_train)].size / float(y_train.size)) * 100.0)) . After running the above codes, we found that our model performs a training accuracy of 91.2591.2591.25 and a test accuracy of 858585 percents. . Multiclass classification . So far, we’ve talked about binary classification, but most classification problems involve more than two categories. Fortunately, this doesn’t require any new ideas: everything pretty much works by analogy with the binary case. The first question is how to represent the targets. We could describe them as integers, but it’s convenient to use indicator vectors or a one-of-K encoding. . Since there are $K$ outputs and DDD inputs, the linear function requires $K times D$ matrix as well as $K$ dimensional bias vector. We use softmax function which is the multivariate generalization given as: . yk=softmax(z1…zk)=ezk∑kezky_k = softmax(z_1 ldots z_k) = frac{e^{z_k}}{ sum_k e^{z_k}}yk​=softmax(z1​…zk​)=∑k​ezk​ezk​​ . and can be implemented in python as . def softmax(x,w): z = z_value(x,w) e_x = np.exp(x - np.max(x)) y = np.exp(z - max(z)) / np.sum(np.exp(z - max(z))) return y.reshape(len(y), 1) . Finally, the loss function (cross-entropy) for multiple-output case can be generalized as follows: . LCE(y,t)=−∑k=1Ktklog⁡yk=−tTlog⁡y begin{aligned} mathcal{L}_{CE}(y,t) &amp;= - sum_{k=1}^K t_k log y_k &amp;= - mathbf{t^T} log mathbf{y} end{aligned}LCE​(y,t)​=−k=1∑K​tk​logyk​=−tTlogy​ . Combining these things together, we get multiclass logistic regression: . z=wx+by=softmax(z)LCE(y,t)=−tTlog⁡y begin{aligned} mathbf{z} &amp;= mathbf{wx + b} mathbf{y} &amp;= softmax( mathbf{z}) mathcal{L}_{CE}(y,t) &amp;=- mathbf{t^T} log mathbf{y} end{aligned}zyLCE​(y,t)​=wx+b=softmax(z)=−tTlogy​ . Gradient Descent for Multiclass Logistic Regression for Multiclass logistic regression: . Let consider the derivative with respect to the loss: . ∂LCE∂wkj=∂∂wkj(−∑ltllog⁡(yl))=−∑ltlyl∂yl∂wkj begin{aligned} frac{ partial { mathcal L}_ text{CE}}{ partial w_{kj}} &amp;= frac{ partial }{ partial w_{kj}} left(- sum_l t_l log(y_l) right) &amp;= - sum_l frac{t_l}{y_l} frac{ partial y_l}{ partial w_{kj}} end{aligned}∂wkj​∂LCE​​​=∂wkj​∂​(−l∑​tl​log(yl​))=−l∑​yl​tl​​∂wkj​∂yl​​​ . Normally in calculus we have the rule: . ∂yl∂wkj=∑m∂yl∂zm∂zm∂wkj begin{aligned} frac{ partial y_l}{ partial w_{kj}} &amp;= sum_m frac{ partial y_l}{ partial z_m} frac{ partial z_m}{ partial w_{kj}} end{aligned}∂wkj​∂yl​​​=m∑​∂zm​∂yl​​∂wkj​∂zm​​​ . But wkjw_{kj}wkj​ is independent of zmz_mzm​ for m≠km ne km​=k, so . ∂yl∂wkj=∂yl∂zk∂zk∂wkj begin{aligned} frac{ partial y_l}{ partial w_{kj}} &amp;= frac{ partial y_l}{ partial z_k} frac{ partial z_k}{ partial w_{kj}} end{aligned}∂wkj​∂yl​​​=∂zk​∂yl​​∂wkj​∂zk​​​ . AND . ∂zk∂wkj=xj frac{ partial z_k}{ partial w_{kj}} = x_j∂wkj​∂zk​​=xj​ . Thus . ∂LCE∂wkj=−∑ltlyl∂yl∂zk∂zk∂wkj=−∑ltlyl∂yl∂zkxj=xj(−∑ltlyl∂yl∂zk)=xj∂LCE∂zk begin{aligned} frac{ partial { mathcal L}_ text{CE}}{ partial w_{kj}} &amp;= - sum_l frac{t_l}{y_l} frac{ partial y_l}{ partial z_k} frac{ partial z_k}{ partial w_{kj}} &amp;= - sum_l frac{t_l}{y_l} frac{ partial y_l}{ partial z_k} x_j &amp;= x_j (- sum_l frac{t_l}{y_l} frac{ partial y_l}{ partial z_k}) &amp;= x_j frac{ partial { mathcal L}_ text{CE}}{ partial z_k} end{aligned}∂wkj​∂LCE​​​=−l∑​yl​tl​​∂zk​∂yl​​∂wkj​∂zk​​=−l∑​yl​tl​​∂zk​∂yl​​xj​=xj​(−l∑​yl​tl​​∂zk​∂yl​​)=xj​∂zk​∂LCE​​​ . Now consider derivative with respect to $z_k$ we can show (onboard) that. . ∂yl∂zk=yk(Ik,l−yl) frac{ partial y_l}{ partial z_k} = y_k (I_{k,l} - y_l)∂zk​∂yl​​=yk​(Ik,l​−yl​) . Where $I_{k,l} = 1$ if $k=l$ and $0$ otherwise. . Therefore . ∂LCE∂zk=−∑ltlyl(yk(Ik,l−yl))=−tkykyk(1−yk)−∑l≠ktlyl(−ykyl)=−tk(1−yk)+∑l≠ktlyk=−tk+tkyk+∑l≠ktlyk=−tk+∑ltlyk=−tk+yk∑ltl=−tk+yk=yk−tk begin{aligned} frac{ partial { mathcal L}_ text{CE}}{ partial z_k} &amp;= - sum_l frac{t_l}{y_l} (y_k (I_{k,l} - y_l)) &amp;=- frac{t_k}{y_k} y_k(1 - y_k) - sum_{l ne k} frac{t_l}{y_l} (-y_k y_l) &amp;= - t_k(1 - y_k) + sum_{l ne k} t_l y_k &amp;= -t_k + t_k y_k + sum_{l ne k} t_l y_k &amp;= -t_k + sum_{l} t_l y_k &amp;= -t_k + y_k sum_{l} t_l &amp;= -t_k + y_k &amp;= y_k - t_k end{aligned}∂zk​∂LCE​​​=−l∑​yl​tl​​(yk​(Ik,l​−yl​))=−yk​tk​​yk​(1−yk​)−l​=k∑​yl​tl​​(−yk​yl​)=−tk​(1−yk​)+l​=k∑​tl​yk​=−tk​+tk​yk​+l​=k∑​tl​yk​=−tk​+l∑​tl​yk​=−tk​+yk​l∑​tl​=−tk​+yk​=yk​−tk​​ . Putting it all together . ∂LCE∂wkj=xj(yk−tk) begin{aligned} frac{ partial { mathcal L}_ text{CE}}{ partial w_{kj}} &amp;= x_j (y_k - t_k) end{aligned}∂wkj​∂LCE​​​=xj​(yk​−tk​)​ . In vectorization form it become: . ∂LCE∂W=(y−t)xT begin{aligned} frac{ partial mathcal {L}_{CE}}{ partial { mathbf W}} = ( mathbf{y} - mathbf{t}) mathbf{x}^T end{aligned}∂W∂LCE​​=(y−t)xT​ . Cross-entropy cost function . The cross entropy cost function for multiclass classification is given with respect to the model parameters θ thetaθ (i.e. the weights and bias) is therefore: . εθ=1N∑i=1NLCE(y,t)=−1N∑i=1N∑k=1Ktklog⁡yk begin{aligned} varepsilon_{ theta} &amp; = frac{1}{N} sum_{i=1}^N mathcal{L}_{CE}(y,t) &amp; = frac{-1}{N} sum_{i=1}^N sum_{k=1}^K t_k log y_k end{aligned}εθ​​=N1​i=1∑N​LCE​(y,t)=N−1​i=1∑N​k=1∑K​tk​logyk​​ . The gradient descent algorithm will be: wk+1=wk−α∂ε∂w mathbf{w_{k+1}} = mathbf{ w_k }- alpha frac{ partial mathbf{ varepsilon}}{ partial mathbf{w}}wk+1​=wk​−α∂w∂ε​ . where: . ∂LCE∂ε=∂ε∂LCE⋅∂LCE∂w=1NxT(y−t) begin{aligned} frac{ partial mathcal{L}_{CE}}{ partial varepsilon} &amp;= frac{ partial varepsilon }{ partial mathcal{L}_{CE}} cdot frac{ partial mathcal{L}_{CE}}{ partial mathbf{w}} &amp;= frac{1}{N} mathbf{x^T(y - t)} end{aligned}∂ε∂LCE​​​=∂LCE​∂ε​⋅∂w∂LCE​​=N1​xT(y−t)​ . References . CSC321 Intro to Neural Networks and Machine Learning | Supervised and Unsupervised Machine Learning Algorithms | .",
            "url": "http://localhost:4000/sambaiga/machine%20learning/2017/07/02/ml-classification.html",
            "relUrl": "/machine%20learning/2017/07/02/ml-classification.html",
            "date": " • Jul 2, 2017"
        }
        
    
  
    
        ,"post3": {
            "title": "Learning HMM parameters for Continous Density Models",
            "content": "Introduction . In the previous post, we considered a scenario where observation sequences YYY are discrete symbols. However, for many practical problems, the observation symbols are continuous vectors. As a result, the continuous probability density function (pdf) is used to model the space of the observation signal associated with each state. The most commonly used emission distribution is gaussian distribution and the gaussian mixture models. . Gaussian Distribution and the Gaussian Mixture Models . It is popular to represent the randomness of continuous-valued using the multivariate Gaussian distribution. A vector-valued random variable x mathbf{x}x is said to have a multivariate normal (or Gaussian) distribution with mean μ=E[x] mu= mathop{ mathbf{E[x]}}μ=E[x] and covariance matrix Σ=cov[x] Sigma= mathbf{cov[x]}Σ=cov[x] if: P(x;μ,Σ)=N(x∣μ,Σ)=1(2π)D/2∣Σ∣12exp⁡(−12[x−μ]Σ−1[x−μ]T)P( mathbf{x}; mu, Sigma) = mathcal{N( mathbf{x} mid mu, Sigma)}= frac{1}{(2 pi)^{D/2} | Sigma|^ frac{1}{2}} quad exp Big(- frac{1}{2}[ mathbf{x} - mu] Sigma^{-1}[ mathbf{x} - mu]^ mathsf{T} Big)P(x;μ,Σ)=N(x∣μ,Σ)=(2π)D/2∣Σ∣21​1​exp(−21​[x−μ]Σ−1[x−μ]T) where DDD is the dimensionality of x mathbf{x}x. The μ muμ represents the location where samples are most likely to be generated, and the Σ SigmaΣ indicates the level to which two variables vary together. . However, a single Gaussian distribution is insufficient to represent the state-dependent observation space for an HMM state st=is_t=ist​=i. This is because there are large amounts of training data collected from various appliance instances with different modes, distortions, background noises, etc which are used to train the parameters of individual HMM states. In this case, a Gaussian mixture model (GMM) is adopted to represent the state-dependent observation space. . A mixture model is a probabilistic model for density estimation using a mixture distribution and can be regarded as a type of unsupervised learning or clustering. They provide a method of describing more complex probability distributions by combining several probability distributions. The following equation gives a multivariate Gaussian mixture distribution: P(x)=∑k=1KωkN(x∣μk,Σk)P( mathbf{x}) = displaystyle sum_{k=1}^{K} omega_k mathcal{N( mathbf{x} mid mu_k, Sigma_k)}P(x)=k=1∑K​ωk​N(x∣μk​,Σk​) The parameters ωk omega_kωk​ are called mixing coefficients, which must fulfill ∑k=1Kωk=1 displaystyle sum_{k=1}^{K} omega_k =1k=1∑K​ωk​=1 and given N(x∣μk,Σk)≥0 mathcal{N( mathbf{x} mid mu_k, Sigma_k)} geq 0N(x∣μk​,Σk​)≥0 and P(x)≥0P( mathbf{x}) geq 0P(x)≥0 we also have that 0≤ωk≥10 leq omega_k geq 10≤ωk​≥1. Each Gaussian density N(x∣μk,Σk) mathcal{N( mathbf{x} mid mu_k, Sigma_k)}N(x∣μk​,Σk​) is called a component of the mixture and has its own mean μk mu_kμk​ and covariance Σk Sigma_kΣk​. . HMM with Gaussian emission distribution . If the observations are continuous, it is common for the emission probabilities to be a conditional Gaussian such that: P(yt∣st=i)=N(yt∣μi,Σi)P( mathbb{y_t} mid s_t =i) = mathcal{N( mathbf{y_t} mid mu_i, Sigma_i)}P(yt​∣st​=i)=N(yt​∣μi​,Σi​) where μi mu_iμi​ and Σi Sigma_iΣi​ are mean vector and covariance matrix associated with state iii. The re-estimation formula for the mean vector and covariance matrix of a state gausian pdf can be derived as: μ^i=∑t=1Tγt(i)y(t)∑t=1Tγt(i)Σ^i=∑t=1Tγt(i)[y(t)−μ^i]⋅[y(t)−μ^i]T∑t=1Tγt(i) begin{aligned} hat{ mu}_i &amp; = frac{ displaystyle sum_{t=1}^{T} gamma_t(i) mathbb{y(t)}}{ displaystyle sum_{t=1}^{T} gamma _t(i)} hat{ Sigma}_i &amp; = frac{ displaystyle sum_{t=1}^{T} gamma_t(i) [ mathbf{y(t)}- hat{ mu}_i] cdot[ mathbf{y(t)}- hat{ mu}_i]^T}{ displaystyle sum_{t=1}^{T} gamma_t(i)} end{aligned}μ^​i​Σ^i​​=t=1∑T​γt​(i)t=1∑T​γt​(i)y(t)​=t=1∑T​γt​(i)t=1∑T​γt​(i)[y(t)−μ^​i​]⋅[y(t)−μ^​i​]T​​ . HMMs with Gaussian Mixture Model . In HMMs with Gaussian mixture pdf, the emission probabilities is given by P(yt∣st=i)=∑k=1Mω_ikN(yt∣μik,Σik)P( mathbb{y_t} mid s_t =i) = displaystyle sum_{k=1}^{M} omega _{ik} mathcal{N( mathbb{y_t} mid mu_{ik}, Sigma_{ik})}P(yt​∣st​=i)=k=1∑M​ω_ikN(yt​∣μik​,Σik​) where ωik omega_{ik}ωik​ is the prior probability of the kthk^{th}kth component of the mixture. The posterior probability of state st=is_t=ist​=i at time ttt and state st+1=js_{t+1}=jst+1​=j at time t+1t+1t+1 given the model λ lambdaλ and the observation sequence YYY is γt(i,j)=P(st=i,st+1=j∣Y,λ)=αt(i)aij[∑k=1MωikN(yt∣μik,Σik)]βt+1(j)∑i=1NαT(i) begin{aligned} gamma_t(i,j)&amp; =P(s_t=i, s_{t+1}=j mid Y, lambda) &amp; = frac{ alpha_t(i)a_{ij} Big[ displaystyle sum_{k=1}^{M} omega_{ik} mathcal{N( mathbf{y_t} mid mu_{ik}, Sigma_{ik})} Big] beta_{t+1}(j)}{ displaystyle sum_{i=1}^{N} alpha_T(i)} end{aligned}γt​(i,j)​=P(st​=i,st+1​=j∣Y,λ)=i=1∑N​αT​(i)αt​(i)aij​[k=1∑M​ωik​N(yt​∣μik​,Σik​)]βt+1​(j)​​ . and the posterior probability of state st=is_t=ist​=i at time ttt given the model λ lambdaλ and observation YYY is γt(i)=αt(i)βt(i)∑i=1NαT(i) gamma_t(i) = frac{ alpha_t(i) beta_t(i)}{ displaystyle sum_{i=1}^{N} alpha _T(i)}γt​(i)=i=1∑N​αT​(i)αt​(i)βt​(i)​ Let define the joint posterior probability of the state sis_isi​ and the kthk^{th}kth gaussian component pdf of state iii at time ttt . ξ(i,k)=P(St=si,m(t)=k∣Y,λ)=∑j=1Nαt(j)aijωikN(yt∣μik,Σik)βt+1(j)∑i=1NαT(i) begin{aligned} xi(i,k) &amp;= P(S_t=s_i, m(t)=k mid Y, lambda) &amp;= frac{ displaystyle sum_{j=1}^{N} alpha_t(j) a_{ij} omega_{ik} mathcal{N( mathbf{y_t} mid mu_{ik}, Sigma_{ik})} beta_{t+1}(j)}{ displaystyle sum_{i=1}^{N} alpha _T(i)} end{aligned}ξ(i,k)​=P(St​=si​,m(t)=k∣Y,λ)=i=1∑N​αT​(i)j=1∑N​αt​(j)aij​ωik​N(yt​∣μik​,Σik​)βt+1​(j)​​ . The re-estimation formula for the mixture coefficients, the mean vectors and the covariance matrices of the state mixture gaussian pdf as . ω^ik=∑t=1Tξt(i,k)∑_t=0Tγt(i)μ^ik=∑ t=1Tξ t(i,k)yt∑t=1Tξt(i,k)Σ^ik=∑t=1Tξt(i,k)[yt−μ^ik]⋅[yt−μ^ik]T∑t=1Tξt(i,k) begin{aligned} hat{ omega}_{ik} &amp;= frac{ displaystyle sum_{t=1}^{T} xi_t(i,k)}{ displaystyle sum _{t=0}^{T} gamma_t(i)} hat{ mu}_{ik} &amp;= frac{ displaystyle sum _{t=1}^{T} xi _t(i,k) mathbf{y_t}}{ displaystyle sum_{t=1}^{T} xi_t(i,k)} hat{ Sigma}_{ik}&amp;= frac{ displaystyle sum_{t=1}^{T} xi_t(i,k)[ mathbf{y_t}- hat{ mu}_{ik}] cdot[ mathbf{y_t}- hat{ mu}_{ik}]^T}{ displaystyle sum_{t=1}^{T} xi_t(i,k)} end{aligned}ω^ik​μ^​ik​Σ^ik​​=∑_t=0Tγt​(i)t=1∑T​ξt​(i,k)​=t=1∑T​ξt​(i,k)∑ t=1T​ξ t​(i,k)yt​​=t=1∑T​ξt​(i,k)t=1∑T​ξt​(i,k)[yt​−μ^​ik​]⋅[yt​−μ^​ik​]T​​ . Limitation of Baum–Welch algorithm . When applying the Baum–Welch algorithm in real data, we need to consider some heuristics in the ML EM algorithm. . How to provide initial parameter values. This is always an important question, and it is usually resolved by using a simple algorithm (e.g., K-means clustering or random initialization). | How to avoid instability in the parameter estimation (especially covariance parameter estimation) due to data sparseness. For example, some mixture components or hidden states cannot have sufficient data assigned in the Viterbi or forward-backward algorithm. This can be heuristically avoided by setting a threshold to update these parameters or setting minimum threshold values for covariance parameters. | Bayesian approaches can solve the above two problems. . References . Saeed V. Vaseghi, Advanced Digital Signal Processing, and Noise Reduction. John Wiley &amp; Sons, 2008. | Kevin P. Murphy, Machine Learning: A Probabilistic Perspective. The MIT Press Cambridge, Massachusetts, 2012. |",
            "url": "http://localhost:4000/sambaiga/machine%20learning/2017/06/06/hmm-gausian.html",
            "relUrl": "/machine%20learning/2017/06/06/hmm-gausian.html",
            "date": " • Jun 6, 2017"
        }
        
    
  
    
        ,"post4": {
            "title": "Introduction to Machine Learning",
            "content": "Introduction . Machine learning is a set of algorithms that automatically detect patterns in data and use the uncovered pattern to make inferences or predictions. It is a subfield of artificial intelligence that aims to enable computers to learn on their own. Any machine learning algorithms involve the necessary three steps: first, you identify a pattern from data, build (train) model that best explains the pattern and the world (unseen data), and lastly, use the model to predict or make an inference. Model training (building) can be seen as a learning process where the model is exposed to new, unfamiliar data step by step. . Machine learning is an exciting and fast-moving field of computer science with many new applications. Applications where machine learning algorithms are regularly deployed includes: . Computer vision: Object Classification in Photograph, image captioning. | Speech recognition, Automatic Machine Translation. | Detecting anomalies (e.g. Security, credit card fraud) | Speech recognition. | Communication systemsref | Robots learning complex behaviors | Recommendations services like in Amazo or Netflix where intelligent machine learning algorithms analyze your activity and compare it to the millions of other users to determine what you might like to buy or binge watch nextref. | . Machine learning algorithms that learn to recognize what they see have been the heart of Apple, Google, Amazon, Facebook, Netflix, Microsoft, etc. . Why Machine learning . For many problems such as recognizing people and objects and understanding human speech, it’s difficult to program the correct behavior by hand. However, with machine learning, these tasks are easier. Other reasons we might want to use machine learning to solve a given problem: . A system might need to adapt to a changing environment. For instance, spammers are always trying to figure out ways to trick our e-mail spam classifiers, so the classification algorithms will need to adapt continually. | A learning algorithm might be able to perform better than its human programmers. Learning algorithms have become world champions at a variety of games, from checkers to chess to Go. This would be impossible if the programs were only doing what they were explicitly told to do. | We may want an algorithm to behave autonomously for privacy or fairness reasons, such as with ranking search results or targeting ads. | . Types of Machine Learning . Machine learning is usually divided into three major types: Supervised Learning, Unsupervised Learning and . Supervised Learning: Supervised learning is where you have input variables x and an output variable y, and use an algorithm to learn the mapping function from the input to the outputref. For instance, if we’re trying to train a machine-learning algorithm to distinguish cars and trucks, we would collect car and truck images and label each one as a car or a truck. Supervised learning problems can be further grouped into regression and classification problems. . A regression problem: is when the output variable is a real value, such as “dollars” or “weight” e.g Linear regression and Random forest. | Classification: A classification problem is when the output variable is a category, such as “red” or “blue” or “disease” and “no disease”, e.g. Support vector machines, random forest, and logistic regression. Some famous examples of supervised machine learning algorithms are: | . Unsupervised Learning : Unsupervised learning is where you only have input data (X) and no corresponding output variables. We just have a bunch of data and want to look for patterns in the data. For instance, maybe we have lots of examples of patients with autism and want to identify different subtypes of the condition. The most important types of unsupervised learning include: . Distribution modeling where one has an unlabeled dataset (such as a collection of images or sentences), and the goal is to learn a probability distribution that matches the dataset as closely as possible. | Clustering where the aim is to discover the inherent groupings in the data, such as grouping customers by purchasing behavior. | . Reinforcement Learning: is learning best actions based on reward or punishment. It involves learning what actions to take in a given situation, based on rewards and penalties. For example, a robot takes a big step forward, then falls. The next time, it takes a smaller stage and is able to hold its balance. The robot tries variations like this many times; eventually, it learns the right size of steps to take and walks steadily. It has succeeded. . There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. Action is what an agent can do in each state. When a robot takes action in a state, it receives a reward, feedback from the environment. A reward can be positive or negative (penalties). . Typical ML task: Linear Regression . In regression, we are interested in predicting a scalar-valued target, such as the price of a stock. By linear, we mean that the target must be predicted as a linear function of the inputs. This is a kind of supervised learning algorithm; recall that, in supervised learning, we have a collection of training examples labeled with the correct outputs. Example applications of linear regression include weather forecasting, house pricing prediction, student performance (GPA) prediction, just to mention a few. . Linear Regression: Formulating a learning problem . To formulate a learning problem mathematically, we need to define two things: a model (hypothesis)** and a *loss function. After defining model and loss function, we solve an optimization problem with the aim to find the model parameters that best fit the data. . Model (Hypothesis): It is the set of allowable hypotheses or functions that compute predictions from the inputs. In the case of linear regression, the model simply consists of linear functions given by: . y=∑jwjxj+by = sum_j w_jx_j + by=j∑​wj​xj​+b . where www is the weights, and bbb is an intercept term, which we’ll call the bias. These two terms are called model parameters denoted as θ thetaθ. . Loss function: It defines how well the model fit the data and thus show how far off the prediction yyy is from the target ttt and given as: . L(y,t)=12(y−t)2 mathcal{L(y,t)} = frac{1}{2}(y - t)^2L(y,t)=21​(y−t)2 . Since the loss function show how far off the prediction is from the target for one data point. We also need to define a cost function. The cost function is simply the loss, averaged over all the training examples. . J(w1…wD,b)=1N∑i=1NL(y(i),t(i))=12N∑i=1N(y(i)−t(i))2=12N∑i=1N(∑jwjxj(i)+b−t(i)) begin{aligned} J (w_1 ldots w_D,b) &amp; = frac{1}{N} sum_{i=1}^N mathcal{L}(y^{(i)},t^{(i)}) &amp; = frac{1}{2N} sum_{i=1}^N (y^{(i)} - t^{(i)})^2 &amp;= frac{1}{2N} sum_{i=1}^N left( sum_j w_jx_j^{(i)} + b -t^{(i)} right) end{aligned}J(w1​…wD​,b)​=N1​i=1∑N​L(y(i),t(i))=2N1​i=1∑N​(y(i)−t(i))2=2N1​i=1∑N​(j∑​wj​xj(i)​+b−t(i))​ . In vectorized form: . J=12N∥y−t∥2=12N(y−t)T(y−t)wherey=wTx mathbf{J} = frac{1}{2N} lVert mathbf{y-t} lVert^2 = frac{1}{2N} mathbf{(y - t)^T(y-t)} quad text{where} quad mathbf{y = w^Tx}J=2N1​∥y−t∥2=2N1​(y−t)T(y−t)wherey=wTx . The python implementation of the cost function (vectorized) is shown below. . def loss(x, w, t): N, D = np.shape(x) y = np.matmul(x,w.T) loss = (y - t) return loss . def cost(x,w, t): &#39;&#39;&#39; Evaluate the cost function in a vectorized manner for inputs `x` and targets `t`, at weights `w1`, `w2` and `b`. N, D = np.shape(x) return (loss(x, w,t) **2).sum() / (2.0 * N) . Combine our model and loss function, we get an optimization problem, where we are trying to minimize a cost function concerning the model parameters θ thetaθ (i.e. the weights and bias). . Solving the optimization problem . We now want to find the choice of model parameters θw1…wD,b theta _{w_1 ldots w_D,b}θw1​…wD​,b​ that minimizes J(w1…wD,b)J (w_1 ldots w_D,b)J(w1​…wD​,b) as given in the cost function above.There are two methods that we can use: direct solution and gradient descent. . Direct Solution . One way to compute the minimum of a function is to set the partial derivatives to zero. For simplicity, let’s assume the model doesn’t have a bias term, as shown in the equation below. . Jθ=12N∑i=1N(∑jwjxj(i)−t(i))J_ theta = frac{1}{2N} sum_{i=1}^N left( sum_j w_jx_j^{(i)} -t^{(i)} right)Jθ​=2N1​i=1∑N​(j∑​wj​xj(i)​−t(i)) . In vectorized form . J=12N∥y−t∥212N(y−t)T(y−t)wherey=wx mathbf{J} = frac{1}{2N} lVert mathbf{y-t} rVert ^2 frac{1}{2N} mathbf{(y - t)^T(y-t)} quad text{where} quad mathbf{y = wx}J=2N1​∥y−t∥22N1​(y−t)T(y−t)wherey=wx . For matrix differentiation we need the following results: . ∂Ax∂x=AT∂(xTAx)∂x=2ATx begin{aligned} frac{ partial mathbf{Ax}}{ partial mathbf{x}} &amp; = mathbf{A}^T frac{ partial ( mathbf{x}^T mathbf{Ax})}{ partial mathbf{x}} &amp; = 2 mathbf{A}^T mathbf{x} end{aligned}∂x∂Ax​​=AT∂x∂(xTAx)​=2ATx​ . Setting the partial derivatives of cost function in vectorized form to zero we obtain: . ∂J∂w=12N∂(wTxTxw−2tTwx+tTt)∂w=12N(2xTxw−2xTt)w=(xTx)−1xTt begin{aligned} frac{ partial mathbf{J}}{ partial mathbf{w}} &amp; = frac{1}{2N} frac{ partial left( mathbf{w^Tx^Tx w} -2 mathbf{t^Twx} + mathbf{t^Tt} right)}{ partial mathbf{w}} &amp;= frac{1}{2N} left(2 mathbf{x}^T mathbf{xw} -2 mathbf{x}^T mathbf{t} right) mathbf{w} &amp;= ( mathbf{x^Tx})^{-1} mathbf{x^Tt} end{aligned}∂w∂J​w​=2N1​∂w∂(wTxTxw−2tTwx+tTt)​=2N1​(2xTxw−2xTt)=(xTx)−1xTt​ . In python, this result can be implemented as follows: . def direct method(x, t): &#39;&#39;&#39; Solve linear regression exactly. (fully vectorized) Given `x` - NxD matrix of inputs `t` - target outputs Returns the optimal weights as a D-dimensional vector &#39;&#39;&#39; N, D = np.shape(x) A = np.matmul(x.T, x) c = np.dot(x.T, t) return np.matmul(linalg.inv(A), c) . Gradient Descent . The optimization algorithm commonly used to train machine learning is the gradient descent algorithm. It works by taking the derivative of the cost function JJJ with respect to the parameters at a specific position on this cost function and updates the parameters in the direction of the negative gradient. The entries of the gradient vector are simply the partial derivatives with respect to each of the variables: . ∂J∂w=(∂J∂w1⋮∂J∂wD) frac{ partial mathbf{J}}{ partial mathbf{w}} = begin{pmatrix} frac{ partial J}{ partial w_1} vdots frac{ partial J}{ partial w_D} end{pmatrix}∂w∂J​=⎝⎜⎜⎛​∂w1​∂J​⋮∂wD​∂J​​⎠⎟⎟⎞​ . The parameter w mathbf{w}w is iteratively updated by taking steps proportional to the negative of the gradient: . wt+1=wt−α∂J∂w=wt−αNxT(y−t) mathbf{w_{t+1}} = mathbf{ w_t }- alpha frac{ partial mathbf{J}}{ partial mathbf{w}} = mathbf{w_t} - mathbf{ frac{ alpha}{N}x^T(y-t)}wt+1​=wt​−α∂w∂J​=wt​−Nα​xT(y−t) . In coordinate systems this is equivalent to: . wt+1=wt−α1N∑i=1Nxt(y(i)−t(i))w_{t+1} = w_t - alpha frac{1}{N} sum_{i=1}^{N} x_t (y^{(i)}-t^{(i)})wt+1​=wt​−αN1​i=1∑N​xt​(y(i)−t(i)) . The python implementation of gradient descent is shown below: . def getGradient(x, w, t): N, D = np.shape(x) gradient = (1.0/ float(N)) * np.matmul(np.transpose(x), loss(x,w,t)) return gradient . def gradientDescentMethod(x, t, alpha=0.1, tolerance=1e-2): N, D = np.shape(x) #w = np.random.randn(D) w = np.zeros([D]) # Perform Gradient Descent iterations = 1 w_cost = [(w, cost(x,w, t))] while True: dw = getGradient(x, w, t) w_k = w - alpha * dw w_cost.append((w, cost(x, w, t))) # Stopping Condition if np.sum(abs(w_k - w)) &lt; tolerance: print (&quot;Converged.&quot;) break if iterations % 100 == 0: print (&quot;Iteration: %d - cost: %.4f&quot; %(iterations, cost(x, w, t))) iterations += 1 w = w_k return w, w_cost . Generalization . The goal of a learning algorithm is not only to make correct predictions on the training examples but also to be generalized to patterns not seen before. The average squared error on new examples is known as the generalization error, and we’d like this to be as small as possible. In practice, we normally tune model parameters by partitioning the dataset into three different subsets: . The training set is used to train the model. | The validation set is used to estimate the generalization error of each hyperparameter setting. | The test set is used at the very end, to estimate the generalization error of the final model, once all hyperparameters have been chosen. | .",
            "url": "http://localhost:4000/sambaiga/machine%20learning/2017/06/01/ml-introduction.html",
            "relUrl": "/machine%20learning/2017/06/01/ml-introduction.html",
            "date": " • Jun 1, 2017"
        }
        
    
  
    
        ,"post5": {
            "title": "Learning HMM parameters with Discrete Observation Models",
            "content": "Introduction . In the previous post, we discussed the basics of HMM modeling given model parameters λ lambdaλ and compute the likelihood values etc., efficiently based on the forward, backward, and Viterbi algorithms. In the like manner, we can efficiently train the HMM to obtain the model parameter λ^ hat{ lambda}λ^ from data. In this post, we will discuss different methods for training HMM models. . This is the solution to Problem 3, which involves determining a method to learn model parameters λ^ hat{ lambda}λ^ given the sequence of observation variables YYY. Given the observation sequences YYY as training data, there is no optimal way of estimating the model parameters. However, using iterative procedure we can choose λ^=(A^,B^,π^) hat{ lambda} = ( hat{A}, hat{B}, hat{ pi})λ^=(A^,B^,π^) such that P(Y∣λ)P(Y mid lambda)P(Y∣λ) is locally maximized.The most common procedure which has been employed to his problem is the Baum-Welch method. . Baum-Welch Methods . This method assumes an initial model parameter of λ lambdaλ, which should be adjusted to increase P(Y∣λ)P(Y mid lambda)P(Y∣λ). The initial parametrs can be constructed in any way or employ the first five procedure of the Segmental K-means algorithm. The optimazation criteria is called the maximum likelihood criteria.The function P(Y∣λ)P(Y mid lambda)P(Y∣λ) is called the likelihood function. . The E-M Auxilliary Function . Let λ lambdaλ represent the current model and λ^ hat{ lambda}λ^ represent the candidate models. The learning objective is to make: P(Y∣λ^)≥P(Y∣λ)P(Y mid hat{ lambda}) geq P(Y mid lambda)P(Y∣λ^)≥P(Y∣λ) which is equivalently to log⁡[P(Y∣λ^)]≥log⁡[P(Y∣λ)] log[P(Y mid hat{ lambda})] geq log [P(Y mid lambda)]log[P(Y∣λ^)]≥log[P(Y∣λ)] . Let also define the auxilliary function Q(λ^∣λ)Q( hat{ lambda} mid lambda)Q(λ^∣λ) such that: Q(λ^∣λ)=E[log⁡P(Y,S∣λ^)∣Y,λ]=∑sP(S∣Y,λ)⋅log⁡[P(Y,S∣λ^)] begin{aligned} Q( hat{ lambda} mid lambda) &amp; = mathbb{E} Big[ log P(Y,S mid hat{ lambda}) mid Y, lambda Big] &amp; = sum_s P(S mid Y, lambda) cdot log [P(Y,S mid hat{ lambda})] end{aligned}Q(λ^∣λ)​=E[logP(Y,S∣λ^)∣Y,λ]=s∑​P(S∣Y,λ)⋅log[P(Y,S∣λ^)]​ . The Maximum Likelihood Estimation (MLE) of the model parameter λ lambdaλ for complete data YYY and hidden state SSS is; . λ^=arg⁡max⁡λ∑sP(Y,S∣λ) hat{ lambda} = arg max _{ lambda} sum_s P(Y, S mid lambda)λ^=argλmax​s∑​P(Y,S∣λ) . However due to the presence of several stochatsic constraints it turns out to be easier to mximize uxilliary function Q(λ^∣λ)Q( hat{ lambda} mid lambda)Q(λ^∣λ) rather than directly maximize P(Y∣λ^)P(Y mid hat{ lambda})P(Y∣λ^). Thus the MLE of the model parameter λ lambdaλ for complete data YYY and hidden state SSS become: . λ^=arg⁡max⁡λQ(λ^∣λ) hat{ lambda} = arg max _{ lambda} Q( hat{ lambda} mid lambda)λ^=argλmax​Q(λ^∣λ) . It can be shown that the parameter estimated by the EM procedure, Q(λ^∣λ)Q( hat{ lambda} mid lambda)Q(λ^∣λ), always increases the likelihood value. You may concert reference 2 chapter 3 for details on the prove. . Expectation step . To find ML estimates of HMM parameters, we first expand the auxiliary function rewrite it by substituting the joint distribution of complete data likelihood. . Q(λ^∣λ)=E[log⁡P(Y,S∣λ^)∣Y,λ]=∑sP(S∣Y,λ)⋅log⁡[P(Y,S∣λ^)]=∑sP(S∣Y,λ)⋅[log⁡π^1+log⁡b^1(y1)+∑t=2T(log⁡a^ij+log⁡b^i(yt))] begin{aligned} Q( hat{ lambda} mid lambda) &amp; = mathbf{E} Big[ log P(Y,S mid hat{ lambda}) mid Y, lambda Big] &amp; = sum_s P(S mid Y, lambda) cdot log [P(Y,S mid hat{ lambda})] &amp; = sum_s P(S mid Y, lambda) cdot Big[ log hat{ pi}_1 + log hat{b}_1(y_1) + sum _{t=2}^T big( log hat{a} _{ij} + log hat{b}_i({y_t}) big) Big] end{aligned}Q(λ^∣λ)​=E[logP(Y,S∣λ^)∣Y,λ]=s∑​P(S∣Y,λ)⋅log[P(Y,S∣λ^)]=s∑​P(S∣Y,λ)⋅[logπ^1​+logb^1​(y1​)+t=2∑T​(loga^ij​+logb^i​(yt​))]​ . We have three term to solve: . The initial probability π^ hat{ pi}π^ , | State transition probability A^=a^ij hat{A} = hat{a}_{ij}A^=a^ij​ and | Emission probability B^=b^i(yt) hat{B} = hat{b}_i(y_t)B^=b^i​(yt​). | . Let first define important parameters that we will use. For t=1,2...Tt = 1,2...Tt=1,2...T, 1≤i≥N1 leq i geq N1≤i≥N and 1≤j≥N1 leq j geq N1≤j≥N, we define: . ξt(i,j)=P(st=i,st+1=j∣Y,λ) xi_t(i,j)=P(s_t=i, s_{t+1}=j mid Y, lambda)ξt​(i,j)=P(st​=i,st+1​=j∣Y,λ) . an expected transition probability from st=is_t=ist​=i to , st+1=js_{t+1}=jst+1​=j. The probability of being in state sis_isi​ at time ttt and state sjs_jsj​ at time t+1t+1t+1 given the model λ lambdaλ and observation sequences YYY. . ξt(i,j) xi_t(i,j)ξt​(i,j) can be written in terms of forward αt(i) alpha_t(i)αt​(i) and backward βt+1(j) beta_{t+1}(j)βt+1​(j) variables as: . ξt(i,j)=αt(i)aijbi(yt+1)βt+1(j)P(Y∣λ)=αt(i)aijbi(yt+1)βt+1(i)∑i=1N∑j=1Nαt(i)aijbj(yt+1)βt+1(j) begin{aligned} xi_t(i,j) &amp;= frac{ alpha_t(i)a_{ij}b_i(y_{t+1}) beta_{t+1}(j)}{P(Y mid lambda)} &amp;= frac{ alpha_t(i)a_{ij}b_i(y_{t+1}) beta_{t+1}(i)}{ displaystyle sum_{i=1}^{N} displaystyle sum_{j=1}^{N} alpha_t(i)a_{ij}b_j(y_{t+1}) beta_{t+1}(j)} end{aligned}ξt​(i,j)​=P(Y∣λ)αt​(i)aij​bi​(yt+1​)βt+1​(j)​=i=1∑N​j=1∑N​αt​(i)aij​bj​(yt+1​)βt+1​(j)αt​(i)aij​bi​(yt+1​)βt+1​(i)​​ . where the numerator term is just P(St=si,St+1=sj∣Y,λ)P(S_t=s_i, S_{t+1}=s_j mid Y, lambda)P(St​=si​,St+1​=sj​∣Y,λ) and the division by P(Y∣λ)P(Y mid lambda)P(Y∣λ) gives the desire probability measures. . We have previosly difined γt(i)=αt(i)βt(i)P(Y∣λ) gamma_t(i) = frac{ alpha_t(i) beta_t(i)}{P(Y mid lambda)}γt​(i)=P(Y∣λ)αt​(i)βt​(i)​ as the probability of being in state sis_isi​ at time ttt given the observation sequence and model parameter. γt(i) gamma_t(i)γt​(i) relate to ξt(i,j) xi_t(i,j)ξt​(i,j) as follows: . γt(i)=∑j=1Nξt(i,j) gamma_t(i) = displaystyle sum_{j=1}^{N} xi_t(i,j)γt​(i)=j=1∑N​ξt​(i,j) . It follows that: . ∑t=1T−1γt(i)= displaystyle sum_{t=1}^{T-1} gamma_t(i)=t=1∑T−1​γt​(i)= Expected number of transitions from state iii . . ∑t=1T−1ξt(i,j)= displaystyle sum_{t=1}^{T-1} xi_t(i,j)=t=1∑T−1​ξt​(i,j)= Expected number of transitions from state iii to state jjj. . We provide a solution for each term. Considering the first term Q(π^∣π)Q( hat{ pi} mid pi)Q(π^∣π) we define the following auxiliary function for πi pi _iπi​ as: . Q(π^∣π)=∑sP(S∣Y,λ)⋅log⁡π^s1Q( hat{ pi} mid pi) = sum_s P(S mid Y, lambda) cdot log hat{ pi}_{s_1}Q(π^∣π)=s∑​P(S∣Y,λ)⋅logπ^s1​​ . Since π^s1 hat{ pi}_{s_1}π^s1​​ only depends on s1s_1s1​, it clear that: . P(S∣Y,λ)=P(s1∣Y,λ)P(S mid Y, lambda) = P(s_1 mid Y, lambda)P(S∣Y,λ)=P(s1​∣Y,λ) . Therefore Q(π^∣pi)Q( hat{ pi} mid pi)Q(π^∣pi) can be rewritten as: . Q(π^∣π)=∑s1P(s1∣Y,λ)⋅log⁡π^s1=∑i=1NP(s1=i∣Y,λ)⋅log⁡π^i=∑i=1Nγt(i)log⁡π^i begin{aligned} Q( hat{ pi} mid pi) &amp;= sum_{s_1}P(s_1 mid Y, lambda) cdot log hat{ pi}_{s_1} &amp;= sum_{i=1}^N P(s_1=i mid Y, lambda) cdot log hat{ pi}_{i} &amp; = sum_{i=1}^N gamma_t(i) log hat{ pi}_{i} end{aligned}Q(π^∣π)​=s1​∑​P(s1​∣Y,λ)⋅logπ^s1​​=i=1∑N​P(s1​=i∣Y,λ)⋅logπ^i​=i=1∑N​γt​(i)logπ^i​​ . Next, we focus on the second term Q(A^∣A)Q( hat{A} mid A)Q(A^∣A) . Q(A^∣A)=∑sP(S∣Y,λ)⋅∑t=2Tlog⁡a^st,st+1Q( hat{A} mid A) = sum_s P(S mid Y, lambda) cdot sum _{t=2}^T log hat{a}_{s_t,s_{t+1}}Q(A^∣A)=s∑​P(S∣Y,λ)⋅t=2∑T​loga^st​,st+1​​ . Similar to Q(π^∣π)Q( hat{ pi} mid pi)Q(π^∣π) , we obtain P(S∣Y,λ)=P(s1∣Y,λ)=P(st,st+1∣Y,λ)P(S mid Y, lambda) = P(s_1 mid Y, lambda) = P(s_t,s_{t+1} mid Y, lambda)P(S∣Y,λ)=P(s1​∣Y,λ)=P(st​,st+1​∣Y,λ) . Therefore . Q(A^∣A)=∑t=1T−1∑sP(st,st+1∣Y,λ)log⁡a^st,st+1=∑t=1T−1∑i=1N∑j=1NP(st=i,st+1=j∣Y,λ)log⁡a^ij=∑t=1T−1∑i=1N∑j=1Nξt(i,j)log⁡a^ij begin{aligned} Q( hat{A} mid A) &amp; = sum _{t=1}^{T-1} sum_s P(s_t,s_{t+1} mid Y, lambda) log hat{a}_{s_t,s_{t+1}} &amp; = sum _{t=1}^{T-1} sum_{i=1}^N sum_{j=1}^N P(s_t=i,s_{t+1}=j mid Y, lambda) log hat{a}_{ij} &amp; = sum _{t=1}^{T-1} sum_{i=1}^N sum_{j=1}^N xi_t(i,j) log hat{a}_{ij} end{aligned}Q(A^∣A)​=t=1∑T−1​s∑​P(st​,st+1​∣Y,λ)loga^st​,st+1​​=t=1∑T−1​i=1∑N​j=1∑N​P(st​=i,st+1​=j∣Y,λ)loga^ij​=t=1∑T−1​i=1∑N​j=1∑N​ξt​(i,j)loga^ij​​ . Finally, we focus on the last term Q(B^∣B)Q( hat{B} mid B)Q(B^∣B) . Q(B^∣B)=∑sP(S∣Y,λ)⋅∑t=1Tlog⁡b^i(yt)Q( hat{B} mid B) = sum_s P(S mid Y, lambda) cdot sum _{t=1}^T log hat{b}_{i}(y_t)Q(B^∣B)=s∑​P(S∣Y,λ)⋅t=1∑T​logb^i​(yt​) . Similary P(S∣Y,λ)=P(st=i∣Y,λ)P(S mid Y, lambda) = P(s_t = i mid Y, lambda)P(S∣Y,λ)=P(st​=i∣Y,λ). Therefore . Q(B^∣B)=∑t=1T∑sP(st=i∣Y,λ)log⁡b^i(yt)=∑t=1T∑i=1Nγt(i)log⁡b^i(yt) begin{aligned} Q( hat{B} mid B) &amp;= sum _{t=1}^T sum_s P(s_t = i mid Y, lambda) log hat{b}_{i}(y_t) &amp; = sum _{t=1}^T sum_{i=1}^N gamma_t(i) log hat{b}_{i}(y_t) end{aligned}Q(B^∣B)​=t=1∑T​s∑​P(st​=i∣Y,λ)logb^i​(yt​)=t=1∑T​i=1∑N​γt​(i)logb^i​(yt​)​ . Thus, we summarize the auxiliary function. . Q(λ^∣λ)=Q(π^∣π)+Q(A^∣A)+Q(B^∣B)Q( hat{ lambda} mid lambda)= Q( hat{ pi} mid pi)+ Q( hat{A} mid A) + Q( hat{B} mid B)Q(λ^∣λ)=Q(π^∣π)+Q(A^∣A)+Q(B^∣B) . Maximization step . In the maximization step, we aim to maximize Q(π^∣π)Q( hat{ pi} mid pi)Q(π^∣π), Q(A^∣A)Q( hat{A} mid A)Q(A^∣A) and Q(B^∣B)Q( hat{B} mid B)Q(B^∣B) with respect to π^ hat{ pi}π^, A^ hat{A}A^ and B^ hat{B}B^ under the following constraints. . ∑_i=1Nπ^=1, and ∑i=1NA^=1 sum _{i=1}^N hat{ pi} = 1, text{ and } sum_{i=1}^N hat{A} = 1∑_i=1Nπ^=1, and i=1∑N​A^=1 . Considering the estimation of initial state probabilities π^=π^i mathbf{ hat{ pi}} = { hat{ pi}_i}π^=π^i​ , we construct a Lagrange function (or Lagrangian): . Q∗(π^∣π)=∑i=1Nγ1(i)log⁡π^i+η(∑i=1Nπ^−1)Q^*( hat{ pi} mid pi) = sum_{i=1}^N gamma_1(i) log hat{ pi}_{i} + eta left( sum_{i=1}^N hat{ pi} - 1 right)Q∗(π^∣π)=i=1∑N​γ1​(i)logπ^i​+η(i=1∑N​π^−1) . Differentiating this Lagrangian with respect to individual probability parameter π^i hat{ pi}_iπ^i​ and set it to zero we obtain. . ∂Q∗(π^∣π)∂π^i=γ1(i)1π^i+η=0π^i=−1ηγ1(i) begin{aligned} frac{ partial Q^*( hat{ pi} mid pi)}{ partial hat{ pi}_i } &amp; = gamma_1(i) frac{1}{ hat{ pi}_i} + eta = 0 hat{ pi}_i &amp;= - frac{1}{ eta} gamma_1(i) end{aligned}∂π^i​∂Q∗(π^∣π)​π^i​​=γ1​(i)π^i​1​+η=0=−η1​γ1​(i)​ . Substituting the above equation into ∑i=1Nπ^=1 sum_{i=1}^N hat{ pi} = 1∑i=1N​π^=1 constraint, we obtain: . ∑i=1Nπ^=∑i=1N−1ηγ1(i)=1⇒η=−∑i=1Nγ1(i) begin{aligned} sum_{i=1}^N hat{ pi} &amp;= sum_{i=1}^N - frac{1}{ eta} gamma_1(i) = 1 Rightarrow eta &amp;= - sum_{i=1}^N gamma_1(i) end{aligned}i=1∑N​π^⇒η​=i=1∑N​−η1​γ1​(i)=1=−i=1∑N​γ1​(i)​ . The ML estimate of new initial state probability is obtained by substituting the above equation into π^i=−1ηγ1(i) hat{ pi}_i = - frac{1}{ eta} gamma_1(i)π^i​=−η1​γ1​(i): . π^i=γ1(i)∑i=1Nγ1(i)=γ1(i) hat{ pi}_i = frac{ gamma_1(i)}{ sum _{i=1}^N gamma_1(i)} = gamma_1(i)π^i​=∑i=1N​γ1​(i)γ1​(i)​=γ1​(i) . In the same manner, we can derive the ML estimates of new state transition probability and new emission probability, which can be shown to be: . a^ij=∑t=1T−1ξt(i,j)∑t=1T−1γt(i) hat{a}_{ij} = frac{ displaystyle sum_{t=1}^{T-1} xi_t(i,j)}{ displaystyle sum_{t=1}^{T-1} gamma_t(i)}a^ij​=t=1∑T−1​γt​(i)t=1∑T−1​ξt​(i,j)​ . And . b^i(k)=∑t=1Tτγt(i)∑t=1Tγt(i) hat{b}_i(k) = frac{ displaystyle sum_{t=1}^{T} tau gamma_t(i)}{ displaystyle sum_{t=1}^{T} gamma_t(i)}b^i​(k)=t=1∑T​γt​(i)t=1∑T​τγt​(i)​ . where . τ={1 if yt=k,0 otherwise  tau = begin{cases} 1 text{ if } y_t = k, 0 text{ otherwise } end{cases}τ={1 if yt​=k,0 otherwise ​ . If we denote the initial model λ{ lambda}λ and the re-estimation model by λ^=(π^i,a^ij,b^j(k)) hat{ lambda}=( hat{ pi}_i, hat{a}_{ij}, hat{b}_j(k))λ^=(π^i​,a^ij​,b^j​(k)). Then i t can be shown that either: . The initial model λ lambdaλ is a critical point of the likelihood in which case λ^=λ hat{ lambda}= lambdaλ^=λ or | P(Y∣λ^)≤P(Y∣λ)P(Y mid hat lambda) leq P(Y mid lambda)P(Y∣λ^)≤P(Y∣λ), i.e we have find the better model from which the observation sequence Y=y1,…YTY=y_1, ldots Y_TY=y1​,…YT​ is more likely to be produced. | Hence we can go on iteractively computing until P(Y∣λ^)P(Y mid hat{ lambda})P(Y∣λ^) is maximazed. . The Baum-Welch Algorithm can be summarized as: . Require: λ←λinit lambda leftarrow lambda ^{init}λ←λinit . repeat | Compute the forward variable αt(i) alpha _t(i)αt​(i) from the forward algorithm | Compute the backward variable βt(i) beta _t(i)βt​(i) from the backward algorithm | Compute the occupation probabilities γt(i) gamma _t(i)γt​(i), and ξt(i,j) xi _t(i,j)ξt​(i,j) | Estimate the new HMM parameters λ^ hat{ lambda}λ^ | Update the HMM parameters λ←λ^ lambda leftarrow hat{ lambda}λ←λ^ | until Convergence | References . L. R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proceedings of the IEEE, Vol. 77, No. 2, February 1989. | Shinji Watanabe, Jen-Tzung Chien, Bayesian Speech and Language Processing, Cambridge University Press, 2015. | Viterbi Algorithm in Speech Enhancement and HMM | Nikolai Shokhirev, Hidden Markov Models |",
            "url": "http://localhost:4000/sambaiga/machine%20learning/2017/05/29/hmm-discrete.html",
            "relUrl": "/machine%20learning/2017/05/29/hmm-discrete.html",
            "date": " • May 29, 2017"
        }
        
    
  
    
        ,"post6": {
            "title": "The Basic of Hidden Markov Model",
            "content": "Introduction . HMM is a Markov model whose states are not directly observed; instead, each state is characterized by a probability distribution function. The probability distribution model the observation corresponding to that state. HMM has been extensively used in temporal pattern recognition such as speech, handwriting, gesture recognition, robotics, biological sequences, and recently in energy disaggregation. This tutorial will introduce the basic concept of HMM. . There are two variables in HMM: observed variables and hidden variables. The sequences of hidden variables form a Markov process, as shown in the figure below. In the context of NILM, the hidden variables are used to model states(ON, OFF, standby etc) of individual appliances, and the observed variables are used to model the electric usage. HMMs have been widely used in most of the recently proposed NILM approach. This is because HMM represents well the individual appliance internal states which are not directly observed in the targeted energy consumption. . A typical HMM is characterised by the following: . The finite set of hidden states SSS (e.g ON, stand-by, OFF, etc.) of an appliance, S={s1,s2....,sN}S = {s_1, s_2....,s_N }S={s1​,s2​....,sN​}. | The finite set of MMM observable symbol YYY per states (power consumption) observed in each state, Y={y1,y2....,yM}Y = {y_1, y_2....,y_M }Y={y1​,y2​....,yM​}. The observable symbol YYY can be discrete or a continuous set. | The transition matrix A={aij,1≤i,j≥N} mathbf{A}= {a_{ij},1 leq i,j geq N }A={aij​,1≤i,j≥N} represents the probability of moving from state st−1=is_{t-1}=ist−1​=i to st=js_t =jst​=j such that: aij=P(st=j∣st−1=i)a_{ij} = P(s_{t} =j mid s_{t-1}=i)aij​=P(st​=j∣st−1​=i), with aij≤0a_{ij} leq 0aij​≤0 and where sts_tst​ denotes the state occupied by the system at time ttt. The matrix A mathbf{A}A is NxNN x NNxN. | The emission matrix B={bj(k)} mathbf{B} = {b_j(k) }B={bj​(k)} representing the probability of emission of symbol kkk ϵ epsilonϵ YYY when system state is st=js_t=jst​=j such that: bj(k)=p(yt=k∣st=j)b_j(k) = p(y_t = k mid s_t=j)bj​(k)=p(yt​=k∣st​=j) The matrix B mathbf{B}B is an NxMN x MNxM. The emission probability can be discrete or continous distribution. If the emission is descrete a multinomial distribution is used and multivariate Gaussian distribution is usually used for continous emission. | And the initial state probability distribution π={πi} mathbf{ pi} = { pi_i }π={πi​} indicating the probability of each state of the hidden variable at t=1t = 1t=1 such that, πi=P(q1=si),1≤i≥N pi _i = P(q_1 = s_i), 1 leq i geq Nπi​=P(q1​=si​),1≤i≥N. | . For brief introduction of optuna, you can watch this video . The complete HMM specification requires; . Finite set of hidden states NNN and observation symbols MMM | Length of observation seqences TTT and | Specification of three probability measures A,B mathbf{A}, mathbf{B}A,B and π mathbf{ pi}π | The set of all HMM model parameters is represented by λ=(π,A,B) mathbf{ lambda} =( pi, A, B)λ=(π,A,B). . Since $S$ is not observed, the likelihood function of YYY is given by the joint distribution of $Y$ and $S$ over all possible state. . P(Y∣λ)=∑P(Y,S∣λ)P(Y mid lambda) = sum P(Y, S mid lambda)P(Y∣λ)=∑P(Y,S∣λ) . where . P(Y,S∣λ)=P(Y∣S,λ)P(S∣λ)P(Y,S mid lambda) = P(Y mid S, lambda)P(S mid lambda)P(Y,S∣λ)=P(Y∣S,λ)P(S∣λ) . Note that yty_tyt​ is independent and identically distributed given state sequence S={s1,s2....,sN}S = {s_1, s_2....,s_N }S={s1​,s2​....,sN​}. Also each state at time ttt depend on the state at its previous time t−1t-1t−1. Then . P(Y∣S,λ)=∏t=1TP(yt∣st)P(Y mid S, lambda) = prod_{t=1}^T P(y_t mid s_t)P(Y∣S,λ)=t=1∏T​P(yt​∣st​) . Similary . P(S∣λ)=πs1∏t=2TaijP(S mid lambda) = pi _{s_1} prod _{t=2}^T a_{ij}P(S∣λ)=πs1​​t=2∏T​aij​ . The joint probability is therefore: . P(Y∣λ)=πs1P(y1∣s1)∑∏t=2TaijP(yt∣st)P(Y mid lambda) = pi _{s_1}P(y_1 mid s_1) sum prod_{t=2}^T a_{ij} P(y_t mid s_t)P(Y∣λ)=πs1​​P(y1​∣s1​)∑t=2∏T​aij​P(yt​∣st​) . Three main problems in HMMs . When applying HMM to a real-world problem, three crucial issues must be solved. . Evaluation Problem: Given HMM parameters λ lambdaλ and the observation sequence Y={Y1,Y2....,YM}Y = {Y_1, Y_2....,Y_M }Y={Y1​,Y2​....,YM​}, find P(Y∣λ)P(Y mid lambda)P(Y∣λ) the likelihood of the observation sequence YYY given the model λ lambdaλ. This problem gives a score on how well a given model matches a given observation and allows you to choose the model that best matches the observation. | Decoding Problem: Given HMM parameters λ lambdaλ and the observation seqence Y={Y1,Y2....,YM}Y = {Y_1, Y_2....,Y_M }Y={Y1​,Y2​....,YM​}, find an optimal state sequense S={S1,S2....,SN}S = {S_1, S_2....,S_N }S={S1​,S2​....,SN​} which best explain the observation.This problem attempt to cover the hidden part of the model. | Learning Problem: Given the obseravtion seqence Y={Y1,Y2....,YM}Y = {Y_1, Y_2....,Y_M }Y={Y1​,Y2​....,YM​}, find the model parameters λ lambdaλ that maximize P(Y∣λ)P(Y mid lambda)P(Y∣λ).This problem attempt to optimize the model parameters so as to describe the model. | The first and the second problem can be solved by the dynamic programming algorithms known as the Viterbi algorithm and the Forward-Backward algorithm, respectively. The last one can be solved by an iterative Expectation-Maximization (EM) algorithm, known as the Baum-Welch algorithm. We will discuss the first and the second problem in this post. . Solution to Problem 1 . A straight forward way to solve this problem is to find P(Y∣S,λ)P(Y mid S, lambda)P(Y∣S,λ) for fixed state sequences S={s1,...sT}S = {s_1,...s_T }S={s1​,...sT​} and then sum up over all possible states. This is generally infeasible since it requires about 2TNT2TN^T2TNT multiplications. Nevertheless, this problem can be efficiently solved by using the forward algorithm as follows: . The forward-backward Algorithm . Let us define the forward variable . αt(i)=P(y1,…yt,st=i∣λ) alpha _t(i)=P(y_1, ldots y_t, s_t=i mid lambda)αt​(i)=P(y1​,…yt​,st​=i∣λ) . the probability of the partial observation sequences y1…yty_1 ldots y_ty1​…yt​ up to time ttt and the state st=is_t =ist​=i at time ttt given the model λ{ lambda}λ. We also define an emission probability given HMM state iii at time ttt as bi(yt)b_i(y_t)bi​(yt​). . Forward-Algorithm . Initialization . Let . α1(i)=P(y1,s1=i∣λ)=P(y1∣s1=i,λ)P(s1=i∣λ)=πibi(y1) for 1≤i≥N begin{aligned} alpha _1(i)&amp;=P(y_1, s_1=i mid lambda) &amp; = P(y_1 mid s_1=i, lambda)P(s_1=i mid lambda) &amp;= pi _i b_i(y_1) text{ for } 1 leq i geq N end{aligned}α1​(i)​=P(y1​,s1​=i∣λ)=P(y1​∣s1​=i,λ)P(s1​=i∣λ)=πi​bi​(y1​) for 1≤i≥N​ . Induction . For t=2,3...Tt=2,3...Tt=2,3...T and 1≤i≥N1 leq i geq N1≤i≥N, compute: . αt(i)=P(y1…yt,st=i∣λ)=∑j=1NP(y1…yt,st−1=j,st=i∣λ)=∑j=1NP(yt∣st=i,y1,…yt−1,st−1=j,λ)×P(st=i∣y1…yt−1…,st−1=j,λ)×P(y1…yt−1,st−1=j,λ)=P(yt∣st=i,λ)∑j=1NP(st=i∣st−1=j)⋅P(y1,…yt−1,st−1)=bi(yt)∑j=1Nαt−1(i)aij begin{aligned} alpha _{t}(i) &amp; = P(y_1 ldots y_t, s_t=i mid lambda) &amp;= displaystyle sum_{j=1}^{N} P(y_1 ldots y_{t}, s_{t-1}=j,s_t=i mid lambda) &amp;= displaystyle sum_{j=1}^{N} P(y_t mid s_t=i, y_1, ldots y_{t-1}, s_{t-1}=j, lambda) &amp; times P(s_t=i mid y_1 ldots y_{t-1} ldots , s_{t-1}=j, lambda) &amp; times P(y_1 ldots y_{t-1}, s_{t-1}=j, lambda) &amp; = P(y_t mid s_t=i, lambda) displaystyle sum_{j=1}^{N} P(s_t=i mid s_{t-1}=j) cdot P(y_1, ldots y_{t-1}, s_{t-1}) &amp; = b_i(y_{t}) displaystyle sum_{j=1}^{N} alpha _{t-1}(i)a_{ij} end{aligned}αt​(i)​=P(y1​…yt​,st​=i∣λ)=j=1∑N​P(y1​…yt​,st−1​=j,st​=i∣λ)=j=1∑N​P(yt​∣st​=i,y1​,…yt−1​,st−1​=j,λ)×P(st​=i∣y1​…yt−1​…,st−1​=j,λ)×P(y1​…yt−1​,st−1​=j,λ)=P(yt​∣st​=i,λ)j=1∑N​P(st​=i∣st−1​=j)⋅P(y1​,…yt−1​,st−1​)=bi​(yt​)j=1∑N​αt−1​(i)aij​​ . Termination . From αt(i)=P(y1,...yt,st=i∣λ) alpha _t(i)=P(y_1,...y_t, s_t=i mid lambda)αt​(i)=P(y1​,...yt​,st​=i∣λ), it cear that: . P(Y∣λ)=∑i=1NP(y1,…yT,sT=i∣λ)=∑i=1NαT(i) begin{aligned} P(Y mid lambda) &amp;= displaystyle sum_{i=1}^{N} P(y_1, ldots y_T, s_T = i mid lambda) &amp;= displaystyle sum_{i=1}^{N} alpha _T(i) end{aligned}P(Y∣λ)​=i=1∑N​P(y1​,…yT​,sT​=i∣λ)=i=1∑N​αT​(i)​ . The forward algorithm only requires about N2TN^2TN2T multiplications and is it can be implemented in Python as follows. . def forward(obs_seq): T = len(obs_seq) N = A.shape[0] alpha = np.zeros((T, N)) alpha[0] = pi*B[:,obs_seq[0]] for t in range(1, T): alpha[t] = alpha[t-1].dot(A) * B[:, obs_seq[t]] return alpha def likelihood(obs_seq): # returns log P(Y mid model) # using the forward part of the forward-backward algorithm return forward(obs_seq)[-1].sum() . Backward Algorithm . This is the same as the forward algorithm discussed in the previous sectionexcept that it start at the end and works backward toward the beginning. We first define the backward variable βt(i)=P(yt+1,yt+2…yT∣st=i,λ) beta_t(i)=P(y_{t+1},y_{t+2} ldots y_{T} mid s_t=i, { lambda})βt​(i)=P(yt+1​,yt+2​…yT​∣st​=i,λ): probability of the partial observed sequence from t+1t+1t+1 to the end at TTT given state iii at time ttt and the model λ lambdaλ. . Then βt(i) beta_t(i)βt​(i) can be recursively computed as follows. . Initialization . Let βT(i)=1 beta_{T}(i)= 1βT​(i)=1, for 1≤i≥N1 leq i geq N1≤i≥N . Induction . For t=T−1,T−2,…1t =T-1, T-2, ldots1t=T−1,T−2,…1 for 1≤i≥N1 leq i geq N1≤i≥N and by using the sum and product rules, we can rewrite βt(j) beta_t(j)βt​(j) as: . βt(i)=P(yt+1,…yT∣st=j,λ)=∑i=1NP(yt+1…yT,st+1=i∣st=j,λ)=∑i=1NP(yt+1…yT,st+1=i,st=j,λ)⋅P(st+1=i∣st=j)=∑i=1NP(yt+2…yT,st+1=i,λ)⋅P(yt+1∣st+1=i,λ)⋅P(st+1=i∣st=j)=∑i=1Naijbi(yt+1)βt+1(i) begin{aligned} beta_t(i)&amp;=P(y_{t+1}, ldots y_{T} mid s_t=j, { lambda}) &amp;= displaystyle sum_{i=1}^{N} P(y_{t+1} ldots y_T, s_{t+1}=i mid s_t=j, lambda) &amp; = displaystyle sum_{i=1}^{N} P(y_{t+1} ldots y_T, s_{t+1}=i, s_t=j, lambda) cdot P(s_{t+1}=i mid s_t=j) &amp;= displaystyle sum_{i=1}^{N} P(y_{t+2} ldots y_T, s_{t+1}=i, lambda) cdot P(y_{t+1} mid s_{t + 1}=i, lambda) cdot P(s_{t+1}=i mid s_t=j) &amp; = displaystyle sum_{i=1}^{N} a_{ij}b_i(y_{t+1}) beta _{t+1}(i) end{aligned}βt​(i)​=P(yt+1​,…yT​∣st​=j,λ)=i=1∑N​P(yt+1​…yT​,st+1​=i∣st​=j,λ)=i=1∑N​P(yt+1​…yT​,st+1​=i,st​=j,λ)⋅P(st+1​=i∣st​=j)=i=1∑N​P(yt+2​…yT​,st+1​=i,λ)⋅P(yt+1​∣st+1​=i,λ)⋅P(st+1​=i∣st​=j)=i=1∑N​aij​bi​(yt+1​)βt+1​(i)​ . Termination . β0=P(Y∣λ)=∑i=1NP(y1,…yT,s1=i)=∑i=1NP(y1,…yT∣s1=i)⋅P(s1=i)=∑i=1NP(y1∣s1=i)⋅P(y2,…yT∣s1=i)⋅P(s1=i)=∑i=1Nπibi(y1)β1(i) begin{aligned} beta_{0} &amp; = P(Y mid lambda) &amp; = displaystyle sum_{i=1}^{N} P(y_1, ldots y_T, s_1=i) &amp;= displaystyle sum_{i=1}^{N} P(y_1, ldots y_T mid s_1=i) cdot P(s_1=i) &amp; = displaystyle sum_{i=1}^{N} P(y_1 mid s_1=i) cdot P(y_2, ldots y_T mid s_1=i) cdot P(s_1=i) &amp; = displaystyle sum_{i=1}^{N} pi _i b_i(y_1) beta _1(i) end{aligned}β0​​=P(Y∣λ)=i=1∑N​P(y1​,…yT​,s1​=i)=i=1∑N​P(y1​,…yT​∣s1​=i)⋅P(s1​=i)=i=1∑N​P(y1​∣s1​=i)⋅P(y2​,…yT​∣s1​=i)⋅P(s1​=i)=i=1∑N​πi​bi​(y1​)β1​(i)​ . Python implementation of the forward algorithm is as shown below; . def backward(obs_seq): N = A.shape[0] T = len(obs_seq) beta = np.zeros((N,T)) beta[:,-1:] = 1 for t in reversed(range(T-1)): for n in range(N): beta[n,t] = np.sum(beta[:,t+1] * A[n,:] * B[:, obs_seq[t+1]]) return beta . Posterior Probability . The forward variable αt(i) alpha _t(i)αt​(i) and backward variable βt(i) beta _t(i)βt​(i) are used to calculate the posterior probability of a specific case. Now for t=1...Tt=1...Tt=1...T and i=1..Ni=1..Ni=1..N, let define posterior probability γt(i)=P(st=i∣Y,λ) gamma_t(i)=P(s_t=i mid Y, lambda)γt​(i)=P(st​=i∣Y,λ) the probability of being in state st=is_t = ist​=i at time ttt given the observation YYY and the model λ lambdaλ. . γt(i)=P(st=1,Y∣λ)P(Y∣λ)=P(y1,…yt,st=1,∣λ)P(Y∣λ) begin{aligned} gamma_t(i) &amp; = frac{P(s_t=1, Y mid lambda)}{P(Y mid lambda)} &amp;= frac{P(y_1, ldots y_t, s_t=1, mid lambda)}{P(Y mid lambda)} end{aligned}γt​(i)​=P(Y∣λ)P(st​=1,Y∣λ)​=P(Y∣λ)P(y1​,…yt​,st​=1,∣λ)​​ . Consider: . P(y1,…yt,st=1,∣λ)=P(y1,…yt∣st=1,λ)⋅P(yt+1,…yT∣st=1,λ)⋅P(st=i∣λ)=αt(i)⋅βt(i) begin{aligned} P(y_1, ldots y_t, s_t=1, mid lambda) &amp; = P(y_1, ldots y_t mid s_t=1, lambda) cdot P(y_{t+1}, ldots y_T mid s_t=1, lambda) cdot P(s_t =i mid lambda) &amp; = alpha _t(i) cdot beta _t(i) end{aligned}P(y1​,…yt​,st​=1,∣λ)​=P(y1​,…yt​∣st​=1,λ)⋅P(yt+1​,…yT​∣st​=1,λ)⋅P(st​=i∣λ)=αt​(i)⋅βt​(i)​ . Thus . γt(i)=αt(i)⋅βt(i)P(Y∣λ) gamma_t(i) = frac{ alpha _t(i) cdot beta _t(i)}{P(Y mid lambda)}γt​(i)=P(Y∣λ)αt​(i)⋅βt​(i)​ . where . P(Y∣λ)=∑i=1NαT(i)P(Y mid { lambda}) = displaystyle sum_{i=1}^{N} alpha _T(i)P(Y∣λ)=i=1∑N​αT​(i) . In python: . def gamma(obs_seq): alpha = forward(obs_seq) beta = backward(obs_seq) obs_prob = likelihood(obs_seq) return (np.multiply(alpha,beta.T) / obs_prob) . We can use γt(i) gamma_t(i)γt​(i) to find the most likely state at time ttt which is the state st=is_t=ist​=i for which γt(i) gamma_t(i)γt​(i) is maximum. This algorithm works fine in the case when HMM is ergodic i.e., there is the transition from any state to any other state. If applied to an HMM of another architecture, this approach could give a sequence that may not be a legitimate path because some transitions are not permitted. To avoid this problem, Viterbi algorithm is the most common decoding algorithm used. . Viterbi Algorithm . Viterbi is a kind of dynamic programming algorithm that makes uses of a dynamic programming trellis. . The virtebi algorithm offer an efficient way of finding the single best state sequence.Let define the highest probability along a single path, at time ttt, which accounts for the first ttt observations and ends in state jjj using a new notation: . δt(i)=max⁡s1,…st−1P(s1,…st=1,y1,…yt∣λ) begin{aligned} delta_t(i) &amp; = max_{s_1, ldots s_{t-1}} P(s_1, ldots s_t =1, y_1, ldots y_t mid lambda) end{aligned}δt​(i)​=s1​,…st−1​max​P(s1​,…st​=1,y1​,…yt​∣λ)​ . By induction, a recursive formula of δt+1(i) delta_{t+1}(i)δt+1​(i) from δt(i) delta_t(i)δt​(i) is derived to calculate this probability as follows: . Consider the joint distribution appearing in δt+1(i) delta_{t+1}(i)δt+1​(i), which can be rewritten when st+1=is_{t+1}=ist+1​=i and st=js_t = jst​=j as: . P(s1,…,st=j,st+1=i,y1,…yt,yt+1∣λ)=P(s1…st=j,y1,…yt∣λ)×P(st+1=i,yt+1∣s1,…st,y1,…yt,λ)=P(s1…st=j,y1,…yt∣λ)⋅P(st+1∣st,λ)×P(yt+1∣st+1,λ)=P(s1…st=j,y1,…yt∣λ)⋅aijbi(yt+1) begin{aligned} P(s_1, ldots, s_t=j,s_{t+1}=i, y_1, ldots y_t, y_{t+1} mid lambda) &amp; = P(s_1 ldots s_t=j, y_1, ldots y_t mid lambda) &amp; times P(s_{t+1}=i,y_{t+1} mid s_1, ldots s_t, y_1, ldots y_t, lambda) &amp; = P(s_1 ldots s_t=j, y_1, ldots y_t mid lambda) cdot P(s_{t+1} mid s_t, lambda) &amp; times P(y_{t+1} mid s_{t+1}, lambda) &amp; = P(s_1 ldots s_t=j, y_1, ldots y_t mid lambda) cdot a_{ij}b_i(y_{t+1}) end{aligned}P(s1​,…,st​=j,st+1​=i,y1​,…yt​,yt+1​∣λ)​=P(s1​…st​=j,y1​,…yt​∣λ)×P(st+1​=i,yt+1​∣s1​,…st​,y1​,…yt​,λ)=P(s1​…st​=j,y1​,…yt​∣λ)⋅P(st+1​∣st​,λ)×P(yt+1​∣st+1​,λ)=P(s1​…st​=j,y1​,…yt​∣λ)⋅aij​bi​(yt+1​)​ . Thus δt+1(i) delta_{t+1}(i)δt+1​(i) is computed recursively from δt+1(j) delta_{t+1}(j)δt+1​(j) as: . δt+1(i)=max⁡s1,…st=jP(s1…st=j,y1,…yt∣λ)⋅aijbi(yt+1)=max⁡j[δt(j)aij]⋅bi(yt+1) begin{aligned} delta_{t+1}(i) &amp;= max_{s_1, ldots s_{t}=j} P(s_1 ldots s_t=j, y_1, ldots y_t mid lambda) cdot a_{ij}b_i(y_{t+1}) &amp; = max_{j} Big[ delta_t(j) a_{ij} Big] cdot b_i(y_{t+1}) end{aligned}δt+1​(i)​=s1​,…st​=jmax​P(s1​…st​=j,y1​,…yt​∣λ)⋅aij​bi​(yt+1​)=jmax​[δt​(j)aij​]⋅bi​(yt+1​)​ . Therefore, we need to keep track of the state that maximizes the above equation to backtrack to the single best state sequence in the following Viterbi algorithm: . Initialization . For 1≤i≥N1 leq i geq N1≤i≥N, let: . δ1(i)=πsibi(y1)Θ1(i)=0 begin{aligned} delta _1(i)&amp;= pi _{s_i}b_i(y_1) Theta _1(i)&amp;=0 end{aligned}δ1​(i)Θ1​(i)​=πsi​​bi​(y1​)=0​ . Recursion . Calculate the ML (maximum likelihood) state sequences and their probabilities. For t=2,3,...Tt=2,3,...Tt=2,3,...T and 1≤i≥N1 leq i geq N1≤i≥N . δt(i)=max⁡jϵ1,..N[δt−1(j)aij]⋅bi(yt)Θt(i)=arg⁡max⁡j[δt−1(j)aij] begin{aligned} delta_t(i) &amp; = displaystyle max_{j epsilon{1,..N}} Big[ delta_{t-1}(j)a_{ij} Big] cdot b_i(y_t) Theta_t(i) &amp; = arg max_j Big[ delta_{t-1}(j)a_{ij} Big] end{aligned}δt​(i)Θt​(i)​=jϵ1,..Nmax​[δt−1​(j)aij​]⋅bi​(yt​)=argjmax​[δt−1​(j)aij​]​ . Termination: . Retrieve the most likely final state . P^=max⁡jϵ1,..N[δT(j)]S^T=arg⁡max⁡j[δT(j)] begin{aligned} hat{P} &amp;= displaystyle max_{j epsilon{1,..N}}[ delta_T(j)] hat{S}_T &amp; = arg max_j [ delta_T(j)] end{aligned}P^S^T​​=jϵ1,..Nmax​[δT​(j)]=argjmax​[δT​(j)]​ . State sequence backtracking: . Retrieve the most likely state sequences (Viterbi path) . S^t=Θt+1(S^t+1), where t=T−1,T−2,…1 hat{S}_t = Theta_{t+1}( hat{S}_{t+1}) text{, where } t=T-1,T-2, ldots1S^t​=Θt+1​(S^t+1​), where t=T−1,T−2,…1 . The Viterbi algorithm uses the same schema as the Forward algorithm except for two differences: . It uses maximization in place of summation at the recursion and termination steps. | It keeps track of the arguments that maximize δt(i) delta_t(i)δt​(i) for each ttt and iii, storing them in the N by T matrix Θ ThetaΘ. This matrix is used to retrieve the optimal state sequence at the backtracking step. | Python implementation of virtebi algorithm . def viterbi(obs_seq): # returns the most likely state sequence given observed sequence x # using the Viterbi algorithm T = len(obs_seq) N = A.shape[0] delta = np.zeros((T, N)) psi = np.zeros((T, N)) delta[0] = pi*B[:,obs_seq[0]] for t in range(1, T): for j in range(N): delta[t,j] = np.max(delta[t-1]*A[:,j]) * B[j, obs_seq[t]] psi[t,j] = np.argmax(delta[t-1]*A[:,j]) # backtrack states = np.zeros(T, dtype=np.int32) states[T-1] = np.argmax(delta[T-1]) for t in range(T-2, -1, -1): states[t] = psi[t+1, states[t+1]] return states . To summarize, we can compute the following from HMM: . The marginalized likelihood function P(Y∣λ)P(Y mid lambda)P(Y∣λ) from the forward or backward algorithm. | The posterior probability γt(i)=P(st=i∣Y,λ) gamma_t(i) = P(s_t=i mid Y, lambda)γt​(i)=P(st​=i∣Y,λ) from the forward–backward algorithm. | The optimal state sequence S^=max⁡sP(S∣Y,λ)=max⁡sP(S,Y∣λ) hat{S} = max_{s} P(S mid Y, lambda) = max_{s} P(S, Y mid lambda)S^=maxs​P(S∣Y,λ)=maxs​P(S,Y∣λ)from the Viterbi algorithm. | The segmental joint likelihood function P(S^,Y∣λ)P( hat{S},Y mid lambda)P(S^,Y∣λ) from the Viterbi algorithm. | These values are used in the decoding step, and the training step of estimating model parameters λ lambdaλ. . Example 1 . Consider the Bob-Alice example as described here. Two friends, Alice and Bob, who live far apart and talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no specific information about the weather where Bob lives, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been. . Alice believes that the weather operates as a discrete Markov chain. There are two states, “Rainy” and “Sunny”, but she cannot observe them directly; that is, they are hidden from her. On each day, there is a pure chance that Bob will perform one of the following activities, depending on the weather: “walk”, “shop”, or “clean”. Since Bob tells Alice about his actions, those are the observations. . states = (&#39;Rainy&#39;, &#39;Sunny&#39;) observations = (&#39;walk&#39;, &#39;shop&#39;, &#39;clean&#39;) pi = np.array([0.6, 0.4]) #initial probability A = np.array([[0.7, 0.3],[0.4, 0.6]]) #Transmission probability B = np.array([[0.1, 0.4, 0.5],[0.6, 0.3, 0.1]]) #Emission probability . Suppose Bob says walk, clean, shop, shop, clean, walk. What will Alice hears. . bob_says = np.array([0, 2, 1, 1, 2, 0]) alice_hears=viterbi(bob_says) print(&quot;Bob says:&quot;, &quot;, &quot;,list(map(lambda y: observ_bob[y], bob_says))) print(&quot;Alice hears:&quot;, &quot;, &quot;, list(map(lambda s: states_bob[s], alice_hears))) . (&#39;Bob says:&#39;, &#39;walk, clean, shop, shop, clean, walk&#39;) (&#39;Alice hears:&#39;, &#39;Sunny, Rainy, Rainy, Rainy, Rainy, Sunny&#39;) The notebook with codes for the above example can be found in here . References . L. R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proceedings of the IEEE, Vol. 77, No. 2, February 1989. | Shinji Watanabe, Jen-Tzung Chien, Bayesian Speech and Language Processing, Cambridge University Press, 2015. | Viterbi Algorithm in Speech Enhancement and HMM | Nikolai Shokhirev, Hidden Markov Models |",
            "url": "http://localhost:4000/sambaiga/machine%20learning/2017/05/03/hmm-intro.html",
            "relUrl": "/machine%20learning/2017/05/03/hmm-intro.html",
            "date": " • May 3, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Portfolio",
          "content": "Anthony Faustine is a Data Scientist at CeADAR (UCD), Dublin, Ireland with over four years of successful experience in data analytics and Artificial Intelligence techniques for multiple applications. He effectively investigates methods for novel approaches to problems and develops prototypes to assess their viability. Although Anthony is a person who takes the initiative, he has a strong team spirit with experience working in a highly international environment. He is proficient in machine learning techniques and deep learning for multiple applications with sound data analytical, data visualization, and programming skills, especially in Python. . At CeADER, Anthony is devising and implementing data analytics/AI technical solutions for various application domains. He is also involved in the research and development of the applicability of Artificial Intelligence for Earth Observation (AI4EO). He is currently finalizing a six-month research project that explores how EO imagery data can be combined with geospatial data to automate the process of generating an annotated dataset for building AI models specific to EO imagery. During this time, Mr. Anthony successfully led the team to develop a flexible end-to-end pipeline that combines satellite imagery and geo-referenced dataset to create the annotated dataset and build machine learning models. He also works on another project in partnership with the European Space Agency(ESA) to investigate specifications and best practices for creating the EO dataset for AI applications. . Mr. Faustine received the B.sc. Degree in Electronics Science and Communication from the University of Dar es Salaam, Tanzania, and the M.sc. Degree in Telecommunications Engineering from the University of Dodoma, Tanzania, in 2010. From 2010 to 2017, he worked as an assistant lecturer at the University of Dodoma, Tanzania, where he was involved in several research projects within the context of ICT4D. . In 2017, Anthony joined IDLab, imec research group of the University of Ghent, in Belgium as a Machine learning researcher. His research focused on machine learning techniques applied to energy smart-meter data. He was also involved in the development of machine learning models that analyze data monitored from the internet of things (IoT) devices to detect and classify activities in the smart-homes environment. This opportunity has allowed me to drastically expand his research, data analytics, and machine learning skills. Working closely with other departments allowed him to improve his interpersonal skills and present complex technical information to technical and non-technical personnel. Since Feb 2020, Mr. Faustine has been working with Dr. Lucas Pereira of ITI, LARSyS, Técnico Lisboa &amp; Prsma.com on robust machine learning techniques for energy-disaggregation and how energy-disaggregation could solve real-world problems in smart-grid. These opportunities have allowed him to drastically expand his technical ability, critical thinking, project management, and research skills. . By participating in different research projects, Mr. Faustine has learned to describe the project’s specific objectives and its excellence and impact. He has also gained experience writing grant proposals and scientific publications, handling and analyzing large datasets, and generating insights from data. . Moreover, Mr. Faustine is competent in communicating complex ideas to groups of different disciplines in simple language. He acquired these abilities from his previous job as an assistant lecturer. The competence advanced through volunteer activities where He organized and facilitated a capacity development workshop on data science and machine learning to university students in Tanzania. As a result, Mr. Faustine can conduct training, demonstrations, and sharing results to technical and non-technical audiences. . Mr. Faustine’s research interests lie in the intersections between Machine learning, Signal processing, and Computational Sustainability. He works towards bridging the gap between research and real-world applicability of machine learning for sustainable development. .",
          "url": "http://localhost:4000/sambaiga/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "Project 1 . a project with a background image . Water Resources Governance System (WaGoSy) . Water Resources Governance System (WaGoSy) . UjamaaSMS - Project . . DVS - Project . . e-ADR - Project . Adverse Reaction e-Reporting System (ADR) . Robust Machine learning methods for Non-Intrusive Load Monitoring (NILM) . Applying machine learning techniques to recognize the labeled appliances and estimate their energy consumption once they are switched on . AI for Earth Observation . AI for Earth Observation Data . AIREO . AI-ready EO training datasets (AIREO) Dataset specifications &amp; Best-practices . Project 2 . a project with a background image . Project 3 . a project that redirects to another website . Project 4 . another without an image . Project 5 . a project with a background image . Project 6 . a project with no image . AIREO . AI-ready EO training datasets (AIREO) Dataset .",
          "url": "http://localhost:4000/sambaiga/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Project",
          "content": "",
          "url": "http://localhost:4000/sambaiga/project/",
          "relUrl": "/project/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Publications",
          "content": "2020 . A. Faustine, L. Pereira, and C. Klemenjak. Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring. IEEE Transactions on Smart Grid, :1–1, 2020. [doi: 10.1109/TSG.2020.3010621] [Abs] [] [] To this day, hyperparameter tuning remains a cumbersome task in Non-Intrusive Load Monitoring (NILM) research, as researchers and practitioners are forced to invest a considerable amount of time in this task. This paper proposes adaptive weighted recurrence graph blocks (AWRG) for appliance feature representation in event-based NILM. An AWRG block can be combined with traditional deep neural network architectures such as Convolutional Neural Networks for appliance recognition. Our approach transforms one cycle per activation current into an weighted recurrence graph and treats the associated hyper-parameters as learn-able parameters. We evaluate our technique on two energy datasets, the industrial dataset LILACD and the residential PLAID dataset. The outcome of our experiments shows that transforming current waveforms into weighted recurrence graphs provides a better feature representation and thus, improved classification results. It is concluded that our approach can guarantee uniqueness of appliance features, leading to enhanced generalisation abilities when compared to the widely researched V-I image features. Furthermore, we show that the initialisation parameters of the AWRG’s have a significant impact on the performance and training convergence. . | Anthony Faustine and Lucas Pereira. Multi-label Learning for Appliance Recognition in NILM using Fryze-Current Decomposition and Convolutional Neural Network. Energies, 2020. [Abs] The advance in energy-sensing and smart-meter technologies have motivated the use of a Non-Intrusive Load Monitoring (NILM), a data-driven technique that recognizes active end-use appliances by analyzing the data streams coming from these devices. NILM offers an electricity consumption pattern of individual loads at consumer premises, which is crucial in the design of energy efficiency and energy demand management strategies in buildings. Appliance classification, also known as load identification is an essential sub-task for identifying the type and status of an unknown load from appliance features extracted from the aggregate power signal. Most of the existing work for appliance recognition in NILM uses a single-label learning strategy which, assumes only one appliance is active at a time. This assumption ignores the fact that multiple devices can be active simultaneously and requires a perfect event detector to recognize the appliance. In this paper proposes the Convolutional Neural Network (CNN)-based multi-label learning approach, which links multiple loads to an observed aggregate current signal. Our approach applies the Fryze power theory to decompose the current features into active and non-active components and use the Euclidean distance similarity function to transform the decomposed current into an image-like representation which, is used as input to the CNN. Experimental results suggest that the proposed approach is sufficient for recognizing multiple appliances from aggregated measurements. . | Anthony Faustine and Lucas Pereira. Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks. Energies, 13(13) :3374, 2020. [doi: 10.3390/en13133374] [Abs] [] Appliance recognition is one of the vital sub-tasks of NILM in which a machine learning classier is used to detect and recognize active appliances from power measurements. The performance of the appliance classifier highly depends on the signal features used to characterize the loads. Recently, different appliance features derived from the voltage–current (V–I) waveforms have been extensively used to describe appliances. However, the performance of V–I-based approaches is still unsatisfactory as it is still not distinctive enough to recognize devices that fall into the same category. Instead, we propose an appliance recognition method utilizing the recurrence graph (RG) technique and convolutional neural networks (CNNs). We introduce the weighted recurrent graph (WRG) generation that, given one-cycle current and voltage, produces an image-like representation with more values than the binary output created by RG. Experimental results on three different sub-metered datasets show that the proposed WRG-based image representation provides superior feature representation and, therefore, improves classification performance compared to V–I-based features . | 2019 . Christoph Klemenjak, Anthony Faustine, Stephen Makonin, and Wilfried Elmenreich. On Metrics to Assess the Transferability of Machine Learning Models in Non-Intrusive Load Monitoring. ArXiv, abs/1912.06200 2019. [doi: ] [Abs] [] To assess the performance of load disaggregation algorithms it is common practise to train a candidate algorithm on data from one or multiple households and subsequently apply cross-validation by evaluating the classification and energy estimation performance on unseen portions of the dataset derived from the same households. With an emerging discussion of transferability in Non-Intrusive Load Monitoring (NILM), there is a need for domain-specific metrics to assess the performance of NILM algorithms on new test scenarios being unseen buildings. In this paper, we discuss several metrics to assess the generalisation ability of NILM algorithms. These metrics target different aspects of performance evaluation in NILM and are meant to complement the traditional performance evaluation approach. We demonstrate how our metrics can be utilised to evaluate NILM algorithms by means of two case studies. We conduct our studies on several energy consumption datasets and take into consideration five state-of-the-art as well as four baseline NILM solutions. Finally, we formulate research challenges for future work . | Dirk Deschrijver Anthony Faustine and Tom Dhaene. Improved Appliance Classification in NILM using Recurrence Plots and Convolutional Neural Networks. In: EU NILM Workshop, Thessaloniki, Greece 2019. [] | 2017 . Anthony Faustine. Convolutional Neural Network for Appliance Classification in NILM. In: Data Science Africa, Arusha, Tanzania 2017. | Anthony Faustine, Nerey Henry Mvungi, Shubi Kaijage, and Kisangiri Michael. A Survey on Non-Intrusive Load Monitoring Methodies and Techniques for Energy Disaggregation Problem. CoRR, abs/1703.00785 2017. [doi: 1223334555] [Abs] [arXiv] [] The rapid urbanization of developing countries coupled with explosion in construction of high rising buildings and the high power usage in them calls for conservation and efficient energy program. Such a program require monitoring of end-use appliances energy consumption in real-time. The worldwide recent adoption of smart-meter in smart-grid, has led to the rise of Non-Intrusive Load Monitoring (NILM); which enables estimation of appliance-specific power consumption from building’s aggregate power consumption reading. NILM provides households with cost-effective real-time monitoring of end-use appliances to help them understand their consumption pattern and become part and parcel of energy conservation strategy. This paper presents an up to date overview of NILM system and its associated methods and techniques for energy disaggregation problem. This is followed by the review of the state-of-the art NILM algorithms. Furthermore, we review several performance metrics used by NILM researcher to evaluate NILM algorithms and discuss existing benchmarking framework for direct comparison of the state of the art NILM algorithms. Finally, the paper discuss potential NILM use-cases, presents an overview of the public available dataset and highlight challenges and future research directions. . | 2016 . Maria Gabriel Faustine Anthony and Bertha Shao. Open Source Cellular Technologies for Cost Effective Cellular Connectivity in Rural Areas. International Journal of Computer Applications, 146(15) 2016. [doi: 10.5120/ijca2016910894] [Abs] [] [] Cellular coverage is often a challenge in low population density and low income rural areas of the developing world like Tanzania. This is because big Telecom companies defer from deploying their expensive infrastructure in these areas fearing operations cost. It is proposed that the use of cellular open source technology as the basis for a new rural cellular network can go a long way in meeting this challenge by providing coverage in rural areas whilst simultaneously bringing down the cost of communication. This paper provides survey of recent open source cellular technologies. It further presents a prototype based on Open BTS for cellular connectivity in developing countries. In lab performance test for the proposed prototype confirm the feasibility of deploying cellular network based on open source technologies as an alternative to conventional mobile operator networks in a bid to solve the challenge. . | 2015 . Aloys N Mvuma Nyaura Kibinda and Anthony Faustine. Handover Algorithm for Machine Type Communication in LTE Network. Informatics and Virtual Education, 3(1) 2015. [Abs] [] Machine Type Communication (MTC) is a new type of data communication between machines and devices without human interactions. Long Term Evolution (LTE) is a recent third Generation Partnership Program (3GPP) cellular standard and is a promising technology to support future MTC data traffic. This paper evaluates two existing handover algorithms namely A2-A4-RSRQ and A3-RSRP. Based on the analysis of the optimal settings of both algorithms, the performances of the selected algorithms were compared and the results proved that A2-A4-RSRQ performs better than A3-RSRP. A2-A4-RSRQ handover algorithm is able to maintain acceptable throughput and handover delay as per 3GPP specification. . | 2014 . Anthony Faustine and Aloys N. Mvuma. Ubiquitous Mobile Sensing for Water Quality Monitoring and Reporting within Lake Victoria Basin. Wireless Sensor Network, 6(12) :257–264, 2014. [doi: 10.4236/wsn.2014.612025] [Abs] [] [] [] As the human population growth and industry pressure in most developing countries continue to increase, effective water quality monitoring and evaluation has become critical for water resources management programs. This paper presents the ubiquitous mobile sensing system for water quality data collection and monitoring applications in developing countries. The system was designed based on the analysis of the existing solution. Open source hardware and software was used to develop the prototype of the system. Field testing of the system conducted in Nkokonjero, Uganda and Mwanza, Tanzania verified the functionalities of the system and its practical application in actual environment. Results show that proposed solution is able to collect and present data in a mobile environment. . | Aloys N Mvuma Anthony Faustine and Hector J Mongi. Wireless Sensor Networks for Water Quality Monitoring and Control within Lake Victoria Basin: Prototype Development. Wireless Sensor Network, 6(12) :281–290, 2014. [doi: 10.4236/wsn.2014.612027] [Abs] [] [] [] The need for effective and efficient monitoring, evaluation and control of water quality in Lake Victoria Basin (LVB) has become more demanding in this era of urbanization, population growth and climate change and variability. Traditional methods that rely on collecting water samples, testing and analyses in water laboratories are not only costly but also lack capability for real-time data capture, analyses and fast dissemination of information to relevant stakeholders for making timely and informed decisions. In this paper, a Water Sensor Network (WSN) system prototype developed for water quality monitoring in LVB is presented. The development was preceded by evaluation of prevailing environment including availability of cellular network coverage at the site of operation. The system consists of an Arduino microcontroller, water quality sensors, and a wireless network connection module. It detects water temperature, dissolved oxygen, pH, and electrical conductivity in real-time and disseminates the information in graphical and tabular formats to relevant stakeholders through a web-based portal and mobile phone platforms. The experimental results show that the system has great prospect and can be used to operate in real world environment for optimum control and protection of water resources by providing key actors with relevant and timely information to facilitate quick action taking. . | Deo Shao Anthony Faustine and Aloys Mvuma. Combating Medical Drug Counterfeit in Tanzania: The Role of Technology. In: Proceedings of the 1st Cotul Annual Conference, , Ruaha Catholic University, Iringa, Tanzania, 2014. [Abs] he infiltration of Counterfeit Drugs (CDs) in the Pharmaceutical Supply Chain (PSC) is adversely affecting health sectors in their efforts to provide quality health services to the public. According to the World Health Organisation (WHO), over 30 percent of anti-malaria drugs sold in the developing countries are substandard. Furthermore, the Confederation of Tanzania Industries (CTI) estimated that 60 percent of the medicines imported into Tanzania are counterfeits. Several initiatives have been taken in an attempt to curb the selling of CDs. However, the effectiveness of these initiatives remains questionable. There is an urgent need to formulate more effective strategies to secure the PSC by involving all stakeholders. In this regard, technology-based solution could be an effective uptake in mitigating CDs. This study assesses how the technology-based system can secure PSC and enable medical drugs verification in Tanzania. The proven adoption of Information and Communication Technologies (ICTs), especially mobile technology, could be leveraged to enable the enforcement of laws and regulations and advocate community awareness on medical safety . | 2013 . Anthony Faustine and Aloys N Mvuma. Design and Simulation of Wireless Sensor Network for Water Quality Monitoring Around Lake Victoria. Journal of Informatics and Virtual Education, :37, 2013. | Anthony Faustine and Lucian Ngeze. Enhancing Teaching and Learning Communication Network Courses in Higher Learning Institutions in Tanzania Using Simulations. Journal of Informatics and Virtual Education, :30, 2013. | 2012 . Anthony Faustine. Design of Wireless Sensor Network for Water Quality Monitoring around Lake Victoria Basin in Tanzania, Master&#39;s thesis, The University of Dodoma, 2012. [Abs] Water quality monitoring and evaluation are important practices for water resources management. However for decades, water quality monitoring and evaluation in developing countries like Tanzania have depended on costly, time- and labor-intensive on-site sampling and data collection and transport to laboratories for evaluation. Emerging technologies like Wireless Sensor Network (WSN) can be used to provide relatively inexpensive, coordinated, intelligent networks allowing a well coordinated and continuous monitoring of water quality. This dissertation investigates the application of WSN in monitoring water quality and propose a low cost water quality monitoring system based on a WSN, which is to be applied to monitor water quality around Lake Victoria basin. Appropriate open source WSN technologies are identified and the proposed network is then analyzed through simulation using NS-2 with the aim of estimating the maximum number of nodes, maximum number of sources nodes and transmit power level that could be used while still achieving a desirable network performance. Using Radio Mobile software, this dissertation has demonstrated that long distant WSN link is possible in Musoma with link margin greater than 10dB and Fresnel zone greater than 0.6F. . |",
          "url": "http://localhost:4000/sambaiga/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Resources",
          "content": "",
          "url": "http://localhost:4000/sambaiga/resources.html",
          "relUrl": "/resources.html",
          "date": ""
      }
      
  

  
  

  
  

  

  
      ,"page10": {
          "title": "Talk",
          "content": "",
          "url": "http://localhost:4000/sambaiga/talks/",
          "relUrl": "/talks/",
          "date": ""
      }
      
  

  
  

  
  

  
      ,"page13": {
          "title": "Blogs",
          "content": "",
          "url": "http://localhost:4000/sambaiga/post/index.html",
          "relUrl": "/post/index.html",
          "date": ""
      }
      
  

  
      ,"page14": {
          "title": "Blogs",
          "content": "",
          "url": "http://localhost:4000/sambaiga/post/index.html",
          "relUrl": "/post/index.html",
          "date": ""
      }
      
  

  
  

  
      ,"page16": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://localhost:4000/sambaiga/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}