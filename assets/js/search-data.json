{
  
    
        "post0": {
            "title": "Vectorization and Distribution shapes in Pyro",
            "content": "Introduction . In the previous post we introduced pyro and its building blocks such as schotastic function, primitive sample and param primitive statement, model and guide. We also defined pyro model and use it to generate data, learn from data and predict future observations. . In this section, we will learn in details about inference in Pyro, how to use Pyro primitives and the effect handling library (pyro.poutine) to build custom tools for analysis. . Consider a previous poison regression model . import torch import pyro import pyro.distributions as dist from torch.distributions import constraints import matplotlib.pyplot as plt import seaborn as sns import numpy as np pyro.set_rng_seed(101) torch.manual_seed(101) %matplotlib inline . def model_(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) for t in range(len(y)): rate = torch.exp(intercept + slope * t) y[t] = pyro.sample(&quot;count_{}&quot;.format(t), dist.Poisson(rate), obs=y[t]) . Plate statement . From the given model above , pyro.param designate model parameters that we would like to optimize. Observations are denoted by the obs= keyword argument to pyro.sample. This specifies the likelihood function. Instead of log transforming the data, we use a LogNormal distribution. The observations are conditionally independent given the latent random variable slope and intercept. To explicitly mark this in Pyro, plate statement is used to construct conditionally independent sequences of variables. . with pyro.plate(&quot;name&quot;, size, subsample_size, device) as ind: # ...do conditionally independent stuff with ind... . However compared to range() each invocation of plate requires the user to provide a unique name. The plate statement can be used either sequentially as a generator or in parallel as a context manager. Sequential plate is similar to range()in that it generates a sequence of values. . # This version declares sequential independence and subsamples data: for i in plate(&#39;data&#39;, 100, subsample_size=10): if z[i]: # Control flow in this example prevents vectorization. obs = sample(&#39;obs_{}&#39;.format(i), dist.Normal(loc, scale), obs=data[i]) . Vectorized plate is similar to torch.arange() in that it yields an array of indices by which other tensors can be indexed. However, unlike torch.arange() plate also informs inference algorithms that the variables being indexed are conditionally independent. . # This version declares vectorized independence: with plate(&#39;data&#39;): obs = sample(&#39;obs&#39;, dist.Normal(loc, scale), obs=data) . Additionally, plate can take advantage of the conditional independence assumptions by subsampling the indices and informing inference algorithms to scale various computed values. This is typically used to subsample minibatches of data: . with plate(&quot;data&quot;, len(data), subsample_size=100) as ind: batch = data[ind] assert len(batch) == 100 . You can additionally nest plates, e.g. if you have per-pixel independence: . with pyro.plate(&quot;x_axis&quot;, 320): # within this context, batch dimension -1 is independent with pyro.plate(&quot;y_axis&quot;, 200): # within this context, batch dimensions -2 and -1 are independent . Finaly you can declare multiple plates and use them as reusable context managers. For example if you want to mix and match plates for e.g. noise that depends only on x, some noise that depends only on y, and some noise that depends on both . x_axis = pyro.plate(&quot;x_axis&quot;, 3, dim=-2) y_axis = pyro.plate(&quot;y_axis&quot;, 2, dim=-3) with x_axis: # within this context, batch dimension -2 is independent with y_axis: # within this context, batch dimension -3 is independent with x_axis, y_axis: # within this context, batch dimensions -3 and -2 are independent . def model_(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) with pyro.plate(&#39;N&#39;, len(y)) as t: log_y_hat = slope * t.type(torch.float) + intercept y=pyro.sample(&#39;y&#39;, dist.LogNormal(log_y_hat, 1.), obs=y) . Distribution shapes . Unlike PyTorch Tensors which have a single .shape attribute, pyro Distributions have two shape batch_shape and event_shape. These two combine to define the total shape of a sample. The batch_shape denote conditionally independent random variables, whereas .event_shape denote dependent random variables (ie one draw from a distribution). Because the dependent random variables define probability together, the .log_prob() method only produces a single number for each event of shape .event_shape. . d = dist.Bernoulli(0.5) print(d.batch_shape) print(d.event_shape) . torch.Size([]) torch.Size([]) . x = d.sample() x.shape . torch.Size([]) . Distributions can be batched by passing in batched parameters. . d = dist.Bernoulli(0.5*torch.ones(50)) print(d.batch_shape) print(d.event_shape) . torch.Size([50]) torch.Size([]) . x = d.sample() x.shape . torch.Size([50]) . From the two examples above, we observe that univariate distributions have empty event shape (because each number is an independent event). Let also consider multivariate distribution. . md = dist.MultivariateNormal(torch.zeros(3), torch.eye(3)) print(md.batch_shape) print(md.event_shape) . torch.Size([]) torch.Size([3]) . y = md.sample() y.shape . torch.Size([3]) . We can also create batched multivariate distribution as follows. . md = dist.MultivariateNormal(torch.zeros(3), torch.eye(3)).expand([50]) print(md.batch_shape) print(md.event_shape) . torch.Size([50]) torch.Size([3]) . y = md.sample() y.shape . torch.Size([50, 3]) . Because Multivariate distributions have nonempty .event_shape, the shapes of .sample() and .log_prob(x) differ: . md.log_prob(y).shape . torch.Size([50]) . The Distribution.sample() method also takes a sample_shape parameter that indexes over independent identically distributed (iid) random varables, such that: . sample.shape == sample_shape + batch_shape + event_shape . y_sample =md.sample([10]) y_sample.shape . torch.Size([10, 50, 3]) . Reshaping distributions . You can treat a univariate distribution as multivariate by calling the .to_event(n) property where n is the number of batch dimensions (from the right) to declare as dependent. . d = dist.Bernoulli(0.5*torch.ones(50, 3)).to_event(1) print(d.batch_shape) print(d.event_shape) . torch.Size([50]) torch.Size([3]) . While working with distributions in pyro it is essential to note that: . Samples have shape batch_shape + event_shape, | .log_prob(x) values have shape batch_shape. | You’ll need to ensure that batch_shape is carefully controlled by either trimming it down with .to_event(n) or by declaring dimensions as independent via pyro.plate. | Often in Pyro we’ll declare some dimensions as dependent even though they are in fact independent. This allows us to easily swap in a MultivariateNormal distribution later, but aslo it simplifies the code as we don’t need a plate. Consider the following two codes . x = pyro.sample(&quot;x&quot;, dist.Normal(0, 1).expand([10]).to_event(1)) . x.shape . torch.Size([10]) . with pyro.plate(&quot;y_plate&quot;, 10): y = pyro.sample(&quot;y&quot;, dist.Normal(0, 1)) # .expand([10]) is automatic . y.shape . torch.Size([10]) . From the two code examples, the second version with plate informs Pyro that it can make use of conditional independence information when estimating gradients, whereas in the first version Pyro must assume they are dependent (even though the normals are in fact conditionally independent). .",
            "url": "https://sambaiga.github.io/sambaiga/jupyter/2020/03/04/ppl-pyro-two.html",
            "relUrl": "/jupyter/2020/03/04/ppl-pyro-two.html",
            "date": " • Mar 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Probabilistic Programming with Pyro",
            "content": "Intro to Pyro . Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. It enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling. . Models and Probability distributions . Models are the basic unit of probabilistic programs in pyro, they represent simplified or abstract descriptions of a process by which data are generated. Models in pyro are expressed as stochastic functions which implies that models can be composed, reused, imported, and serialized just like regular Python callables. Probability distributions (pimitive stochastic functions) are important class of models (stochastic functions) used explicitly to compute the probability of the outputs given the inputs. Pyro uses PyTorch’s distribution library which contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. Each probability distributions are equipped with several methods such as: . prob(): $ log p( mathbf{x} mid theta ^{*})$ | mean: $ mathbb{E}_{p( mathbf{x} mid theta ^{*})}[ mathbf{x}]$ | sample: $ mathbf{x}^{*} sim {p( mathbf{x} mid theta ^{*})}$ | . You can also create custom distributions using transforms. . Example 1: Let define the unit normal distribution $ mathcal{N}(0,1)$, draw sample $x$ and compute the log probability according to the distribution. . import torch import pyro import pyro.distributions as dist from torch.distributions import constraints import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pyro.set_rng_seed(101) torch.manual_seed(101) torch.set_printoptions(precision=3) %matplotlib inline . mu = 0 sigma = 1 normal=dist.Normal(mu, sigma) x = normal.rsample() # draw a sample from N(1,1) print(&quot;sample&quot;, x.item()) #To compute the log probability according to the distribution print(&quot;prob&quot;, torch.exp(normal.log_prob(x)).item()) # score the sample from N(1,1) . sample -1.3905061483383179 prob 0.15172401070594788 . Sample and Param statements . Pyro simplifies the process of sampling from distributions with the use of pyro.sample statement. The pyro.sample statement call stochastic functions or models with a unique name as identifier. Pyro’s backend uses these names to uniquely identify sample statements and change their behavior at runtime depending on how the enclosing stochastic function is being used. Using pyro.sample statement, Pyro can implement various manipulations that underlie inference algorithms. . x = pyro.sample(&quot;name&quot;, fn, obs) &quot;&quot;&quot; name – name of sample fn – distribution class or function obs – observed datum (optional; should only be used in context of inference) optionally specified in kwargs &quot;&quot;&quot; . Example 2: Let sample from previous normal distribution created in example 1. . mu = 0 sigma = 1 x = pyro.sample(&quot;my_sample&quot;, dist.Normal(mu, sigma)) print(x) . tensor(-0.815) . The above code generate a random value and records it in the Pyro runtime. . data=2 x = pyro.sample(&quot;my_sample&quot;, dist.Normal(mu, sigma), obs=data) print(x) . 2 . /opt/miniconda3/lib/python3.7/site-packages/pyro/primitives.py:86: RuntimeWarning: trying to observe a value outside of inference at my_sample RuntimeWarning) . The above code conditions a stochatsic function on observed data. This should run on inference. . Pyro use pyro.param statement to saves the variable as a parameter in the param store. To interact with the param store. The pyro.param statement is used by pyro to declares a learnable parameter. . x = pyro.param(&quot;name&quot;, init_value, constraints) &quot;&quot;&quot; name – name of param init_value – initial value constraint – torch constraint &quot;&quot;&quot; . Example 3: Let create theta parameter . theta = pyro.param(&quot;theta&quot;, torch.tensor(1.0), constraint=dist.constraints.positive) . Simple PPL model . Consider the following Poison Regression model begin{align} y(t) &amp; sim lambda exp(- lambda) lambda &amp; sim exp(c + m(t)) c &amp; sim mathcal{N}(1, 1) m &amp; sim mathcal{N}(0, 1) end{align} . def model(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) for t in range(len(y)): rate = torch.exp(intercept + slope * t) y[t] = pyro.sample(&quot;count_{}&quot;.format(t), dist.Poisson(rate), obs=y[t]) return slope, intercept, y . Given a pyro model. We can . Generate data from model | Learn parameters of the model from data | Use the model to predict future observation. | Generate data from model . Running a Pyro model will generate a sample from the prior. . pyro.set_rng_seed(0) # We pass counts = [None, ..., None] to indicate time duration. true_slope, true_intercept, true_counts = model([None] * 50) fig, ax = plt.subplots(figsize=(6,4)) ax = sns.lineplot(x=np.arange(len(true_counts)),y=[c.item() for c in true_counts]) . Learn parameters of the model from data . To learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data. Inference algorithms in Pyro us arbitrary stochastic functions as approximate posterior distributions. that s. These functions are called guide functions or guides and contains pyro.sample and pyro.param statement. It is a stochastic function that represents a probability distribution over the latent (unobserved) variables. The guide can be arbitrary python code just like the model, but with a few requirements: . All unobserved sample statements that appear in the model appear in the guide. | The guide has the same input signature as the model (i.e. takes the same arguments). | There are no pyro.sample statements with the obs keyword in the guide. These are exclusive to the model. | There are pyro.param statements, which are exclusive to the guide. These provide differentiation for the inputs to the pay_probs sample in the guide vs. the model. | For example if the model contains a random variable z_1 . def model(): pyro.sample(&quot;z_1&quot;, ...) . then the guide needs to have a matching sample statement . def guide(): pyro.sample(&quot;z_1&quot;, ...) . Once a guide has been specified, we can then perform learning and inference which is an optimization problem of maximizing the evidence lower bound (ELBO). The ELBO, is a function of both $ theta$ and $ phi$, defined as an expectation w.r.t. to samples from the guide: . $${ rm ELBO} equiv mathbb{E}_{q_{ phi}({ bf z})} left [ log p_{ theta}({ bf x}, { bf z}) - log q_{ phi}({ bf z}) right]$$The SVI class is unified interface for stochastic variational inference in Pyro. To use this class you need to provide: . the model, | the guide, and an | optimizer which is a wrapper a for a PyTorch optimizer as discusseced in below | . from pyro.infer import SVI, Trace_ELBO svi = SVI(model, guide, optimizer, loss=Trace_ELBO()) . The SVI object provides two methods, step() and evaluate_loss(), . The method step() takes a single gradient step and returns an estimate of the loss (i.e. minus the ELBO). | The method evaluate_loss() returns an estimate of the loss without taking a gradient step. | . Both of these methods accept an optional argument: num_particles, which denotes the number of samples used to compute the loss and gradient. . The module pyro.optim provides support for optimization in Pyro. In particular it provides PyroOptim, which is used to wrap PyTorch optimizers and manage optimizers for dynamically generated parameters. PyroOptim takes two arguments: . a constructor for PyTorch optimizers optim_constructor and | a specification of the optimizer arguments optim_args | . from pyro.optim import Adam adam_params = {&quot;lr&quot;: 0.005, &quot;betas&quot;: (0.95, 0.999)} optimizer = Adam(adam_params) . Thus to learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data (here true_counts). . For the above example we will use Autoguide pyro inference algorithm: . AutoLaplaceApproximation:Laplace approximation (quadratic approximation) approximates the posterior log𝑝(𝑧|𝑥) by a multivariate normal distribution in the unconstrained space. | Autodelta: This implementation of AutoGuide uses Delta distributions to construct a MAP guide over the entire latent space. | . from pyro.infer.autoguide import AutoDelta from pyro.infer import SVI, Trace_ELBO from pyro.optim import Adam guide = AutoDelta(model) svi = SVI(model, guide, Adam({&quot;lr&quot;: 0.1}), Trace_ELBO()) for i in range(101): loss = svi.step(true_counts) # true_counts is passed as argument to model() if i % 10 == 0: print(&quot;loss = {}&quot;.format(loss)) . loss = 87295.88946688175 loss = 64525.8595520854 loss = 80838.8460238576 loss = 33014.93229973316 loss = 13704.865498423576 loss = 6232.828522503376 loss = 2017.9879159331322 loss = 631.3558134436607 loss = 170.10323333740234 loss = 198.57187271118164 loss = 207.4590385556221 . print(&quot;true_slope = {}&quot;.format(true_slope)) print(&quot;true_intercept = {}&quot;.format(true_intercept)) guess = guide() print(&quot;guess = {}&quot;.format(guess)) . true_slope = 0.15409961342811584 true_intercept = -0.293428897857666 guess = {&#39;slope&#39;: tensor(0.147, grad_fn=&lt;ExpandBackward&gt;), &#39;intercept&#39;: tensor(-0.054, grad_fn=&lt;ExpandBackward&gt;)} . Use model to predict future observation . A third way to use a Pyro model is to predict new observed data by guiding the model. This uses two of Pyro&#39;s effects: . trace records guesses made by the guide, and | replay conditions the model on those guesses, allowing the model to generate conditional samples. | . Traces are directed graphs whose nodes represent primitive calls or input/output, and whose edges represent conditional dependence relationships between those primitive calls. It return a handler that records the inputs and outputs of primitive calls and their dependencies. . We can record its execution using trace and use the resulting data structure to compute the log-joint probability of all of the sample sites in the execution or extract all parameters. . trace = pyro.poutine.trace(model).get_trace([]) pprint({ name: { &#39;value&#39;: props[&#39;value&#39;], &#39;prob&#39;: props[&#39;fn&#39;].log_prob(props[&#39;value&#39;]).exp() } for (name, props) in trace.nodes.items() if props[&#39;type&#39;] == &#39;sample&#39; }) . {&#39;intercept&#39;: {&#39;prob&#39;: tensor(0.250), &#39;value&#39;: tensor(-0.966)}, &#39;slope&#39;: {&#39;prob&#39;: tensor(2.818), &#39;value&#39;: tensor(0.083)}} . print(trace.log_prob_sum().exp()) . tensor(0.705) . Here, the trace feature will collect values every time they are sampled with sample and store them with the corresponding string name (that’s why we give each sample a name). With a little cleanup, we can print out the value and probability of each random variable’s value, along with the joint probability of the entire trace. . Replay return a callable that runs the original, reusing the values at sites in trace at those sites in the new trace. makes sample statements behave as if they had sampled the values at the corresponding sites in the trace . from pyro import poutine def forecast(forecast_steps=10): counts = true_counts + [None] * forecast_steps # observed data + blanks to fill in guide_trace = poutine.trace(guide).get_trace(counts) _, _, counts = poutine.replay(model, guide_trace)(counts) return counts . We can now call forecast() multiple times to generate samples. . for _ in range(1): full_counts = forecast(10) forecast_counts = full_counts[len(true_counts):] plt.plot([c.item() for c in full_counts], &quot;r&quot;, label=None if _ else &quot;forecast&quot;, alpha=0.3) plt.plot([c.item() for c in true_counts], &quot;k-&quot;, label=&quot;truth&quot;) plt.legend(); . References . Pyro-ducomentation | PPL models for timeseries forecasting |",
            "url": "https://sambaiga.github.io/sambaiga/jupyter/2020/03/01/ppl-pyro-intro.html",
            "relUrl": "/jupyter/2020/03/01/ppl-pyro-intro.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Stochastic Variational Inference (SVI)",
            "content": "Introduction . The previous post introduced the basic principle of Variational Inference (VI) as one of the approach used to approximate difficult probability distribution, derived the ELBO function and discussed about Mean Field Variational Inference (MFVI) and the Coordinate Ascent Variational Inference (CAVI) algorithms. This post introduce another stochastic gradient based algorithm (SVI) used in practise to do VI under mean filed assumptions. It also present two important tricks re-parametrization trick and amortized inference that are useful when using SVI in solving problems. . Stochastic Variational Inference (SVI) . Consider the graphical model of the observations $ mathbf{x}$ and latent variable $ mathbf{z}={ theta, z}$ in figure 1 where $ theta$ is the global variable and $z = {z_1, ldots z_n}$ is the local (per-data-point) variable such that: . . p(x,z)=p(θ∣α)∏i=1Np(xi∣zi,θ)⋅p(zi∣α)p( mathbf{x}, mathbf{z}) = p( theta| alpha) prod_{i=1}^N p(x_i|z_i, theta) cdot p(z_i| alpha)p(x,z)=p(θ∣α)i=1∏N​p(xi​∣zi​,θ)⋅p(zi​∣α) . Similarly the variational parameters are given by λ={γ,ϕ} lambda = { gamma, phi }λ={γ,ϕ} where the variational parameter γ gammaγ correspond to latent variable and ϕ phiϕ denote set of local variational parameters. The variational distribution q(z∣ϕ)q( mathbf{z} mid phi)q(z∣ϕ) is given by . q(z∣ϕ)=q(θ∣γ)∏i=1Nq(zi∣ϕi,α)q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i| phi_i, alpha)q(z∣ϕ)=q(θ∣γ)i=1∏N​q(zi​∣ϕi​,α) . which also depend on hyper-parameter α alphaα. The ELBO of this graphical model LVI(q)=Eq[log⁡p(x,z,α)−log⁡q(z,γ)] mathcal{L}_{VI}(q) = mathbb{E}_q[ log p( mathbf{x}, mathbf{z}, alpha) - log q( mathbf{z}, gamma)]LVI​(q)=Eq​[logp(x,z,α)−logq(z,γ)] has the following form: . begin{split} mathcal{L}{VI}(q) &amp;= mathbb{E}_q[ log p( theta| alpha)- log q( theta| gamma)] &amp;+ sum{i=1}^{N} mathbb{E}_q[ log p(z_i| theta) . log p(x_i|z_i, theta)- log q(z_i| phi_i)] end{split}&lt;/span&gt; | . The equation above could be optimized by CAVI algorithm discussed in previous post which is expensive for large data sets. The CAVI algorithm scales with NNN as it require to optimize the local variational parameters for each data point before re-estimating the global variational parameters. . Unlike CAI, SVI uses stochastic optimization to fit the global variational parameters by repeatedly sub-sample the data to form stochastic estimate of ELBO. In every iteration one randomly selects mini-batches of size bszb_{sz}bsz​ to obtain a stochastic estimate of ELBO. . begin{split} mathcal{L}{VI}(q) &amp;= mathbb{E}_q[ log p( theta| alpha)- log q( theta| gamma)] &amp;+ frac{N}{b{sz}} sum_{s=1}^{b_{sz}} mathbb{E}q[ log p(z{i_s}| theta) + log p(x_{i_s}|z_{i_s}, theta)- log q(z_{i_s}| phi_{i_s})] end{split} . SVI algorithms follow noisy estimates of the gradient with a decreasing step size which is often cheaper to compute than the true gradient. Following such noisy estimates allows SVI to escape shallow local optima of complex objective functions. . Natural Gradient for SVI . To solve the optimization problem standard gradient-based methods such as SGD, Adam or Adagrad can be used. However, for SVI these gradient based methods cause slow convergence or converge to inferior local models. This is because, gradient based methods use the following update . θt+1=θt+α∂LVI(q)∂θ theta^{t+1}= theta^t + alpha frac{ partial mathcal{L}_{VI}(q)}{ partial theta}θt+1=θt+α∂θ∂LVI​(q)​ . where . ∂LVI(q)∂θ=∂LVI(q)∂θ1,…∂LVI(q)∂θk frac{ partial mathcal{L}_{VI}(q)}{ partial theta} = frac{ partial mathcal{L}_{VI}(q)}{ partial theta_1}, ldots frac{ partial mathcal{L}_{VI}(q)}{ partial theta_k}∂θ∂LVI​(q)​=∂θ1​∂LVI​(q)​,…∂θk​∂LVI​(q)​ . is the the gradient vector which point in the direction where the function increases most quickly while the changes in the function are measured with respect to euclidean distance. As the result, if the euclidean distance between the variational parameter being optimized is not good measure of variation in objective function then gradient descent will move suboptimal through the parameter value. . Consider the following two set of gausian distributions {d(1)=N(−2,3),d(2)=N(2,3)} {d_{(1)}= mathcal{N}(-2, 3), d_{(2)}= mathcal{N}(2, 3) }{d(1)​=N(−2,3),d(2)​=N(2,3)} and {d(1)=N(−2,1),d(2)=N(2,1)} {d_{(1)}= mathcal{N}(-2, 1), d_{(2)}= mathcal{N}(2, 1) }{d(1)​=N(−2,1),d(2)​=N(2,1)}. . . The euclidean distance between the two distributions d=(μ1−μ2)2+(σ12−σ22)2=4d_{}= sqrt{( mu_1- mu_2)^2+ ( sigma^2_1- sigma^2_2)^2}=4d​=(μ1​−μ2​)2+(σ12​−σ22​)2​=4. It clear that, considering only the euclidean distance the two images are the same. However, when we consider the shape of the distribution, the distance is different in the first and second image. In the first image, the KL-divergence should be lower as there is more overlap between between the two distribution unlike the second image where their support barely overlap. The reason for this difference is that probability distribution do not naturally fit in euclidean space rather it fit on a statistical manifold also called Riemannian manifold. . Statistical manifold give a natural way of measuring distances between distribution that euclidean distance use in SGD. A common Riemannian metric for statistical manifold is the fisher information matrix defined by . Fλ=Ep(x;λ)[∇log⁡p(x;λ)(∇log⁡p(x;θ))T]F_{ lambda} = mathbb{E}_{p(x; lambda)}[ nabla log p(x; lambda) ( nabla log p(x; theta))^T ]Fλ​=Ep(x;λ)​[∇logp(x;λ)(∇logp(x;θ))T] . It can be showed that the fisher information matrix FλF_{ lambda}Fλ​ is the second derivative of the KL divergence between two distributions. . Fθ=∇θ2KL(q(x;λ)∣∣p(x;θ))F_{ theta} = nabla^2_{ theta} KL(q(x; lambda)||p(x; theta))Fθ​=∇θ2​KL(q(x;λ)∣∣p(x;θ)) . Thus for SVI, the standard gradients descent techniques can be replaced with the natural gradient as follows: . ∇q~L(q)=F−1∇qLVI(q) tilde{ nabla_{q}} mathcal{L}(q) = F^{-1} nabla{q} mathcal{L}_{VI}(q)∇q​~​L(q)=F−1∇qLVI​(q) . The update procedure for natural gradient can be summarized as follows: . Compute the loss LVI(q) mathcal{L}_{VI}(q)LVI​(q) | Compute the gradient of the loss ∇qLVI(q) nabla{q} mathcal{L}_{VI}(q)∇qLVI​(q) | Compute the Fisher Information Matrix F. | Compute the natural gradient ∇q~LVI(q) tilde{ nabla_{q}} mathcal{L}_{VI}(q)∇q​~​LVI​(q) | Update the parameter qt+1=qt−α∇θ~LVI(q)q^{t+1} =q^t - alpha tilde{ nabla_{ theta}} mathcal{L}_{VI}(q)qt+1=qt−α∇θ​~​LVI​(q) | Using natural gradient instead of standard gradients simplify SVI gradient update. However the same conditions for convergence as standard SDG have to be fulfilled. First, the mini-batch indices must be drawn uniformly at random size where the size bszb_{sz}bsz​ of the mini-batch must satisfies 1≤bsz≤N1 leq b_{sz} leq N1≤bsz​≤N The learning rate α alphaα needs to decrease with iterations ttt satisying the Robbins Monro conditions ∑t=1∞αt=∞ sum_{t=1}^{ infty} alpha_t = infty∑t=1∞​αt​=∞ and ∑t=1∞αt2&lt;∞ sum_{t=1}^{ infty} alpha_t^2 &lt; infty∑t=1∞​αt2​&lt;∞ This guarantee that every point in the parameter space can be reached while the gradient noise decreases quickly enough to ensure convergence. . The next section presents two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems. . Re-parametrization trick . Consider the graphical model presented in figure 1, where gradient based stochastic optimization is used to learn the variational parameter ϕ phiϕ. For example; for Gaussian distribution . qϕ(z∣x)=N(μϕ(x),Σϕ(x))q_{ phi}(z|x)= mathcal{N}( mu_{ phi}(x), Sigma_{ phi}(x))qϕ​(z∣x)=N(μϕ​(x),Σϕ​(x)) . to maximize the likelihood of the data, we need to back propagate the loss to the parameter ϕ phiϕ across the distribution of zzz or across sample z∼qϕ(z∣x)z sim q_ phi(z mid x)z∼qϕ​(z∣x) However, it is difficulty to back-propagate through random variable. To address this problem, the re-parametrization trick is used.First let consider the Law of the Unconscious Statistician (LOTUS), used to calculate the expected value of a function g(ϵ)g( epsilon)g(ϵ) of a random variable ϵ epsilonϵ when only the probability distribution p(ϵ)p( epsilon)p(ϵ) of ϵ epsilonϵ is known. It state that: . To compute the expectation of a measurable function g(.)g(.)g(.) of a random variable ϵ epsilonϵ, we have to integrate g(ϵ)g( epsilon)g(ϵ) with respect to the distribution function of ϵ epsilonϵ, that is: . E(g(ϵ))=∫g(ϵ)dFϵ(ϵ) mathbb{E}(g( epsilon)) = int g( epsilon)dF_{ epsilon}( epsilon)E(g(ϵ))=∫g(ϵ)dFϵ​(ϵ) . In other words, to compute the expectation of z=g(ϵ)z =g( epsilon)z=g(ϵ) we only need to know g(.)g(.)g(.) and the distribution of ϵ epsilonϵ. We do not need to explicitly know the distribution of zzz. Thus the above equation can be expression in the convenient alternative notation: . Eϵ∼p(ϵ)(g(ϵ))=Ez∼p(z)(z) mathbb{E}_{ epsilon sim p( epsilon)}(g( epsilon)) = mathbb{E}_{z sim p(z)} (z)Eϵ∼p(ϵ)​(g(ϵ))=Ez∼p(z)​(z) . Therefore the reparameteriztaion trick states that: . A random variable zzz with distribution qϕ(z,ϕ)q_{ phi}(z, phi)qϕ​(z,ϕ) which is independent to ϕ phiϕ can be expressed as transformation of random variable ϵ∼p(ϵ) epsilon sim p( epsilon)ϵ∼p(ϵ) that come from noise distribution such as uniform or gaussian such that z=g(ϕ,ϵ)z = g( phi, epsilon)z=g(ϕ,ϵ) . For instance for Gaussian variable zzz in the above example z=μ(ϕ)+σ2(ϕ)⋅ϵz = mu( phi) + sigma^2( phi) cdot epsilonz=μ(ϕ)+σ2(ϕ)⋅ϵ where ϵ∼N(0,1) epsilon sim mathcal{N}(0, 1)ϵ∼N(0,1) . Since p(ϵ)p( epsilon)p(ϵ) is independent of the parameter of qϕ(z,ϕ)q_{ phi}(z, phi)qϕ​(z,ϕ), we can apply the change of variables in integral theory to compute any expectation over zzz or any expectation over ϕ phiϕ. The SDG estimator can therefore be estimated by pulling the gradient into expectations and approximating it by samples from the noise distribution such that for any measurable function fθ(.)f_{ theta}(.)fθ​(.): . ΔϕEz∼pϕ(z)=1M∑i=1MΔf(g(ϕ,ϵi)) Delta_{ phi} mathbb{E}_{z sim p_{ phi}(z)} = frac{1}{M} sum_{i=1}^M Delta f(g( phi, epsilon_i))Δϕ​Ez∼pϕ​(z)​=M1​i=1∑M​Δf(g(ϕ,ϵi​)) . where ϵi∼p(ϵ) epsilon_i sim p( epsilon)ϵi​∼p(ϵ) , fθ(.)f_{ theta}(.)fθ​(.) must be differentiable w.r.t its input zzz and g(ϕ,ϵi)g( phi, epsilon_i)g(ϕ,ϵi​) must exist and be differentiable with respect to ϕ phiϕ. . Amortized Variational Inference . Consider the graphical model presented in figure 1 where ecah data point xix_ixi​ is governed by its latent variable ziz_izi​ with variational parameter phiiphi_iphii​ such that . q(z∣ϕ)=q(θ∣γ)∏i=1Nq(zi∣ϕi,α)q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i| phi_i, alpha)q(z∣ϕ)=q(θ∣γ)i=1∏N​q(zi​∣ϕi​,α) . Using traditional SVI make it necessary to optimize ϕi phi_iϕi​ for each data point xix_ixi​. As the results the number parameters to be optimized will grows with the number of observations xxx. This is not ideal for larger datasets. Apart from that, it requires one to re-run the optimization procedure in case of new observation or when we have to perform inference. To address these problem amortized VI introduce a parametrized function that maps from observation space to the parameter of the approximate posterior distribution. . Amortized VI try to learn from past inference/pre-computation so that future inferences run faster. Instead of approximating separate variables for each data point xix_ixi​, amortized VI assume that the local variational parameter ϕ phiϕ can be predicted by a parametrized function fϕ(.)f_{ phi}(.)fϕ​(.) of data whose parameters are shared across all data points. Thus instead of introducing local variational parameter, we learn a single parametric function and work with a variational distribution that has the form . q(z∣ϕ)=q(θ∣γ)∏i=1Nq(zi∣fϕ(.))q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i|f_{ phi}(.))q(z∣ϕ)=q(θ∣γ)i=1∏N​q(zi​∣fϕ​(.)) . where fϕ(.)f_{ phi}(.)fϕ​(.) is the deep neural net function of zzz . Deep neural network used in this context are called inference networks. Therefore amortized inference with inference networks combines probabilistic modelling with representation power of deep learning. Using amortized VI instead of traditional VI, has two important advantages. First the number of variational parameters remain constant with respect to the data size. We only need to specify the parameter of the neural networks which is independent to the number of observations. Second, for new observation or during inference all we need to do is to call the inference network. As the result, we can invest time upfront optimizing the inference network and during inference we use the trained network for fast inference. . Reference . [Cheng Zhang,(2017)]: Advances in Variational Inference. | [Daniel Ritchie,(2016)]:Deep Amortized Inference for Probabilistic Programs. | [Andrew Miller,(2016)]:Natural Gradients and Stochastic Variational Inference. | Shakir Mohamed:Variational Inference for Machine Learning. | DS3 workshop:Approximate Bayesian Inference: Old and New. | Variational Inference and Deep Generative Models:Variational Inference for NLP audiences |",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/probabilistic%20model/2018/06/08/stochastic-vi.html",
            "relUrl": "/machine%20learning/probabilistic%20model/2018/06/08/stochastic-vi.html",
            "date": " • Jun 8, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ". Anthony Faustine is a Data Scientist at CeADAR (UCD), Dublin, Ireland with over four years of successful experience in data analytics and Artificial Intelligence techniques for multiple applications. He effectively researches techniques for novel approaches to problems and develops prototypes to assess their viability. Although Anthony is a person who takes the initiative, he has a strong team-work spirit with experience of working in a highly international environment. . At CeADER, Anthony is devising and implementing data analytics/AI technical solutions for multiple application domains. He is also involved in the research and development of the applicability of Artificial Intelligence for Earth Observation (AI4EO). . Mr Faustine, received the B.sc. Degree in Electronics Science and Communication from the University of Dar es Salaam, Tanzania, and the M.sc. Degree in Telecommunications Engineering from the University of Dodoma, Tanzania, in 2010. From 2010 to 2017, he worked as an assistant lecturer at the University of Dodoma, Tanzania, where he was involved in several research projects within the context of ICT4D. . In 2017, Anthony joined IDLab, imec research group of the University of Ghent, in Belgium as a Ph.D. Machine learning researcher advised by Tom Dhaene and Dirk Deschrijver. His research focused on machine learning techniques applied to energy smart-meter data. He develops methods to identify active appliances and extract their corresponding power consumption from aggregate power (energy-disaggregation) in residential and industrial buildings. . His research interests lie in the intersections between Computational Sustainability and Artificial Intelligence. He works towards bridging the gap between laboratory and real-world applicability of machine learning for sustainable development. .",
          "url": "https://sambaiga.github.io/sambaiga/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}