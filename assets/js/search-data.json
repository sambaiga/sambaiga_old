{
  
    
        "post0": {
            "title": "Super-charge Deep learning hyper-paramater search with Optuna",
            "content": "Introduction . Training machine learning sometimes involves various hyperparameter settings. Performing a hyperparameter search is an integral element in building machine learning models. It consists of attuning different sets of parameters to find the best settings for best model performance. It should be remarked that deep neural networks can involve many hyperparameter settings. Getting the best set parameters for such a high dimensional space might a challenging task. Opportunely, different strategies and tools can be used to simplify the process. This post will guide you on how to use Optuna for a hyper-parameter search using PyTorch and PyTorch lightning framework. The notebook with all the code for this post can be found on this colab . Optuna . Optuna is an open-source hyperparameter optimization framework. It automates the process of searching for optimal hyperparameter using Python conditionals, loops, and syntax. The optuna library offers efficiently hyper-parameter search in large spaces while pruning unpromising trials for faster results. It is also possible to run a hyperparameter search over multiple processes without modifying code. For a brief introduction of optuna, you can watch this video . The optuna optimization problem consists of three main building blocks; objective function, trial, and study. Let consider a simple optimisation problem: Suppose a rectangular garden is to be constructed using a rock wall as one side of the garden and wire fencing for the other three sides as shown in figure below (taken from this link). Given 500m of wire fencing, determine the dimensions that would create a garden of maximum area. What is the maximum area? . Let $x$ denote the side of the garden‚Äôs side perpendicular to the rock wall, and $y$ indicates the side parallel to the rock wall. Then the area of the garden $A= x cdot y$. We want to find the maximum possible area subject to the constraint that the total fencing is 500m. The total amount of fencing used will be $2x+y$. Therefore, the constraint equation is 500=2x+yy=500‚àí2xA(x)=x‚ãÖ(500‚àí2x)=500x‚àí2x2 begin{aligned} 500 &amp; = 2x +y y &amp; = 500-2x A(x) &amp;= x cdot (500-2x) = 500x - 2x^2 end{aligned}500yA(x)‚Äã=2x+y=500‚àí2x=x‚ãÖ(500‚àí2x)=500x‚àí2x2‚Äã . From equation above, $A(x) = 500x - 2x^2$ is an objective function, the function to be optimized. To maximize this function, we need to determine optimization constraints. We know that to construct a rectangular garden, we certainly need the lengths of both sides to be positive $y&gt;0$, and $x&gt;0$. Since $500 = 2x +y$ and $y&gt;0$ then $x&lt;250$. Therefore, we will try to determine the maximum value of A(x) for x over the open interval (0,50). . Optuna trial corresponds to a single execution of the objective function and is internally instantiated upon each invocation of the function. To obtain the parameters for each trial within a provided constraints the [suggest] method (https://optuna.readthedocs.io/en/stable/reference/trial.html) is used. . trial.suggest_uniform(&#39;x&#39;, 0, 250) . We can now code the objective function that be optimized for our problem. . def gardent_area(trial): x = trial.suggest_uniform(&#39;x&#39;, 0, 250) return (500*x - 2*x**2 ) . Once the objective function has been defined, the study object is used to start the optimization. The study is an optimization session, a set of trials. Optuna provide different sampler strategies such as Random Sampler and Tree-structured Parzen Estimator (TPE) sampler. A sampler has the responsibility to determine the parameter values to be evaluated in a trial. By default, Optuna uses TPE sampler, which is a form of Bayesian Optimization. The TPE provides a more efficient search than a random sampler search by choosing points closer to past good results. It possible to add a custom sampler as described in this link We can now create a study and start the optimization process. . 400: Invalid request . Once the study is completed, you can get the best parameters using study.best_params and study.best_value will give you the best value. . Hyper-param search for deep neural net . Suppose we want to build MLP classifier to recognize handwritten digits using the MNIST dataset. We will first build a pytorch MLP model with the following default parameters . hparams = {&quot;in_size&quot;: 28*28, &quot;hidden_size&quot;:128, &quot;out_size&quot;:10, &quot;layer_size&quot;:5, &quot;dropout&quot;:0.2} . 400: Invalid request . For the above MLP model, we need to specify the following parameters hidden size, dropout, and number of linear layers. The critical question is, how do we pick these parameters. We will use optuna to search for optimal parameters that will give us an excellent performance. First, we will create a PyTorch lightning model that will provide the structure for organizing the fundamentals component of any machine learning project. These elements include the data, architecture or model, optimizer, loss function, training, and evaluation step. Since we fine defined our MLP, we go ahead and create a PyTorch lightning module. The complete code with all the component mentioned above can be found on gist link To learn the parameters of the MLP we will use Stochastic Gradient Descent Optimizer (SGD) optimizer. The SGD has several other hper-parameters such as learning rate just we can also optimize. . optimizer = torch.optim.SGD(self.model.parameters(), lr=self.hparams[&#39;learning_rate&#39;], momentum=self.hparams[&#39;momentum&#39;], nesterov=self.hparams[&#39;nesterov&#39;], weight_decay=self.hparams[&#39;weight_decay&#39;]) . Thus the SGD optimizer will add four additional parameters. We can also treat the batch size as hyper-parameter to optimize. We will have the following set of parameters to optimizers. . default_params = {&quot;in_size&quot;: 28*28, &quot;hidden_size&quot;:128, &quot;out_size&quot;:10, &quot;layer_size&quot;:5, &quot;dropout&quot;:0.2, &quot;batch_size&quot;:32, &#39;learning_rate&#39;:1e-3, &#39;momentum&#39;:0.9, &#39;nesterov&#39;: True, &#39;weight_decay&#39;:1e-5, &#39;epochs&#39;:2} . Defining the hyperparameters and objective function to be optimized . Since we know all the parameters that we want to optimize, we will use the optuna suggest to define a search space for each hyperparameter that we want to tune. Optuna supports a variety of suggests which can be used to optimize floats, integers, or discrete categorical values. Numerical values such as learning rate can be suggested using a logarithmic scale. . 400: Invalid request . To create an objective function, we use the trainer module within PyTorch lightning with the default TensorBoard logger. The trainer will return the validation score. Optuna will use this score to evaluate the performance of the hyperparameters and decide where to sample in upcoming trials. In addition to sampling strategies, Optuna provides a mechanism to automatically stops unpromising trials at the early stages of the training. This allows computing time to be used for tests that show more potential. This feature is called pruning, and it is a form of automated early-stopping. The PyTorchLightingPruningCallBack provides integration Optuna pruning function to PyTorch lightning . 400: Invalid request . To start the optimization, we create a study object and pass the objective function to method optimize() as follows. . def run_study(num_trials=2): study = optuna.create_study(direction=&#39;maximize&#39;) study.optimize(objective, n_trials=num_trials) return study . After the study is completed, we can export trials as a pandas data frame. This provides various features to analyze studies. It is also useful to draw a histogram of objective values and to export trials as a CSV file. . df = study.trials_dataframe() . The notebook with all the code for this post can be found on this colab link . References . Using Optuna to Optimize PyTorch Lightning Hyperparameters | Optuna documentation | Pytorch-lightning example | .",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/deep%20learning/2020/06/22/hyper-search.html",
            "relUrl": "/machine%20learning/deep%20learning/2020/06/22/hyper-search.html",
            "date": " ‚Ä¢ Jun 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "AI4EO-An Opportunity for Computation Sustainability",
            "content": "Introduction . Computational Sustainability focuses on developing computational models, methods, and tools to help policymakers design more effective solutions and policies for sustainable development. The advancement of Information and Communication Technologies (ICT), particularly Earth Observation (EO) and Artificial intelligence (AI) offer prospects of addressing sustainability challenges. A more in-depth explanation about the above project can be viewed in this video: . Earth observations (EO) are data and information about the planet‚Äôs physical, chemical, and biological systems. It involves the collection, analysis, and presentation about the status of, and changes in, the natural and human-made environment. The most common sources of EO data include drones, land stations, and satellites. While drones capture high-resolution images on a small scale, satellites generate growing amounts of multi-resolution and multi-bands imagery and other data sources for the whole Earth. These data could be used to create all kinds of different products so that businesses, scientists, policymakers, and even everyday citizens can understand the past, present, and future trends in the Earth systems. Figure below shows multiband imagery from satellites by the electromagnetic. . On the other hand, AI is an area of computer science devoted to developing systems that can learn (from data) to make decisions and predictions within specific contexts. Indeed, AI technology can extract more in-depth insights from datasets than other techniques. Lately, AI has been used with success in solving complex problems in several domains such as machine translation, computer vision, autonomous cars, to mention a few. Machine learning and particularly computer vision models provide explicitly useful and practical approaches for analyzing and extracting relevant information from EO imagery data. Deep learning models and especially Convolution Neural Networks (CNNs) have proven effective in several computer vision tasks such as object detection, classification, and video processing, image generations, and image captioning, to mention a few. These models could be applied to detect and classify objects from complex EO imagery at a larger scale. Figure 2 presents AI capability for object detection and using computer vision techniques for multiband satellite images. This image has been taken from here). . Applying these techniques to EO data will make it easy to efficiently automate the recognition of known and unknown patterns at large-scale. This is likely to reveal useful insights and opportunities for addressing sustainability challenges. For example, AI models could be applied to perform automated change detection, crop mapping, and yield estimation from high-resolution imagery in a larger-scale. The fusion of EO data and other data sources such as geo-referenced, demographics, and social-network data can be used to facilitate the more targeted developmental challenge. For instance, it has been demonstrated that the AI model can be used to predict the poverty level by analyzing satellite imagery, night lights, and demographic data. . EO data sources . There are many EO data sources made available recently. These data sources offer virtual visualization of any location on earth with resolution ranging from 5 centimeters to 120 meters depending on the instruments of satellites, airbus, or drones. The data sources are published as public or commercial data sources. . Public EO data providers . The EO puplic data providers are public service framework that allows full, free and open access to all data collected. Copernicus and Landsat are the famous and largest public satellite data providers. Landsat s one of the world‚Äôs largest satellite image providers. It is a joint program of the National Aeronautics and Space Administration (NASA) and the United States Geological Survey (USGS). It provides access to satellites of the Landsat family, which have access over the archival of 50 years of earth data. Landsat satellites collect data on the forests, farms, urban areas, and water sources, generating the longest continuous record. The freely available information is used to understand environmental change better, manage agricultural practices, allocate scarce water resources, monitor the extent and health of forests and respond to natural disasters, and more. Data can be accessed using LandsLook Viewer, USGS GloVis, Earth Explorer, Free Web-Enabled Landsat Data (WELD). More information is available here. . Copernicus is managed by the Europe Unions EO program and collect data from a constellation of 6 families of satellites, known as Sentinels. Each Sentinel mission focuses on different but interrelated aspects of EO, including Atmospheric monitoring (Sentinels 4 and 5), Marine environment monitoring (Sentinel-3), Land monitoring (Sentinel-2), Climate Change and Emergency management. Currently Copernicus produces 12 terabytes per day of data for the 6 families of satellites, known as ‚ÄúSentinels.‚Äù The data are open access and can be freely downloaded using [Copernicus Open Access Hub]. A summary of Copernicus program can found in this video . Commercial data providers . The commercial satellite imagery providers provide access to data with high resolution with 3 centimeters to 10 meters. These services are paid and have good archival imagery. The most popular commercial EO imagery providers include; Planet Labs, DigitalGlobe and Airbus. . Planet Labs provides access to a wide range of satellite data. It provides access to SkySAT families and RapidEye satellites. With 120+ satellites in orbit, Planet can image anywhere on Earth‚Äôs landmass daily, at 3 - 5-meter resolution. Planet processes and delivers imagery quickly and efficiently. Planet‚Äôs platform downloads and processes 11+ TB of data daily, enabling customers to build and run analytics at scale. Users can access Planet‚Äôs data, using the paid planet API. Nevertheless, university researchers, academics, and scientists apply for free access as decribed in this link. . The DigitalGlobe is similar to Planet Labs and provides data access to a full range constellation of satellites in orbit. It provides access to EarlyBird-1, IKONOS, QuickBird, GeoEye-1, a family of WorldView satellites. It offers a high resolution of up to 30cm, showing crisp details, satellite imagery, geospatial information, and location-based intelligence. Recently, DigitalGlobe has started providing 0.4m resolution imagery today, which is one of the best in the business. . On the other hand, the Airbus, with Pleiades and SPOT missions, provide very high-resolution multispectral twin satellites with 0.5 meters and 1.5-meter resolution, respectively. These imagery data are particularly suitable for emergency response and up-to daily change detection. . AI ready EO datasets . Building ML applications for EO requires access to both EO data and their ground truth. Creating such a data-set is time-consuming and costly. As a result different organisations provide ready-to-use EO dataset which allow ML and GIS researchers and other stakeholders to build and test their ML application specific to EO. Radiant MLHub and Spacenet are the two notable EO training data providers. Radiant MLHub is an open library for geospatial training data to advance machine learning applications on EO. It hosts open training datasets generated by Radiant Earth Foundation‚Äôs team as well as other training data catalogs contributed by Radiant Earth‚Äôs partners. The data provided by Radiant MLHub are stored using a SpatioTemporal Asset Catalog (STAC) compliant catalog and exposed through a standard API. These data are open to anyone to use. It also free stores, register and share your dataset. . The Spacenet, on the other hand, provides access to high-quality geospatial data for developers, researchers, and startups with a specific focus on the four open-source key pillars: data, challenges, algorithms, and tools. It also hosts challenges that focus on applying advanced machine learning techniques to solve difficult mapping challenges. The SpaceNet Dataset is hosted as an Amazon Web Services (AWS) Public Dataset, which is open for geospatial machine learning research. The dataset consists of well-annotated and very high-resolution satellite imagery with foundational mapping features such as building footprints or road networks. . Kaggle, a world‚Äôs largest data science community with powerful tools and resources, is another source of EO training datasets which host several machine learning challenges EO imagery. This challenges includes Dstl Satellite Imagery Feature Detection, Airbus Ship Detection Challenge and Draper Satellite Image Chronology to mention a few. . . API for accessing EO data. . Despite the availability of free and commercial satellite imagery, it is somehow challenging to directly download and use these data. Accessing these data requires one to have expertise in satellite imagery. Several API solutions that make it easy to access, download, and use satellite imagery from different sources have been developed to address these challenges. Sentinel Hub API is one of the easily available data API. . Sentinel Hub API makes satellite data from Sentinel, Landsat, and other Earth observation imagery easily accessible via easy-to-integrate web services. The API allows users to integrate satellite data either into their applications. To this end, Sentinel-hub offers several plugins such as sentinelhub-python, Sentinelhub-js, Sentinel Hub QGIS and EO Browser which is is a search tool for Sentinel-2 and Landsat 5,7,8 satellite imagery. Sentinelhub-js offer seamless integration of Sentinel Hub and other similar EO web services in web or node.js. This allows web developers to access remote sensing data quickly and to integrate it with their applications. On the other hand, the sentinel hub-python enables users to seamlessly make requests from Sentinel Hub OGC web services, download, and process images within their Python scripts. The Sentinel Hub QGIS plugin allows users to configure and harness Sentinel Hub services directly in QGIS. The Sentinel Hub API is the paid services but the also offer free access for research purpose. More details could found on this link. . Users can also use sentinelsat a python API for searching, downloading and retrieving the metadata of Sentinel satellite images from the Copernicus Open Access Hub. Compared to Sentinel-hub API, this is free services limited to only Copernicus satellites. . Opportunity and Challenges of AI4EO . Machine learning, precisely computer vision, can be applied to EO imagery data for multimodal semantic segmentation, detecting objects, detecting changes from a time series satellite image or image retrieval. The computer vision model can automatically generate semantic maps of a large area from EO data [Audebert2017a]. The resulting semantic maps can be used for the cartography of urban areas or to determine land use cover at a massive scale. In change detection, machine learning models could be used to extend the semantic analysis of EO data by incorporating the multi-temporal dimension. This enables us to track changes around the globe or monitor activity in high-revisit rate acquisitions. It also plays an essential role in the production of maps depicting the evolutions of land use, urban coverage, deforestation, and other multi-temporal type analysis. The image retrieval aims to retrieve images with similar visual contents with respect to the query image from a database. . . Even though AI provides a potential application to EO, several challenges need to be addressed to successfully exploits AI potentials. This is because compared to other types of Data, EO present several challenges for machine learning algorithms. The following video discuss some of the opportunities and challenges of machine learning for EO. . First, the EO data is multimodal and high dimensional. For instance, the EO satellite data come from a variety of sensors types such as passive sensors (RBGN), active sensors (Synthetic Aperture Radar (SAR)), near-infrared sensors. The data also contain additional geo-related data like weather, geo-physical or biochemical quantities and other derived products. This data variety raises the following fundamental challenges when applied to the machine learning model: *how to combine all these data types (data fusion) since all these sources provide complementary information that should be used jointly to maximize the model‚Äôs performance. As a result, there is a need to develop novel machine learning models that match EO data taken from different sources with different imaging modalities. Modifying existing vision-based deep networks to these data is not trivial, as this requires to work with new data structures that do not share the same underlying physical and numerical properties. Other exciting topics could be investigating the transferability of deep learning networks to [EO imaging modalities] (https://ieeexplore.ieee.org/document/8113128). Among the other challenges are the sheer number of pixels and the geographic extent per image. For example, a single DigitalGlobe satellite image encompasses &gt; 64 km2 and over 250 million pixels. Also, the objects of interest are microscopic, which complicates traditional computer vision techniques. . The size of EO data is increasing at an exponential rate demanding automation, massive computing, and machine learning algorithms that are fast enough and sufficiently transferrable to be applied for the whole earth‚Äôs surface. Besides, these data contain plenty of unlabelled data, making it challenging to use well-established supervised machine learning techniques. Yet this provides an opportunity to explore the recent progress in semi-supervised learning, self-supervised and active-learning methods for EO application. Furthermore, the existence of meta-data and other geo-referenced data such as the Open-street map (OSM) provides an opportunity for creating annotated satellite imagery data for machine learning algorithms. . It should be noted that EO data are time-variable dependent as satellite guarantees continuous data acquisition for decades. For example, the sentinel-1 images the entire earth every six days. Thus machine learning algorithms for EO imagery analysis should jointly exploit both the temporal, spectral, and spatial information of these data. . The interpretability of the machine learning model applicable to EO is another exciting research opportunity. Machine learning models are useful to estimate correlations, but what about causations. Exploiting graphical models and causal discovery to learns cause and effects relations from EO data is a unique opportunity for machine learning in EO. This can be useful for hypothesis testing, model-data comparison, and understanding the causes of extreme impacts. . Conclusion . The satellite has been in the orbit of the earth for many decades, but the access to the data and applications using satellite images has recently become prominent. In this blog, we have introduced the opportunity of using Earth Observation and Artificial Intelligence to address sustainability challenges. We introduced different sources for EO data sources that include public and private satellite providers. We also presented the prominent AI-ready EO data providers and introduced Sentinel-hub, which is an API for accessing EO data from different sources. Finally, the blog highlight opportunity and challenges of applying advanced machine learning techniques such as deep learning for EO imagery data. In the upcoming blog, we will discuss how to download, process, and use EO data for machine learning applications with a specif focus on computational sustainability. Stay tuned for more! . References . Working towards AI and Earth observation | .",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/deep%20learning/eearth%20observation/2020/06/20/eo-blog-one.html",
            "relUrl": "/machine%20learning/deep%20learning/eearth%20observation/2020/06/20/eo-blog-one.html",
            "date": " ‚Ä¢ Jun 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Energy Based Model Classifier",
            "content": "IDEA: . This post review energy-based model classfier presented in Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One. The paper propose to transform a standard discriminative classifier of $p(y| mathbf{x})$ as an Energy Based Model(EBM) for the joint distribution $p(x, y)$ . An EBM learn to predict if a certain pair of $(x, y)$ fit together or not. Given input variable $ mathbf{x}$ and target variable $ mathbf{y}$. The level of dependency between $x$ and $y$ is defined by the energy function $E_{ theta}(x, y)$ which maps each point to a scalar value. $E_{ theta}(x, y)$ takes low values when $y$ is compatible with $x$ and higher values when $y$ is less compatible with $x$. The energy function $E_{ theta}(x, y)$ can be turned into a normalized joint probability distribution $ p_{ theta}(x, y)$ through the Gibbs distribution: begin{equation} p_{ theta}(x, y) = frac{ exp(- E_{ theta}(x, y)}{Z( theta)} end{equation} . where $Z{ theta} = int_y exp(- E_{ theta}(x, y)$ is is the normalizing constant. . Motivation . Performance gap between the strongest generative modeling approach to downstream tasks (semi-supervised learning, imputation of missing data, and calibration of uncertainty). State-of-the-art generative models have diverged quite heavily from state-of-the-art discriminative architectures. This lead into hand-tailored solutions for each specific problem. . The paper aim at using EBMs to help realize the potential of gen-erative models on downstream discriminative problems. . Approach . The main idea of the paper is to enterpreate the logits of a classifier as the joint density of data points and labels and the density of data points alone. . Consider a machine learning, classifier with K $f_{ theta}( mathbf{x})$ which maps each data point $x in D$ to $K$ real-valued numbers known as logits. The logits parameterize a categorical distribution such as: begin{equation} p_{ theta}(y| mathbf{x})= frac{ exp(f_{ theta}( mathbf{x}))}{ sum exp(f_{ theta}( mathbf{x}))} end{equation} We can re-interpret the logits obtained from $f_{ theta}( mathbf{x})$ to define $p( mathbf{x}, y)$ and $p( mathbf{x})$ as well. Thus the EBM of the joint distribution of data point x and labels y can be defined as: begin{equation} p_{ theta}( mathbf{x}, y)= frac{ exp(f_{ theta}( mathbf{x}))}{Z( theta)} end{equation} where $Z{ theta}$ is unknown normalizing constant and $E{ theta} = -f_{ theta}( mathbf{x}) $ . Marginalizing out y, we obtain an unnormalized density model for x, begin{equation} p_{ theta}( mathbf{x})= sum_y p_{ theta}( mathbf{x}, y) = sum_y frac{ exp(f_{ theta}( mathbf{x}))}{Z( theta)} end{equation} The energy function of a data point x can thus be defined as begin{equation} E_{ theta}(x)= - mathrm{LogSum}_y f_{ theta}( mathbf{x}) = - log sum_y exp(f_{ theta}( mathbf{x})) end{equation} . The conditional distribution $p_{ theta}(y| mathbf{x})$ is can be obitained as begin{equation} p_{ theta}(y| mathbf{x})= frac{p_{ theta}( mathbf{x}, y)}{p_{ theta}( mathbf{x})} end{equation} | . Training EBM . For most choices of $E_{ theta}$, it is hard to compute liably estimate $Z_{ theta}$ which means estimating the normalized densities is intractable and standard maximum likelihood estimation of the parameters $ theta$ is not straightforward. Despite a long period of little development, there has been recent work using this method to train large-scale EBMs on high-dimensional data, parameterized by deep neural networks using Stochastic Gradient Langevin Dynamics (SGLD) . First let consider the derivative of the log-likelihood for a single example $x$ with respect to $ theta$ . begin{equation} frac{ partial log p_{ theta}( mathbf{x})} { partial theta} = mathbb{E}_{p_{ theta}(x^{ prime)})} frac{ partial E_{ theta}( mathbf{x}^{ prime)}} { partial theta} - frac{ partial E_{ theta}( mathbf{x})} { partial theta} end{equation}The SGLD draws samples as follows begin{equation} mathbf{x}_{i+1} = mathbf{x}_i - frac{ alpha}{2} frac{ partial E_{ theta}( mathbf{x})} { partial theta} + epsilon end{equation} where $ mathbf{x}_0 sim p_0( mathbf{x}) $ and $ epsilon sim mathcal{N}(0, alpha)$ . Factorize the likelihood as begin{equation} log p_{ theta}( mathbf{x}, y) = log p_{ theta}( mathbf{x}) + log p_{ theta}(y| mathbf{x}) end{equation} We can therefore optimize $p_{ theta}(y| mathbf{x})$ using standard cross-entropy and optimize $ log p_{ theta}( mathbf{x})$ using Equation 8 with SGLD where gradients are taken with respect to $ mathrm{LogSum}_y f_{ theta}( mathbf{x})$ . Application . Hybrid modeling | Calibration | Out of distribution detection | Robustness | .",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/deep%20learning/generative%20models/2020/06/18/ebm-classifier.html",
            "relUrl": "/machine%20learning/deep%20learning/generative%20models/2020/06/18/ebm-classifier.html",
            "date": " ‚Ä¢ Jun 18, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Vectorization and Distribution shapes in Pyro",
            "content": "Introduction . In the previous post we introduced pyro and its building blocks such as schotastic function, primitive sample and param primitive statement, model and guide. We also defined pyro model and use it to generate data, learn from data and predict future observations. . In this section, we will learn in details about inference in Pyro, how to use Pyro primitives and the effect handling library (pyro.poutine) to build custom tools for analysis. . Consider a previous poison regression model . import torch import pyro import pyro.distributions as dist from torch.distributions import constraints import matplotlib.pyplot as plt import seaborn as sns import numpy as np pyro.set_rng_seed(101) torch.manual_seed(101) %matplotlib inline . def model_(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) for t in range(len(y)): rate = torch.exp(intercept + slope * t) y[t] = pyro.sample(&quot;count_{}&quot;.format(t), dist.Poisson(rate), obs=y[t]) . Plate statement . From the given model above , pyro.param designate model parameters that we would like to optimize. Observations are denoted by the obs= keyword argument to pyro.sample. This specifies the likelihood function. Instead of log transforming the data, we use a LogNormal distribution. The observations are conditionally independent given the latent random variable slope and intercept. To explicitly mark this in Pyro, plate statement is used to construct conditionally independent sequences of variables. . with pyro.plate(&quot;name&quot;, size, subsample_size, device) as ind: # ...do conditionally independent stuff with ind... . However compared to range() each invocation of plate requires the user to provide a unique name. The plate statement can be used either sequentially as a generator or in parallel as a context manager. Sequential plate is similar to range()in that it generates a sequence of values. . # This version declares sequential independence and subsamples data: for i in plate(&#39;data&#39;, 100, subsample_size=10): if z[i]: # Control flow in this example prevents vectorization. obs = sample(&#39;obs_{}&#39;.format(i), dist.Normal(loc, scale), obs=data[i]) . Vectorized plate is similar to torch.arange() in that it yields an array of indices by which other tensors can be indexed. However, unlike torch.arange() plate also informs inference algorithms that the variables being indexed are conditionally independent. . # This version declares vectorized independence: with plate(&#39;data&#39;): obs = sample(&#39;obs&#39;, dist.Normal(loc, scale), obs=data) . Additionally, plate can take advantage of the conditional independence assumptions by subsampling the indices and informing inference algorithms to scale various computed values. This is typically used to subsample minibatches of data: . with plate(&quot;data&quot;, len(data), subsample_size=100) as ind: batch = data[ind] assert len(batch) == 100 . You can additionally nest plates, e.g. if you have per-pixel independence: . with pyro.plate(&quot;x_axis&quot;, 320): # within this context, batch dimension -1 is independent with pyro.plate(&quot;y_axis&quot;, 200): # within this context, batch dimensions -2 and -1 are independent . Finaly you can declare multiple plates and use them as reusable context managers. For example if you want to mix and match plates for e.g. noise that depends only on x, some noise that depends only on y, and some noise that depends on both . x_axis = pyro.plate(&quot;x_axis&quot;, 3, dim=-2) y_axis = pyro.plate(&quot;y_axis&quot;, 2, dim=-3) with x_axis: # within this context, batch dimension -2 is independent with y_axis: # within this context, batch dimension -3 is independent with x_axis, y_axis: # within this context, batch dimensions -3 and -2 are independent . def model_(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) with pyro.plate(&#39;N&#39;, len(y)) as t: log_y_hat = slope * t.type(torch.float) + intercept y=pyro.sample(&#39;y&#39;, dist.LogNormal(log_y_hat, 1.), obs=y) . Distribution shapes . Unlike PyTorch Tensors which have a single .shape attribute, pyro Distributions have two shape batch_shape and event_shape. These two combine to define the total shape of a sample. The batch_shape denote conditionally independent random variables, whereas .event_shape denote dependent random variables (ie one draw from a distribution). Because the dependent random variables define probability together, the .log_prob() method only produces a single number for each event of shape .event_shape. . d = dist.Bernoulli(0.5) print(d.batch_shape) print(d.event_shape) . torch.Size([]) torch.Size([]) . x = d.sample() x.shape . torch.Size([]) . Distributions can be batched by passing in batched parameters. . d = dist.Bernoulli(0.5*torch.ones(50)) print(d.batch_shape) print(d.event_shape) . torch.Size([50]) torch.Size([]) . x = d.sample() x.shape . torch.Size([50]) . From the two examples above, we observe that univariate distributions have empty event shape (because each number is an independent event). Let also consider multivariate distribution. . md = dist.MultivariateNormal(torch.zeros(3), torch.eye(3)) print(md.batch_shape) print(md.event_shape) . torch.Size([]) torch.Size([3]) . y = md.sample() y.shape . torch.Size([3]) . We can also create batched multivariate distribution as follows. . md = dist.MultivariateNormal(torch.zeros(3), torch.eye(3)).expand([50]) print(md.batch_shape) print(md.event_shape) . torch.Size([50]) torch.Size([3]) . y = md.sample() y.shape . torch.Size([50, 3]) . Because Multivariate distributions have nonempty .event_shape, the shapes of .sample() and .log_prob(x) differ: . md.log_prob(y).shape . torch.Size([50]) . The Distribution.sample() method also takes a sample_shape parameter that indexes over independent identically distributed (iid) random varables, such that: . sample.shape == sample_shape + batch_shape + event_shape . y_sample =md.sample([10]) y_sample.shape . torch.Size([10, 50, 3]) . Reshaping distributions . You can treat a univariate distribution as multivariate by calling the .to_event(n) property where n is the number of batch dimensions (from the right) to declare as dependent. . d = dist.Bernoulli(0.5*torch.ones(50, 3)).to_event(1) print(d.batch_shape) print(d.event_shape) . torch.Size([50]) torch.Size([3]) . While working with distributions in pyro it is essential to note that: . Samples have shape batch_shape + event_shape, | .log_prob(x) values have shape batch_shape. | You‚Äôll need to ensure that batch_shape is carefully controlled by either trimming it down with .to_event(n) or by declaring dimensions as independent via pyro.plate. | Often in Pyro we‚Äôll declare some dimensions as dependent even though they are in fact independent. This allows us to easily swap in a MultivariateNormal distribution later, but aslo it simplifies the code as we don‚Äôt need a plate. Consider the following two codes . x = pyro.sample(&quot;x&quot;, dist.Normal(0, 1).expand([10]).to_event(1)) . x.shape . torch.Size([10]) . with pyro.plate(&quot;y_plate&quot;, 10): y = pyro.sample(&quot;y&quot;, dist.Normal(0, 1)) # .expand([10]) is automatic . y.shape . torch.Size([10]) . From the two code examples, the second version with plate informs Pyro that it can make use of conditional independence information when estimating gradients, whereas in the first version Pyro must assume they are dependent (even though the normals are in fact conditionally independent). .",
            "url": "https://sambaiga.github.io/sambaiga/probabilistic%20programming/pyro/statistical%20inference/machine%20learning/2020/03/04/ppl-pyro-two.html",
            "relUrl": "/probabilistic%20programming/pyro/statistical%20inference/machine%20learning/2020/03/04/ppl-pyro-two.html",
            "date": " ‚Ä¢ Mar 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Probabilistic Programming with Pyro",
            "content": "Intro to Pyro . Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. It enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling. . Models and Probability distributions . Models are the basic unit of probabilistic programs in pyro, they represent simplified or abstract descriptions of a process by which data are generated. Models in pyro are expressed as stochastic functions which implies that models can be composed, reused, imported, and serialized just like regular Python callables. Probability distributions (pimitive stochastic functions) are important class of models (stochastic functions) used explicitly to compute the probability of the outputs given the inputs. Pyro uses PyTorch‚Äôs distribution library which contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. Each probability distributions are equipped with several methods such as: . prob(): $ log p( mathbf{x} mid theta ^{*})$ | mean: $ mathbb{E}_{p( mathbf{x} mid theta ^{*})}[ mathbf{x}]$ | sample: $ mathbf{x}^{*} sim {p( mathbf{x} mid theta ^{*})}$ | . You can also create custom distributions using transforms. . Example 1: Let define the unit normal distribution $ mathcal{N}(0,1)$, draw sample $x$ and compute the log probability according to the distribution. . import torch import pyro import pyro.distributions as dist from torch.distributions import constraints import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns pyro.set_rng_seed(101) torch.manual_seed(101) torch.set_printoptions(precision=3) %matplotlib inline . mu = 0 sigma = 1 normal=dist.Normal(mu, sigma) x = normal.rsample() # draw a sample from N(1,1) print(&quot;sample&quot;, x.item()) #To compute the log probability according to the distribution print(&quot;prob&quot;, torch.exp(normal.log_prob(x)).item()) # score the sample from N(1,1) . sample -1.3905061483383179 prob 0.15172401070594788 . Sample and Param statements . Pyro simplifies the process of sampling from distributions with the use of pyro.sample statement. The pyro.sample statement call stochastic functions or models with a unique name as identifier. Pyro‚Äôs backend uses these names to uniquely identify sample statements and change their behavior at runtime depending on how the enclosing stochastic function is being used. Using pyro.sample statement, Pyro can implement various manipulations that underlie inference algorithms. . x = pyro.sample(&quot;name&quot;, fn, obs) &quot;&quot;&quot; name ‚Äì name of sample fn ‚Äì distribution class or function obs ‚Äì observed datum (optional; should only be used in context of inference) optionally specified in kwargs &quot;&quot;&quot; . Example 2: Let sample from previous normal distribution created in example 1. . mu = 0 sigma = 1 x = pyro.sample(&quot;my_sample&quot;, dist.Normal(mu, sigma)) print(x) . tensor(-0.815) . The above code generate a random value and records it in the Pyro runtime. . data=2 x = pyro.sample(&quot;my_sample&quot;, dist.Normal(mu, sigma), obs=data) print(x) . 2 . /opt/miniconda3/lib/python3.7/site-packages/pyro/primitives.py:86: RuntimeWarning: trying to observe a value outside of inference at my_sample RuntimeWarning) . The above code conditions a stochatsic function on observed data. This should run on inference. . Pyro use pyro.param statement to saves the variable as a parameter in the param store. To interact with the param store. The pyro.param statement is used by pyro to declares a learnable parameter. . x = pyro.param(&quot;name&quot;, init_value, constraints) &quot;&quot;&quot; name ‚Äì name of param init_value ‚Äì initial value constraint ‚Äì torch constraint &quot;&quot;&quot; . Example 3: Let create theta parameter . theta = pyro.param(&quot;theta&quot;, torch.tensor(1.0), constraint=dist.constraints.positive) . Simple PPL model . Consider the following Poison Regression model begin{align} y(t) &amp; sim lambda exp(- lambda) lambda &amp; sim exp(c + m(t)) c &amp; sim mathcal{N}(1, 1) m &amp; sim mathcal{N}(0, 1) end{align} . def model(y): slope = pyro.sample(&quot;slope&quot;, dist.Normal(0, 0.1)) intercept = pyro.sample(&quot;intercept&quot;, dist.Normal(0, 1)) for t in range(len(y)): rate = torch.exp(intercept + slope * t) y[t] = pyro.sample(&quot;count_{}&quot;.format(t), dist.Poisson(rate), obs=y[t]) return slope, intercept, y . Given a pyro model. We can . Generate data from model | Learn parameters of the model from data | Use the model to predict future observation. | Generate data from model . Running a Pyro model will generate a sample from the prior. . pyro.set_rng_seed(0) # We pass counts = [None, ..., None] to indicate time duration. true_slope, true_intercept, true_counts = model([None] * 50) fig, ax = plt.subplots(figsize=(6,4)) ax = sns.lineplot(x=np.arange(len(true_counts)),y=[c.item() for c in true_counts]) . Learn parameters of the model from data . To learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data. Inference algorithms in Pyro us arbitrary stochastic functions as approximate posterior distributions. that s. These functions are called guide functions or guides and contains pyro.sample and pyro.param statement. It is a stochastic function that represents a probability distribution over the latent (unobserved) variables. The guide can be arbitrary python code just like the model, but with a few requirements: . All unobserved sample statements that appear in the model appear in the guide. | The guide has the same input signature as the model (i.e. takes the same arguments). | There are no pyro.sample statements with the obs keyword in the guide. These are exclusive to the model. | There are pyro.param statements, which are exclusive to the guide. These provide differentiation for the inputs to the pay_probs sample in the guide vs. the model. | For example if the model contains a random variable z_1 . def model(): pyro.sample(&quot;z_1&quot;, ...) . then the guide needs to have a matching sample statement . def guide(): pyro.sample(&quot;z_1&quot;, ...) . Once a guide has been specified, we can then perform learning and inference which is an optimization problem of maximizing the evidence lower bound (ELBO). The ELBO, is a function of both $ theta$ and $ phi$, defined as an expectation w.r.t. to samples from the guide: . $${ rm ELBO} equiv mathbb{E}_{q_{ phi}({ bf z})} left [ log p_{ theta}({ bf x}, { bf z}) - log q_{ phi}({ bf z}) right]$$The SVI class is unified interface for stochastic variational inference in Pyro. To use this class you need to provide: . the model, | the guide, and an | optimizer which is a wrapper a for a PyTorch optimizer as discusseced in below | . from pyro.infer import SVI, Trace_ELBO svi = SVI(model, guide, optimizer, loss=Trace_ELBO()) . The SVI object provides two methods, step() and evaluate_loss(), . The method step() takes a single gradient step and returns an estimate of the loss (i.e. minus the ELBO). | The method evaluate_loss() returns an estimate of the loss without taking a gradient step. | . Both of these methods accept an optional argument: num_particles, which denotes the number of samples used to compute the loss and gradient. . The module pyro.optim provides support for optimization in Pyro. In particular it provides PyroOptim, which is used to wrap PyTorch optimizers and manage optimizers for dynamically generated parameters. PyroOptim takes two arguments: . a constructor for PyTorch optimizers optim_constructor and | a specification of the optimizer arguments optim_args | . from pyro.optim import Adam adam_params = {&quot;lr&quot;: 0.005, &quot;betas&quot;: (0.95, 0.999)} optimizer = Adam(adam_params) . Thus to learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data (here true_counts). . For the above example we will use Autoguide pyro inference algorithm: . AutoLaplaceApproximation:Laplace approximation (quadratic approximation) approximates the posterior logùëù(ùëß|ùë•) by a multivariate normal distribution in the unconstrained space. | Autodelta: This implementation of AutoGuide uses Delta distributions to construct a MAP guide over the entire latent space. | . from pyro.infer.autoguide import AutoDelta from pyro.infer import SVI, Trace_ELBO from pyro.optim import Adam guide = AutoDelta(model) svi = SVI(model, guide, Adam({&quot;lr&quot;: 0.1}), Trace_ELBO()) for i in range(101): loss = svi.step(true_counts) # true_counts is passed as argument to model() if i % 10 == 0: print(&quot;loss = {}&quot;.format(loss)) . loss = 87295.88946688175 loss = 64525.8595520854 loss = 80838.8460238576 loss = 33014.93229973316 loss = 13704.865498423576 loss = 6232.828522503376 loss = 2017.9879159331322 loss = 631.3558134436607 loss = 170.10323333740234 loss = 198.57187271118164 loss = 207.4590385556221 . print(&quot;true_slope = {}&quot;.format(true_slope)) print(&quot;true_intercept = {}&quot;.format(true_intercept)) guess = guide() print(&quot;guess = {}&quot;.format(guess)) . true_slope = 0.15409961342811584 true_intercept = -0.293428897857666 guess = {&#39;slope&#39;: tensor(0.147, grad_fn=&lt;ExpandBackward&gt;), &#39;intercept&#39;: tensor(-0.054, grad_fn=&lt;ExpandBackward&gt;)} . Use model to predict future observation . A third way to use a Pyro model is to predict new observed data by guiding the model. This uses two of Pyro&#39;s effects: . trace records guesses made by the guide, and | replay conditions the model on those guesses, allowing the model to generate conditional samples. | . Traces are directed graphs whose nodes represent primitive calls or input/output, and whose edges represent conditional dependence relationships between those primitive calls. It return a handler that records the inputs and outputs of primitive calls and their dependencies. . We can record its execution using trace and use the resulting data structure to compute the log-joint probability of all of the sample sites in the execution or extract all parameters. . trace = pyro.poutine.trace(model).get_trace([]) pprint({ name: { &#39;value&#39;: props[&#39;value&#39;], &#39;prob&#39;: props[&#39;fn&#39;].log_prob(props[&#39;value&#39;]).exp() } for (name, props) in trace.nodes.items() if props[&#39;type&#39;] == &#39;sample&#39; }) . {&#39;intercept&#39;: {&#39;prob&#39;: tensor(0.250), &#39;value&#39;: tensor(-0.966)}, &#39;slope&#39;: {&#39;prob&#39;: tensor(2.818), &#39;value&#39;: tensor(0.083)}} . print(trace.log_prob_sum().exp()) . tensor(0.705) . Here, the trace feature will collect values every time they are sampled with sample and store them with the corresponding string name (that‚Äôs why we give each sample a name). With a little cleanup, we can print out the value and probability of each random variable‚Äôs value, along with the joint probability of the entire trace. . Replay return a callable that runs the original, reusing the values at sites in trace at those sites in the new trace. makes sample statements behave as if they had sampled the values at the corresponding sites in the trace . from pyro import poutine def forecast(forecast_steps=10): counts = true_counts + [None] * forecast_steps # observed data + blanks to fill in guide_trace = poutine.trace(guide).get_trace(counts) _, _, counts = poutine.replay(model, guide_trace)(counts) return counts . We can now call forecast() multiple times to generate samples. . for _ in range(1): full_counts = forecast(10) forecast_counts = full_counts[len(true_counts):] plt.plot([c.item() for c in full_counts], &quot;r&quot;, label=None if _ else &quot;forecast&quot;, alpha=0.3) plt.plot([c.item() for c in true_counts], &quot;k-&quot;, label=&quot;truth&quot;) plt.legend(); . References . Pyro-ducomentation | PPL models for timeseries forecasting |",
            "url": "https://sambaiga.github.io/sambaiga/probabilistic%20programming/pyro/statistical%20inference/machine%20learning/2020/03/01/ppl-pyro-intro.html",
            "relUrl": "/probabilistic%20programming/pyro/statistical%20inference/machine%20learning/2020/03/01/ppl-pyro-intro.html",
            "date": " ‚Ä¢ Mar 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://sambaiga.github.io/sambaiga/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Stochastic Variational Inference (SVI)",
            "content": "The previous post introduced the basic principle of Variational Inference (VI) as one of the approach used to approximate difficult probability distribution, derived the ELBO function and discussed about Mean Field Variational Inference (MFVI) and the Coordinate Ascent Variational Inference (CAVI) algorithms. This post introduce another stochastic gradient based algorithm (SVI) used in practise to do VI under mean filed assumptions. It also present two important tricks re-parametrization trick and amortized inference that are useful when using SVI in solving problems. . Stochastic Variational Inference (SVI) . Consider the graphical model of the observations $ mathbf{x}$ and latent variable $ mathbf{z}= { theta, z }$ in figure 1 where $ theta$ is the global variable and $z = {z_1, ldots z_n }$ is the local (per-data-point) variable such that: . . $$ p( mathbf{x}, mathbf{z}) = p( theta| alpha) prod_{i=1}^N p(x_i|z_i, theta) cdot p(z_i| alpha) $$ . Similarly the variational parameters are given by $ lambda = { gamma, phi } $ where the variational parameter $ gamma$ correspond to latent variable and $ phi$ denote set of local variational parameters. The variational distribution $q( mathbf{z} mid phi)$ is given by . $$ q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i| phi_i, alpha) $$which also depend on hyper-parameter $ alpha$. The ELBO of this graphical model $ mathcal{L}_{VI}(q) = mathbb{E}_q[ log p( mathbf{x}, mathbf{z}, alpha) - log q( mathbf{z}, gamma)]$ has the following form: . $$ begin{split} mathcal{L}_{VI}(q) &amp;= mathbb{E}_q[ log p( theta| alpha)- log q( theta| gamma)] &amp;+ sum_{i=1}^{N} mathbb{E}_q[ log p(z_i| theta) + log p(x_i|z_i, theta)- log q(z_i| phi_i)] end{split} $$ The equation above could be optimized by CAVI algorithm discussed in previous post which is expensive for large data sets. The CAVI algorithm scales with $N$ as it require to optimize the local variational parameters for each data point before re-estimating the global variational parameters. . Unlike CAI, SVI uses stochastic optimization to fit the global variational parameters by repeatedly sub-sample the data to form stochastic estimate of ELBO. In every iteration one randomly selects mini-batches of size $b_{sz}$ to obtain a stochastic estimate of ELBO. . $$ begin{split} mathcal{L}_{VI}(q) &amp;= mathbb{E}_q[ log p( theta| alpha)- log q( theta| gamma)] &amp;+ frac{N}{b_{sz}} sum_{s=1}^{b_{sz}} mathbb{E}_q[ log p(z_{i_s}| theta) + log p(x_{i_s}|z_{i_s}, theta)- log q(z_{i_s}| phi_{i_s})] end{split} $$SVI algorithms follow noisy estimates of the gradient with a decreasing step size which is often cheaper to compute than the true gradient. Following such noisy estimates allows SVI to escape shallow local optima of complex objective functions. . Natural Gradient for SVI . To solve the optimization problem standard gradient-based methods such as SGD, Adam or Adagrad can be used. However, for SVI these gradient based methods cause slow convergence or converge to inferior local models. This is because, gradient based methods use the following update . $$ theta^{t+1}= theta^t + alpha frac{ partial mathcal{L}_{VI}(q)}{ partial theta} $$where . $$ frac{ partial mathcal{L}_{VI}(q)}{ partial theta} = frac{ partial mathcal{L}_{VI}(q)}{ partial theta_1}, ldots frac{ partial mathcal{L}_{VI}(q)}{ partial theta_k} $$ . is the the gradient vector which point in the direction where the function increases most quickly while the changes in the function are measured with respect to euclidean distance. As the result, if the euclidean distance between the variational parameter being optimized is not good measure of variation in objective function then gradient descent will move suboptimal through the parameter value. . Consider the following two set of gausian distributions $$ {d_{(1)}= mathcal{N}(-2, 3), d_{(2)}= mathcal{N}(2, 3) }$$ and $$ {d_{(1)}= mathcal{N}(-2, 1), d_{(2)}= mathcal{N}(2, 1) }$$. . The euclidean distance between the two distributions $d_{}= sqrt{( mu_1- mu_2)^2+ ( sigma^2_1- sigma^2_2)^2}=4$ It clear that, considering only the euclidean distance the two images are the same. However, when we consider the shape of the distribution, the distance is different in the first and second image. In the first image, the KL-divergence should be lower as there is more overlap between between the two distribution unlike the second image where their support barely overlap. The reason for this difference is that probability distribution do not naturally fit in euclidean space rather it fit on a statistical manifold also called Riemannian manifold. . Statistical manifold give a natural way of measuring distances between distribution that euclidean distance use in SGD. A common Riemannian metric for statistical manifold is the fisher information matrix defined by . $$ F_{ lambda} = mathbb{E}_{p(x; lambda)}[ nabla log p(x; lambda) ( nabla log p(x; theta))^T ] $$ It can be showed that the fisher information matrix $F_{ lambda}$ is the second derivative of the KL divergence between two distributions. . $$ F_{ theta} = nabla^2_{ theta} KL(q(x; lambda)||p(x; theta)) $$Thus for SVI, the standard gradients descent techniques can be replaced with the natural gradient as follows: . $$ tilde{ nabla_{q}} mathcal{L}(q) = F^{-1} nabla{q} mathcal{L}_{VI}(q) $$The update procedure for natural gradient can be summarized as follows: . Compute the loss $ mathcal{L}_{VI}(q)$ | Compute the gradient of the loss $ nabla{q} mathcal{L}_{VI}(q)$ | Compute the Fisher Information Matrix F. | Compute the natural gradient $ tilde{ nabla_{q}} mathcal{L}_{VI}(q)$ | Update the parameter $q^{t+1} =q^t - alpha tilde{ nabla_{ theta}} mathcal{L}_{VI}(q)$ | Using natural gradient instead of standard gradients simplify SVI gradient update. However the same conditions for convergence as standard SDG have to be fulfilled. First, the mini-batch indices must be drawn uniformly at random size where the size $b_{sz}$ of the mini-batch must satisfies $1 leq b_{sz} leq N$ The learning rate $ alpha$ needs to decrease with iterations $t$ satisying the Robbins Monro conditions $ sum_{t=1}^{ infty} alpha_t = infty$ and $ sum_{t=1}^{ infty} alpha_t^2 &lt; infty$ This guarantee that every point in the parameter space can be reached while the gradient noise decreases quickly enough to ensure convergence. . The next section presents two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems. . Re-parametrization trick . Consider the graphical model presented in figure 1, where gradient based stochastic optimization is used to learn the variational parameter $ phi$. For example; for Gaussian distribution $q_{ phi}(z|x)= mathcal{N}( mu_{ phi}(x), Sigma_{ phi}(x))$ . To maximize the likelihood of the data, we need to back propagate the loss to the parameter $ phi$ across the distribution of $z$ or across sample $z sim q_ phi(z mid x) $ However, it is difficulty to back-propagate through random variable. To address this problem, the re-parametrization trick is used. . First let consider the Law of the Unconscious Statistician (LOTUS), that is used to calculate the expected value of a function $g( epsilon)$ of a random variable $ epsilon$ when only the probability distribution $p( epsilon)$ of $ epsilon$ is known. The Law state that: . To compute the expectation of a measurable function $g(.)$ of a random variable $ epsilon$, we have to integrate $g( epsilon)$ with respect to the distribution function of $ epsilon$, that is:$$ mathbb{E}(g( epsilon)) = int g( epsilon)dF_{ epsilon}( epsilon) $$ . In other words, to compute the expectation of $z =g( epsilon)$ we only need to know $g(.)$ and the distribution of $ epsilon$. We do not need to explicitly know the distribution of $z$. Thus the above equation can be expression in the convenient alternative notation: . $$ mathbb{E}_{ epsilon sim p( epsilon)}(g( epsilon)) = mathbb{E}_{z sim p(z)} (z) $$Therefore the reparameteriztaion trick states that: . A random variable $z$ with distribution $q_{ phi}(z, phi)$ which is independent to $ phi$ can be expressed as transformation of random variable $ epsilon sim p( epsilon)$ that come from noise distribution such as uniform or gaussian such that $z = g( phi, epsilon)$ . For instance for Gaussian variable $z$ in the above example . $$ z = mu( phi) + sigma^2( phi) cdot epsilon $$where $ epsilon sim mathcal{N}(0, 1)$. Since $p( epsilon)$ is independent of the parameter of $q_{ phi}(z, phi)$, we can apply the change of variables in integral theory to compute any expectation over $z$ or any expectation over $ phi$. The SDG estimator can therefore be estimated by pulling the gradient into expectations and approximating it by samples from the noise distribution such that for any measurable function $f_{ theta}(.)$:$$ Delta_{ phi} mathbb{E}_{z sim p_{ phi}(z)} = frac{1}{M} sum_{i=1}^M Delta f(g( phi, epsilon_i)) $$ . where $ epsilon_i sim p( epsilon)$ , $f_{ theta}(.)$ must be differentiable w.r.t its input $z$ and $g( phi, epsilon_i)$ must exist and be differentiable with respect to $ phi$. . Amortized Variational Inference . Consider the graphical model presented in figure 1 where ecah data point $x_i$ is governed by its latent variable $z_i$ with variational parameter $phi_i$ such that . $$ q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i| phi_i, alpha) $$Using traditional SVI make it necessary to optimize $ phi_i$ for each data point $x_i$. As the results the number parameters to be optimized will grows with the number of observations $x$. This is not ideal for larger datasets. Apart from that, it requires one to re-run the optimization procedure in case of new observation or when we have to perform inference. To address these problem amortized VI introduce a parametrized function that maps from observation space to the parameter of the approximate posterior distribution. . Amortized VI try to learn from past inference/pre-computation so that future inferences run faster. Instead of approximating separate variables for each data point $x_i$, amortized VI assume that the local variational parameter $ phi$ can be predicted by a parametrized function $f_{ phi}(.)$ of data whose parameters are shared across all data points. Thus instead of introducing local variational parameter, we learn a single parametric function and work with a variational distribution that has the form . $$ q( mathbf{z} mid phi) = q( theta| gamma) prod_{i=1}^N q(z_i|f_{ phi}(.)) $$where $f_{ phi}(.)$ is the deep neural net function of $z$ . Deep neural network used in this context are called inference networks. Therefore amortized inference with inference networks combines probabilistic modelling with representation power of deep learning. Using amortized VI instead of traditional VI, has two important advantages. First the number of variational parameters remain constant with respect to the data size. We only need to specify the parameter of the neural networks which is independent to the number of observations. Second, for new observation or during inference all we need to do is to call the inference network. As the result, we can invest time upfront optimizing the inference network and during inference we use the trained network for fast inference. . Consider a binary regression problem where we are intrested on predicting whether or not a customer will subscribe a term deposit after the marketing campaign the bank performed. . ## Reference 1. [[Cheng Zhang,(2017)]](https://arxiv.org/abs/1711.05597): Advances in Variational Inference. 2. [[Daniel Ritchie,(2016)]](https://arxiv.org/abs/1610.05735):Deep Amortized Inference for Probabilistic Programs. 3. [[Andrew Miller,(2016)]](https://arxiv.org/abs/1610.05735):Natural Gradients and Stochastic Variational Inference. 4. [Shakir Mohamed](https://www.shakirm.com/papers/VITutorial.pdf):Variational Inference for Machine Learning. 5. [DS3 workshop](https://emtiyaz.github.io/teaching/ds3_2018/ds3.html):Approximate Bayesian Inference: Old and New. 6. [Variational Inference and Deep Generative Models](https://github.com/philschulz/VITutorial):Variational Inference for NLP audiences .",
            "url": "https://sambaiga.github.io/sambaiga/deep%20learning/machine%20learning/generative%20model/2019/05/02/svi.html",
            "relUrl": "/deep%20learning/machine%20learning/generative%20model/2019/05/02/svi.html",
            "date": " ‚Ä¢ May 2, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Basics of Python for Data Science and Machine learning",
            "content": "images/post/python.jpg . Basics . Python is a very popular general-purpose programming language. . Open source general-purpose language | Dynamically semantics (rather than statically typed like Java or C/C++) | Interpreted (rather than compiled like Java or C/C++) | Object Oriented. | . Why is Python such an effective tool in scientific research ? . Interoperability with Other Languages : You can use it in the shell on microtasks, or interactively, or in scripts, or build enterprise software with GUIs. | ‚ÄúBatteries Included‚Äù + Third-Party Modules : Python has built-in libraries and third-party liabraies for nearly everything. | Simplicity &amp; Dynamic Nature : You can run your Python code on any architecture. | Open ethos well-fit to science : Easy to reproduce results with python | . Why is Python such an effective tool for Data Science and Machine learning research . Very rich scientific computing libraries (numpy, matplotlib, pandas, scipy etc) and machine learning frameworks (Pytorch, Tensorflow, keras, mxnet, etc) | All DS and ML tasks can be performed with Python : accessing, collecting, cleaning, analysing, visualising data modelling, evaluating models, integrating in prod, scaling etc. | . | . Python 2 VS. Python 3 . Two major versions of Python in widespread use : Python 2.x and Python 3.x . Some features in Python 3 are not backward compatible with Python 2 | Some Python 2 libraries have not been updated to work with Python 3 | Bottom-line : there is no wrong choice, as long as all the libraries you need are supported by the version you choose but most of libararies are phasing out python 2.x. | In this training : Python3 | . Python vs other language . print(&quot;Hello World&quot;) . Consider hello world in Java and C++ . Java . public class HelloWorld { public static void main(String[] args) { // Prints &quot;Hello, World&quot; to the terminal window. System.out.println(&quot;Hello, World&quot;); } } . C++ . #include &lt;iostream&gt; using namespace std; int main() { cout &lt;&lt; &quot;Hello, World!&quot;; return 0; } . 1.1 Comments . Text to the right of the // symbol are comments used as notes for the reader of the program. . print(&quot;Hello world&quot;) # This is hello world . Use as many useful comments as you can in your program to: . explain assumptions | explain important decisions | explain important details | explain problems you&#39;re trying to solve | explain problems you&#39;re trying to overcome in your program, etc | . Code tells you how, comments should tell you why. . 1.2 Variable and Print Statement . Variable: A name that is used to denote something or a value is called a variable. In python, variables can be declared and values can be assigned to it. . We use a single equals sign to assign labels to variables. Let&#39;s see a few examples of how we can do thi . # Variables and assignment a = 3 print (a) . s = &quot;parrotai&quot; print(s) . c= 10 + 5j print(c) . To know the type of variable use type() function . type(a) . type(c) . type(s) . Choosing a variable name: . The names you use when creating these labels need to follow a few rules: . Names can not start with a number. | There can be no spaces in the name, use _ instead. | Can&#39;t use any of these symbols :&#39;&quot;,&lt;&gt;/?|()!@#$%^&amp;*~-+ | It&#39;s considered best practice (PEP8) that the names are lowercase. | Print Statements . The print statement can be used in the following different ways : . - print(variable_name) - print(&quot;Hello World&quot;) - print (&quot;Hello&quot;, &lt;Variable &gt;) - print (&quot;Hello&quot; + &lt;Variable Containing the String&gt;) - print (&quot;Hello %s&quot; % &lt;variable containing the string&gt;) - print (&quot;Hello&quot; + &lt;Variable Containing the String&gt;) - print (&quot;Hello %d&quot; % &lt;variable containing integer datatype&gt;) - print (&quot;Hello %f&quot; % &lt;variable containing float datatype&gt;) . acc = 89 fs = 60.20 model = &quot;Random Forest&quot; print(&quot;The perfomance results for %s model: Accuracy: %d ; F-score: %.3f&quot; %(model,acc,fs)) . Alternatively you can use .format() in print function . print(&quot;The perfomance results for {0} model: Accuracy: {1} ; F-score: {2}&quot; .format(model,acc,fs)) . print(f&quot;The perfomance results for {model} model: Accuracy: {acc} ; F-score: {fs}&quot;) . Activity 1: Write a program that prints your full name and your birthdate as separate strings. # DO IT HERE . User Input . input( ) accepts input and stores it as a string. Hence, if the user inputs a integer, the code should convert the string to an integer and then proceed... . a = input(&quot;Type something here and it will be stored in this variable t&quot;) . a . type(a) . You can convert the enterd number to int, using int() function to float with foat() function as follows: . a = int(input(&quot;Only integer can be stored in this variable t&quot;)) b = float(input(&quot;Only float can be stored in this variable t&quot;)) . print(a) . print(b) . 1.3 Operators and Expressions . Numeric operator . + (plus) : 2 + 1 =3 | - (minus): : 2 - 1 =1 | * (multiply): 2*1 =2 | ** (power): 3**2 =9 | / (divide): 3/2 =1.5 | // (divide and floor): 3//2 =1 | % (modulo)&quot; 5%3=2 | . x = 5 y = 6.0 . print(x+y) . print(x*y) . print(x**y) . print(20%6) . Logical operators . == (equal to)) : Compares if the objects are equal | != (not equal to): : Compares if the objects are not equal | not : (boolean NOT) | and (boolean AND) | or (boolean OR) | &gt; (greater than): Returns whether x is greater than y | &lt; (less than): Returns whether x is less than y. | &lt;= (less than or equal to) | &gt;= (greater than or equal to) | . Activity 2: Suppose you have $20,000$ tsh, which you can invest with a $10$ % return each year. Write python code to calculate how much money you end up with after $5$ years. 1.4 Control Flow . Indentation . It is important to keep a good understanding of how indentation works in Python to maintain the structure and order of your code. We will touch on this topic again when we start building out functions! . IF statement . The if statement is used to check a condition: if the condition is true, we run a block of statements (called the if-block), else we process another block of statements (called the else-block). The else clause is optional. . if . if some_condition: algorithm . If-else . if some_condition: algorithm else: algorithm . if-elif (nested if) . if some_condition: algorithm elif some_condition: algorithm else: algorithm . # Example avoid division by zero val = 0 num = 10 if val == 0: val += 2e-07 result = num / val print(&quot;{0} dived {1:.2f} = {2:.2f}&quot;.format(num, val, result)) . # Example avoid division by zero user_name = &quot;sambaiga&quot; password = &quot;sa1254&quot; if user_name==&quot;sambaiga&quot;: print(&quot;Hello &quot;,user_name) if password !=&quot;sa1234&quot;: print(&quot;Wrong password&quot;) else: print(&quot;Access granted&quot;) else: print(&quot;user does not exist&quot;) . # Unlike other languages, indentation is significant a = 5 if a &gt; 10: print (&#39;a is greater than 10&#39;) if a &gt;= 15: print (&#39;a is also at least 15&#39;) elif a &gt; 5: print (&#39;a is greater than 5 but not greater than 10&#39;) else: print (&#39;no condition matched&#39;) print (&#39;so a is 5 or less&#39;) . While statement . The while statement allows you to repeatedly execute a block of statements as long as a condition is true. . while some_condition: algorithm . # Example i = 1 while i &lt; 5: print(&quot;The square root of {0} is {1}&quot;.format(i, i**2)) i = i+1 . For Loops . A for loop acts as an iterator in Python, it goes through items that are in a sequence or any other iterable item. We will see more about sequences in detail in later section. . for variable in something: algorithm . for i in range(6): print(i) . Note range( ) function outputs the integers of the specified range. It can also be used to generate a series by specifying the difference between the two numbers within a particular range. The elements are returned in a list. . Example: . Another common idea during a for loop is keeping some sort of running tally during the multiple loops. For example, lets create a for loop that sums up the list of number from 0 to 9: . num_sum = 0 for num in range(10): num_sum +=num print(num_sum) . Activity 3: Compute $$ sum_{k=1}^N frac{1}{2^k}$$ for N = 10, N = 20 and N = 100. What happens when n gets larger and larger? The break and continue Statement . The break statement is used to break out of a loop statement i.e. stop the execution of a looping statement, even if the loop condition has not become False or the sequence of items has not been completely iterated over. . | The continue statement is used to tell Python to skip the rest of the statements in the current loop block and to continue to the next iteration of the loop. . | . while True: s = int(input(&#39;Enter something : &#39;)) if s == 0: break if s%2 == 0: print(&#39;The number is even&#39;) continue #print(&#39;Input is of sufficient length&#39;) # Do other kinds of processing here... . Progress bar . Make your loops show a smart progress meter . import tqdm from tqdm import tqdm as pbar . sum_of_n = 0 N = 1e10 for i in pbar(range(10000000)): sum_of_n+=i print(sum_of_n) . 1.5 Data Structures: . Are structures which can hold some data together, used to store a collection of related data. . 1.5.1 Sequences . Sequence Types:There are three basic sequence types: lists, tuples, and range objects. . Lists . create list | add item to list | access elements of a list | . #name, gender, height, age, weight, region, status data = [&quot;James John&quot;, &quot;M&quot;, 176, 28, &quot;Dodoma&quot;, 1] print(data) . data[0] . data[0:2] . #what about #data[-1] . data.append(&quot;350K&quot;) data . # access all elements in a list for item in data: print(item) . List comprehensions . # create list of square of number from 1 to 10 [x**2 for x in range(1, 11)] . ### alternatively square = [] for x in range(1, 11): square.append(x**2) square . ## size of a list len(square) . More list methods . Activity 4: Build a list that contains every prime number between 1 and 100, in two different ways: 1. Using for loops and conditional if statements. 2. Using a list comprehension. ## CODE HERE . Tuples . A tuple consists of a number of values separated by commas, for instance: . t = 2, 5, &#39;parrot&#39; t . (2, 5, &#39;parrot&#39;) . As you see, on output tuples are always enclosed in parentheses . Though tuples may seem similar to lists, they are often used in different situations and for different purposes. . Tuples are immutable, and usually contain a heterogeneous sequence of elements are accessed via unpacking. | Lists are mutable, and their elements are usually homogeneous and are accessed by iterating over the list. | . The statement t = 2, 5, &#39;parrot&#39; is an example of tuple packing: the values 2, 5, &#39;parrot are packed together in a tuple. The reverse operation is also possible: . x, y,z = t . Range . The range type represents an immutable sequence of numbers and is commonly used for looping a specific number of times in for loops. . syntax: range(start, stop, step) . start: The value of the start parameter (or 0 if the parameter was not supplied) | stop: The value of the stop parameter | step: The value of the step parameter (or 1 if the parameter was not supplied) | . list(range(10)) . list(range(0, 30, 5)) . Common Sequence Operations . The following operations are supported by most sequence types, both mutable and immutable. . x in s: True if an item of s is equal to x, else False | x not in s: False if an item of s is equal to x, else True | s + t :the concatenation of s and t | s * n or n * s: equivalent to adding s to itself n times | len(s): length of s | min(s): smallest item of s | max(s): largest item of s | s.count(x): total number of occurrences of x in s | . Important python function . To loop over two or more sequences at the same time, the entries can be paired with the zip() function. | When looping through a sequence, the position index and corresponding value can be retrieved at the same time using the enumerate() function. | . days = [&#39;Mon&#39;, &#39;Tue&#39;, &#39;Wed&#39;, &#39;Thus&#39;, &#39;Fr&#39;, &quot;Sat&quot;, &#39;Sun&#39;] months = [&#39;Jan&#39;, &#39;Feb&#39;, &#39;March&#39;, &#39;Apr&#39;, &#39;May&#39;, &quot;Jun&quot;, &#39;July&#39;] . ## enumerate: for i, day in enumerate(days): print(f&#39;index {i}, {day}&#39;) . ## enumerate: for day, month in zip(days, months): print(f&#39;{day}, {month}&#39;) . 1.5.2 Dictionary . We&#39;ve been learning about sequences in Python but now we&#39;re going to switch gears and learn about mappings in Python. If you&#39;re familiar with other languages you can think of these Dictionaries as hash tables. Dictionaries are more used like a database because here you can index a particular sequence with your user defined string. . Mappings are a collection of objects that are stored by a key, unlike a sequence that stored objects by their relative position. This is an important distinction, since mappings won&#39;t retain order since they have objects defined by a key. A Python dictionary consists of a key and then an associated value. That value can be almost any Python object. . We create a dictionary with {} and : to signify a key and a value . my_dict = {&#39;key1&#39;:&#39;value1&#39;,&#39;key2&#39;:&#39;value2&#39;} . # create dictionary dic = { &#39;name&#39;:&quot;James John&quot;, &#39;age&#39;: 28, &#39;gender&#39;:&quot;M&quot;, &#39;region&#39;:&quot;Dodoma&quot;, &#39;status&#39;:1 } print(dic) . {&#39;name&#39;: &#39;James John&#39;, &#39;age&#39;: 28, &#39;gender&#39;: &#39;M&#39;, &#39;region&#39;: &#39;Dodoma&#39;, &#39;status&#39;: 1} . In addition, dict comprehensions can be used to create dictionaries from arbitrary key and value expressions: . square={x: x**2 for x in range(1,11)} square . # Access value of element by key - most important feature! print(dic[&#39;age&#39;]) . 28 . dic[&#39;salary&#39;] = &quot;310K&quot; print(dic) . #get list keys of a dictionary dic.keys() . dict_keys([&#39;name&#39;, &#39;age&#39;, &#39;gender&#39;, &#39;region&#39;, &#39;status&#39;]) . #get list values in a dictionary dic.values() . dict_values([&#39;James John&#39;, 28, &#39;M&#39;, &#39;Dodoma&#39;, 1]) . #get list key, values pairs item in a dictionary for key , value in dic.items(): print(&quot;{0}: {1}&quot;.format(key, value)) . 1.6 Functions . Functions will be one of our main building blocks when we construct larger amounts of code to solve problems. . function is a useful device that groups together a set of statements so they can be run more than once. They can also let us specify parameters that can serve as inputs to the functions. . | functions allow us to not have to repeatedly write the same code again and again. . | function in Python is defined by a def statement. The general syntax looks like this: | . def name_of_function(arg1,arg2): &#39;&#39;&#39; This is where the function&#39;s Document String (doc-string) goes &#39;&#39;&#39; # Do stuff here #return desired result . Note: Docstrings: . Documentation about what the function does and its parameters. General convention: . def normalize(data=None, mean=None, std=None): &#39;&#39;&#39; Normalization function arguments: data: the data value you want to normalize mean: mean value of your data std: standard deviation of your data return: z-score: normalized value &#39;&#39;&#39; return (data - mean)/ std . result = normalize(data=27.8, mean=18, std=6) print(&quot;Normalized value is {:.2f}&quot;.format(result)) . Activity 5:Write a function called accept login(users, username, password) with three parameters: users a dictionary of username keys and password values, username a string for a login name and password a string for a password. The function should return True if the user exists and the password is correct and False otherwise. Here is the calling code, test your code with both good and bad passwords as well as non-existent login names: ## CODE HERE . Activity 6: The distance between two points x and y is the square root of the sum of squared differences along each dimension of x and y. Create a function distance(x, y) that takes two vectors and outputs the distance between them. Use your function to find the distance between (0,0) and (1,1). ## CODE HERE . Activity 7: Using distance() function created in activity 6. Make a function in_circle(x, origin) that uses distance to determine if a two-dimensional point falls within the the unit circle with a given origin. That is, find if a two-dimensional point has distance $&lt;1$ from the origin (0,0).Use your function to print whether the point (1,1) lies within the unit circle centered at (0,0). ## CODE HERE . Lambda Expressions . Small anonymous functions can be created with the lambda keyword. In Python, anonymous function means that a function is without a name. . syntax: lambda arguments: expression . sq = lambda x: x**2 . sq(2) . Lambda functions can be used along with built-in functions like . filter(): Construct an iterator from those elements of iterable for which function returns true, | map(): Return an iterator that applies function to every item of iterable, yielding the results and | . Example: . Given list of ages for participants of AISG bootcamp = [10, 12, 22, 80, 50, 16, 33, 18, 17, 29] . Let us filter out teenagers from this list . age = [10, 12, 22, 80, 50, 16, 33, 18, 17, 29] lf=lambda x: (x&lt;=18 and x&gt;=12) teenagers=list(filter(lf , age)) print(teenagers) . Let use standarzide the list of age . import statistics as st mu=st.mean(age) std=st.stdev(age) lf = lambda x: round((x - mu)/std, 2) standarzied_age = list(map(lf ,age)) print(standarzied_age) . 1.7 Module . Modules are organized units (written as files) which contain functions, statements and other definitions. . Any file ending in .py is treated as a module (e.g., my_function.py, which names and defines a function my_function) . | Modules: own global names/functions so you can name things whatever you want there and not conflict with the names in other modules. . | . %%writefile normalizer.py def normal(data=None, mean=None, std=None): &#39;&#39;&#39; Normalization function arguments: data: the data value you want to normalize mean: mean value of your data std: standard deviation of your data return: z-score: normalized value &#39;&#39;&#39; return (data - mean)/ std . Packages . Packages are name-spaces which contain multiple packages and modules themselves. They are simply directories, but with a twist. | Each package in Python is a directory which MUST contain a special file called __init__.py. This file can be empty, and it indicates that the directory it contains is a Python package, so it can be imported the same way a module can be imported. | . To Import modules and packages . Different options are available: . import package-name importing all functionalities as such&lt;/li&gt; from package-name import specific function importing a specific function or subset of the package/module&lt;/li&gt; from package-name import * importing all definitions and actions of the package (sometimes better than option 1)&lt;/li&gt; import package-name as short-package-name Very good way to keep a good insight in where you use what package&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import normalizer result = normalizer.normal(data=27.8, mean=18, std=6) print(&quot;Normalized value is {:.2f}&quot;.format(result)) . #Alternatively from normalizer import normal result = normal(data=27.8, mean=18, std=6) print(&quot;Normalized value is {:.2f}&quot;.format(result)) . Import modules from another directory . import sys sys.path.append(&#39;src/&#39;) import normalizer as norm result = norm.normal(data=27.8, mean=18, std=6) print(&quot;Normalized value is {:.2f}&quot;.format(result)) . Important python module . statistics ‚Äî Mathematical statistics functions: This module provides functions for calculating mathematical statistics of numeric (Real-valued) data | math ‚Äî Mathematical functions: This module provides access to the mathematical functions defined by the C standard. | random‚Äî Generate pseudo-random numbers:This module implements pseudo-random number generators for various distributions. | . import math . Activity 8: Using the math library, calculate and print the value of pi / 4. ## CODE HERE . import random ## Random Shuffle data in a list random.shuffle(age) print(age) . ## Random select element from a list random.choice(age) . # Return a k length list of unique elements chosen from the population sequence or set. Used for random sampling without replacement. random.sample(age, 5) . Activity 9: Using random.uniform, create a function rand(n) that generates a single float between -n and n. Call rand() once. So we can check your solution, we will use random.seed to fix the value called by your function. Note: add random.seed(1) to fixes the value called by your function, def rand(n=1): random.seed(1) #code here . Activity 10: Use the rand function defined from previous exercise to create a list x with 10000 number between -1 and 1 ## CODE HERE . Activity 11: Use the in_circle are defined from previous exercise. Create a list of 10000 booleans called inside that are True if and only if the point in x with that index falls within the unit circle. Make sure to use in_circle. Print the proportion of points within the circle. This proportion is an estimate of the ratio of the two areas! 1.7 Reading and Writing Data to Files in Python . To read and write file in python you must first open files in the appropriate mode and read its contents: . with open(&#39;data/sms.txt&#39;, &#39;r&#39;) as f: sms = f.read() f.close() . open() takes a filename and a mode as its arguments. | r opens the file in read only mode. To write data to a file, pass in w as an argument instead: | . sms . with open(&#39;data/data.txt&#39;, &#39;w&#39;) as f: data = &#39;some data to be written to the file&#39; f.write(data) f.close() . data . Reading text files line-by-line and save to list . sms_list = [] with open(&#39;data/sms.txt&#39;, &#39;r&#39;) as f: for line in f: l=line.strip() #removes any leading (spaces at the beginning) and trailing (spaces at the end) characters if len(l)!=0: # The EOF char is an empty string sms_list+=[float(l)] f.close() . sms_list . st.mean(sms_list) . st.variance(sms_list) . st.stdev(sms_list) . Directory Listing . os.listdir() is the method to use to get a directory listing: . import os dirs = os.listdir(&#39;data/&#39;) dirs . os.listdir() returns a Python list containing the names of the files and subdirectories in the directory given by the path argument. . for file in dirs: print(file) . In modern versions of Python, an alternative to os.listdir() is to use os.scandir() and pathlib.Path(). . with os.scandir(&#39;data/&#39;) as dirs: for file in dirs: print(file.name) . from pathlib import Path dirs = Path(&#39;data/&#39;) for file in dirs.iterdir(): print(file.name) . Making Directories . ##check if path exist os.path.isdir(&quot;data/&quot;) . if os.path.isdir(&quot;results/&quot;): os.mkdir(&#39;results/&#39;) . Exercise 1 : . A list of numbers can be very unsmooth, meaning very high numbers can be right next to very low numbers. This list may represent a smooth path in reality that is masked with random noise.One way to smooth the values in the list is to replace each value with the average of each value&#39;s neighbors, including the value itself. . Write a function moving_window_average(x, n_neighbors) that takes a list x and the number of neighbors n_neighbors on either side to consider. For each value, the function computes the average of each value&#39;s neighbors, including themselves. Have the function return a list of these averaged values that is the same length as the original list. If there are not enough neighbors (for cases near the edge), substitute the original value as many times as there are missing neighbors. Use your function to find the moving window sum of sms list and n_neighbors=2. | Compute the moving window average for sms_list for the range of n_neighbors 1-9. Store sms_list as well as each of these averages as consecutive lists in a list called sms_mva. | For each list in sms_mva, calculate and store the range (the maximum minus the minimum) in two decimal places in a new list ranges. Print your answer. As the window width increases, does the range of each list increase or decrease? Why do you think that is? | #solution 1 def moving_window_average(x, n_neighbors=1): n = len(x) width = n_neighbors*2 + 1 x = [x[0]]*n_neighbors + x + [x[-1]]*n_neighbors # To complete the function, # return a list of the mean of values from i to i+width for all values i from 0 to n-1. return [sum(x[i:(i+width)]) / width for i in range(n)] mv=moving_window_average(sms_list, 2) . #sloution 2 Y = [sms_list] + [moving_window_average(sms_list, n_neighbors) for n_neighbors in range(1, 10)] print(len(Y)) . #solution 3 ranges = [round(max(x)-min(x),2) for x in Y] print(ranges) . References . References . python4datascience-atc | PythonDataScienceHandbook | DS-python-data-analysis | Working With Files in Python | . &lt;/div&gt; . | . | . | . | .",
            "url": "https://sambaiga.github.io/sambaiga/python/2019/04/01/python-basics.html",
            "relUrl": "/python/2019/04/01/python-basics.html",
            "date": " ‚Ä¢ Apr 1, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Variational Inference (VI)",
            "content": "The bayesian method offers a different paradigm for doing statistical analysis. It is a practical method for making inferences from data using probability models. Unlike other analytical approaches, Bayesian models are easy to interpret and incorporate uncertainty. In the Bayesian method we start with a belief which is also called a prior. We then update our belief after observing some data. The outcome is called a posterior. The process repeats as we keep on seeing more data where the old posterior becomes a new prior. The method employs the Bayes rule. . Consider the Bayesian theorem, which allows us to use some knowledge or belief that we already have. Given data point $ mathcal{D} = {x in mathbb{R}^{N times d}, y in mathbb{R}^{N times c} }$. The Bayesian approach treats the latent variable or parameter $z$ as a random variable with some prior distribution $p(z)$. This is the probability of parameters $z$ beforehand. . $$ p(z | mathcal{D} ) = frac{p( mathcal{D} | z ) cdot p(z)}{p( mathcal{D})} $$where . $$ p( mathcal{D}) = int p( mathcal{D} |z ) cdot p(z) dz $$From the bayesian theorem above, $z$ is the hypothesis about the world, and $$ mathcal{D}$$ is the data or evidence. The probability $p( mathcal{D} mid z)$ is called likeli-hood; the probability of data given the latent variable and $p( mathcal{D})$ is the marginal-likelihood and $p(z mid mathcal{D} )$ is the posterior. . Bayesian Inference . Given data set $ mathcal{D}$ and latent variable $z$ that relate $x$ and $y$ such that: $$ y = f_{z}(x:z) $$ The first step in Bayesian inference is to identify the parameter $z$ and express our lack of knowledge about this parameter in terms of probability distribution $p(z)$. This is the prior knowledge about the parameter $z$. After that we express a likelihood $p( mathcal{D} mid z)$ which tell us how the data $ mathcal{D}$ interact with parameter $z$. Together the prior and the likelihood make our model (generative model). It tells us how we can simulate our data. . In training stage we apply Bayesian theorem to get posterior distribution: . $$ p(z| mathcal{D}) = frac{p( mathcal{D}|, z)}{p( mathcal{D})} $$ . In the testing stage, we find predictive-distribution . $$ p( hat{y}| x, mathcal{D}) = int p( hat{y} | x, z) cdot p(z | x, y) dz $$ $y= f_{ theta}(x: theta) + in $ where $ in$ is the noise due to measurement and $f_{ theta}(X: theta)$ is the hypothesis function given; $ f_{ theta}(X: theta) = b + sum_{i=1}^N w_i phi (x_i) = theta^T cdot phi(X) $ . where $ phi(X)$ is the basis function and $ theta$ is the model parameters such that $ theta_{0} =b$$ and $$ phi_0=1 $ . The output of this model is the single point estimate for the best model parameter. The Bayesian modelling approach to this problem offer a systematic framework for learning distribution over values of the parameters and not a single estimate. The bayesian linear regression model $y= f_{ theta}(x: theta) + in $ as a Gaussian distribution such that: $ p(y|x, theta) = mathcal{N}(y|f_{ theta}(x: theta), beta^{-1}) $ . Assuming the data point are drawn independently and identically distributed the likelihood is expressed as: . $$ p(Y| X, theta) = prod_{i=1}^N mathcal{N}(y_i|f_{ theta}(x_i: theta_i), beta^{-1}) $$Let choose a prior that is conjugate to the likelihood . $$ p( theta|X) = mathcal{N}( theta|0, alpha^{-1}) $$Thus the posterior is given as: . $$ p( theta|Y, X) propto mathcal{N}( theta|0, alpha^{-1}) cdot prod_{i=1}^N mathcal{N}(y_i|f_{ theta}(x_i: theta_i), beta^{-1}) $$ Variational Inference (VI) . In the previous section we show that inference in probabilist model is often intractable and introduced several approach used to approximate the inference. Variational Inference (VI) is one of the approach used to approximate difficult probability distribution by turning the calculation about model into optimization. . Consider a probabilistic model which is joint distribution $p(x,z)$ of the latent variable $$z$$ observed variables $x$. To draw inference on the latent variable $z$ we compute the posterior . $$ p(z|x) = frac{p(x,z)}{p(x)} = frac{p(x|z) cdot p(z)}{p(x)} $$where $ p(x)= int p(x|z) cdot p(z) dz $ To approximate $p(z mid x)$ we first choose an approximating family of distribution $q(z)$ over latent variable $z$. Then we find set of parameters that makes $q(z)$ close to posterior distribution $p(z mid { bf x})$. Thus VI approximate $p(z mid x)$ with new distribution $q(z)$ such that $q(z)$ is close to $p(z mid x)$. To achieve this we minimize KL divergence between $q(z)$ and $p(z mid x)$ such that: . $$ q^*(z) = arg min D_{KL}(q(z)||p(z|x)) $$where $ D_{KL}(q(z)||p(z|x)) = int q(z) log frac{q(z)}{p(z|x)} $ . It clear that we can not minimize KL divergence since it is directly depend on posterior $p(z mid x)$. However we can minimize a function that is equal to KL divergence plus constant. This function is called Evidence Lower Bound(ELBO) $ mathcal{L}_{VI}(q)$. . Evidence Lower Bound (ELBO) . To derive the ELBO we first consider the Jensen&#39;s inequality which relates the value of a convex function of an integral to the integral of the convex function such that $f( mathbb{E}[x]) geq mathbb{E}[f(x)]$ where $f(.)$ is the concave function. Since logarithmic are strictly concave function it is clear that . $$ log int p(x)g(x) dx geq int p(x) log g(x) $$Let us consider a log of marginal evidence. . $$ begin{aligned} log p(x) &amp; = log int_z p(x,z) dz &amp; = log int_z p(x,z) cdot frac{q(z)}{q(z)} dz &amp; = log int_z q(z) frac{p(x,z)}{q(z)} dz &amp; = log left( mathbb{E}_q[ frac{p(x,z)}{q(z)}] right) &amp; geq mathbb{E}_q[ log p(x,z)] - mathbb{E}_q[ log q(z)] end{aligned} $$The final line is the ELBO which is the lower bound for the evidence. Thus the evidence lower bound for probability model $$p(x,z)$$ and approximation $$q(z)$$ to the posterior is . $$ mathcal{L}_{VI}(q) = mathbb{E}_q[ log p(x,z)] - mathbb{E}_q[ log q(z)] $$We can now show that KL divergence to the posterior is equal to the negative ELBO plus constant. . $$ begin{aligned} D_{KL}(q(z)||p(z|x)) &amp;= int q(z) log frac{q(z)}{p(z|x)} &amp;= mathbb{E}_q[ log q(z)] - mathbb{E}_q[ log p(x,z)] + mathbb{E}_q[ log p(x)] &amp;=- left( mathbb{E}_q[ log p(x,z)] - mathbb{E}_q[ log q(z)] right) + log p(x) &amp;= - mathcal{L}_{VI}(q) + log p(x) mathcal{L}_{VI}(q) &amp;= log p(x) + D_{KL}(q(z)||p(z|x)) end{aligned} $$From the equation above it clear that minimizing the KL divergence is equivalent to maximizing the ELBO. Recall that we want to find $$q(z)$$ such that KL divergence is small, the variational objective function becomes . $$ q^*(z) = arg min D_{KL}(q(z)||p(z|x)) = arg max mathcal{L}_{VI}(q) $$Mean Field Variational Inference . One of the important question on VI, is how to construct the family of variational distributions from which we want to draw $q(z)$ from. The simplest family is where each latent parameter $z_i$ has its own independent distribution. This means that we can easily factorize the variational distributions into groups: . $$ q(z_1, ldots, z_m) = prod_{i=1}^m q(z_i) $$This family of distribuion are known as Mean-Field Variational Family that make use of mean field theory. Inference using this factorization is known as Mean-Field Variational Inference (MFVI). . It possible to further parameterize the approximating distributions $q(z)$ with variational parameters $ lambda$ such that the approximating distribution become $q(z_i ; lambda_i)$. For example if we set our family of approximating distributions as a set of independent gauasian distributions $ mathcal{N}( mu_i, sigma^2_i)$ and parameterize this distributions with the mean and variance where $ lambda_i = ( mu_i, sigma^2_i)$ is the set of variational parameters. . The common algorithms used in practise to do VI under mean filed assumptions are coordinate ascent optimization (CAVI) and stochastic gradient based method. . Coordinate Ascent Variational Inference (CAVI) . The CAVI algorithm derive variational updates by hand and perform coordinate ascent (iteratively updating each latent variable $z_i$) on the latent until convergence of the ELBO. A common procedure to conduct CAVI is: . Choose variational distributions $q(z)$ | Compute ELBO; | Optimize individual $q(z_i)$ ‚Äôs by taking the gradient for each latent variable; | Repeat until ELBO converges. | . The coordinate ascent update for a latent variable can be derived by maximizing the ELBO function above. First, recall ELBO . $$ mathcal{L}_{VI}(q) = mathbb{E}_q[ log p(x,z)] - mathbb{E}_q[ log q(z)] $$ Applying chain rule we can decomopse the joint $p(x,z)$ as; $$ p(x_{1:n}, z_{1:m}) = p(x_{1:n}) prod_{i=1}^m p(z_i|z_{1:(i-1)}, x_{1:n}) $$ Using mean field approximation, we can decompose the entropy term of the ELBO as $$ mathbb{E}_q[ log q(z)] = sum_{i=1}^m mathbb{E}_q[ log q(z_i)] $$ Under the above assumption the ELBO become: $$ mathcal{L}_{ELBO}(q) = log p(x_{1:n}) + sum_{i=1}^m mathbb{E}_q[ log p(z_i|z_{1:(i-1)}, x_{1:n}) ] - mathbb{E}_q[ log q(z_i) $$ . To find this $ arg max_{q(z_i)} mathcal{L}_{ELBO}(q)$ we take derivative of ELBO with respect to $q(z_i)$ using Lagrange multipliers and set the derivative to zero. It can be shown that the coordinate ascent update rule is equal to . $$ q^*(z_i) propto { mathbb{E}_{q-i}[ log q(z_i,z_{ neg},x)] } $$where the notation $ neg$ denotes all indices other than the $i^{th}$ . Despite being very fast method for some models only work with conditionally conjugate models. . Reference . ICML 2018 tutorial:Variational Bayes and Beyond: Bayesian Inference for Big Data. | Shakir Mohamed:Variational Inference for Machine Learning. | DS3 workshop:Approximate Bayesian Inference: Old and New. | Variational Inference and Deep Generative Models:Variational Inference for NLP audiences |",
            "url": "https://sambaiga.github.io/sambaiga/statistical%20inference/generative%20models/machine%20learning/2019/03/02/bayes-vi.html",
            "relUrl": "/statistical%20inference/generative%20models/machine%20learning/2019/03/02/bayes-vi.html",
            "date": " ‚Ä¢ Mar 2, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Learning from probabilistic models",
            "content": "Introduction . Given some data $ mathbf{x}=[x_1 ldots x_m]$ that come from some probability density function characterized by an unknown parameter $ theta$. How can we find $ hat{ theta}$ that is the best estimator of $ theta$. For example suppose we have flipped a particular coin $ 100$ times and landed head $ N_H = 55$ times and tails $ N_T = 45$ times. We are interested to know what is the probability that it will come-up head if we flip it again. In this case the behavior of the coin can be summerized with parameter $ theta$ the probability that a flip land head (H), which in this case is independent and identically distributed Bernoulli distribution. The key question is, how do we find parameter $ hat{ theta}$ of this distribution that fits the data. This is called parameter estimation, in which three approaches can be used: . Maximum-Likehood estimation | Bayesian parameter estimation and | Maximum a-posterior approximation | Like-hood and log-likehood function . Before discussing the above learning approach, let firts define the like-hood function $L( theta)$ which is the probability of the observed data as function of $ theta$ given as: . $$ L( theta) = P(x_1, ldots x_m; theta) = prod_i^m P(x_i; theta) $$The like-hood function indicates how likely each value of the parameter is to have generated the data. In the case of coin example above, the like-lihood is the probability of particular seqeuence of H and T generated: . $$ L( theta) = theta ^{N_H}(1 - theta ^{N_T}) $$ . We also define the log-likelihood function $ mathcal{L}( theta)$ which is the log of the likelihood function $L( theta)$. . $$ begin{aligned} mathcal{L}( theta) &amp;= log L( theta) &amp; = log prod_i^m P(x_i; theta) &amp; = sum_i^M P(x_i; theta) end{aligned} $$For the above coin example the log-likelihood is . $$ mathcal{L}( theta)= N_H log theta + N_T log(1- theta) $$ Maximum-Likelihood Estimation . The main objective of maximum likelihood estimation (MLE) is to determine the value of $ theta$ that is most likely to have generated the vector of observed data, $ mathbf{x}$ where $ theta$ is assumed to be fixed point (point-estimation). MLE achieve this by finding the parameter that maximize the probability of the observed data. The parameter $ hat{ theta}$ is selected such that it maximize $ mathcal{L}( theta)$: . $ hat{ theta}= arg max_{ theta} mathcal{L}( theta) $ . For the coin example the MLE is : . $$ begin{aligned} frac{ partial mathcal{L}( theta)}{ partial theta} &amp; = frac{ partial }{ partial theta}(N_H log theta + N_T log(1- theta) &amp;= frac{N_H}{ theta} - frac{N_T}{1- theta} end{aligned} $$Set $ frac{ partial mathcal{L}( theta)}{ partial theta} = 0$ and solve for $ theta$ we obtain the MLE: . $ hat{ theta} = frac{N_H}{N_H + N_T} $ . which is simply the fraction of flips that cameup head. . Now suppose we are observing power-meta data which can be modelled as gaussian ditribution with mean $$ mu$$ and standard deviation $ sigma$. We can use MLE to estimate $ hat{ mu}$ and $ hat{ sigma}$. The log-likehood for gausian distribution is given as . $$ begin{aligned} mathcal{L}( theta) &amp;= sum_{i=1}^M log left[ frac{1}{ sqrt{2} pi sigma} exp frac{-(x_i - mu)}{2 sigma ^2} right] &amp; = - frac{M}{2} log 2 pi - M log sigma - frac{1}{2 sigma^2} sum_i^M (x_i - mu)^2 end{aligned} $$Let find $ frac{ partial mathcal{L}( theta)}{ partial mu} $ and $ frac{ partial mathcal{L}( theta)}{ partial sigma} $ and set equal to zero. . $$ begin{aligned} frac{ partial mathcal{L}( theta)}{ partial mu} &amp;= - frac{1}{2 sigma^2} sum_i^M frac{ partial}{ partial mu}(x_i - mu)^2 &amp; = sum_i^M (x_i - mu) = 0 &amp; Rightarrow hat{ mu} = frac{1}{M} sum_{i=1}^M x_i end{aligned} $$which is the mean of the observed values. Similary: . $$ begin{aligned} frac{ partial mathcal{L}( theta)}{ partial sigma} &amp;= frac{M}{ sigma} + frac{1}{ sigma^3} sum_i^M (x_i - mu)^2 &amp; Rightarrow hat{ sigma} = sqrt{ frac{1}{M} sum_{i=1}^M (x_i - mu)^2} end{aligned} $$In the two examples above we manged to obtain the exact maximum likelihood solution analytically. But this is not always the case, let‚Äôs consider how to compute the maximum likelihood estimate of the parameters of the gamma distribution, whose PDF is defined as: . $$ P(x) = frac{b^a}{ Gamma(a)}x^{x-1} exp(-bx) $$where $ Gamma (a)$ is the gamma function which is the generalization of the factorial function to continous values given as: . $ Gamma(t) = int_0^{- infty} x^{t-1} exp(-x) ,dx $ . The model parameters for gamma distribution is $a$ and $b$ both of which are $ geq 0$. the log-likelihood is therefore: . $ begin{aligned} mathcal{L}( a, b) &amp; = sum_{i=1}^M a log b - log Gamma (a) + (a -1) log x_i - bx_i &amp; = Ma log b - M log Gamma (a) + (a - 1) sum_{i=1}^M log x_i - b sum_{i=1}^M x_i end{aligned} $ . To get MLE we need employ gradient descent which consists of computing the derivatives: $ frac{ partial mathcal{L}}{ partial a} $ and $ frac{ partial mathcal{L}}{ partial b} $ and then updating; $ a_{k+1}= a_k + alpha frac{ partial mathcal{L}}{ partial a} $ and $ b_{k+1}= b_k + alpha frac{ partial mathcal{L}}{ partial b} $ . where $ alpha$ is the learning rate. . Limitation of MLE . Despite the fact that MLE is very powerful technique, it has a pitfall for little training data which can lead into seriously overfit. The most painful issue is when it assign a $0$ probability to items that were never seen in the training data but which still might actually happen. Take an example if we flipped a coin twice and $N_H = 2$, the MLE of $ theta$, the probability of H would be $1$. This imply that we are considering it impossible for the coin to come up T. This problem is knowas data sparsity. . Bayesian Parameter Estimation . Unlike MLE which treat only the observation $ mathbf{x}$ as random variable and the parameter $ theta$ as a fixed point, the bayesian approach treat the parameter $ theta $ as random varibale as well with some known prior distribution. Let define the model for joint distribution $$p( theta, mathcal{D})$$ over parameter $ theta$ and data $ mathcal{D}$. To further define this joint distribution we aslo need the following two distribution: . A distribution of $P( theta)$ knowas prior distribution which is the probability of paratemeter $ theta$ availabe beforehand, and before making any additional observations. It account for everything you believed about the parameter $ theta$ before observing the data. In practise choose prior that is computational convinient. . | The likelihood $P( mathcal{D} mid theta)$ which is the probability of data given the parameter like in maximum likelihood. . | . With this two distributions, we can compute the posterior distribution and the posterior predictive distribution. The posterior distribution $P( theta mid mathcal{D})$ which correspond to uncertainty about $ theta$ after observing the data given by: . $$ begin{aligned} P( theta mid mathcal{D}) &amp;= frac{P( theta)p( mathcal{D} mid theta)}{P( mathcal{D})} &amp;= frac{P( theta)P( mathcal{D} mid theta)}{ displaystyle int P( theta ^ { prime} ) P( mathcal{D} mid theta ^{ prime})} end{aligned} $$The denominator is usually considered as a normalizing constant and thus the posterior distribution become: . $$ P( theta mid mathcal{D}) propto P( theta)P( mathcal{D} mid theta) $$On the other hand the posterior predictive distribution $P( mathcal{D}^{ prime} mid) mathcal{D}$ is the distribution of future observation given past observation defined by: . $$ P( mathcal{D}^{ prime} mid mathcal{D} )= int P( theta mid mathcal{D}) P( mathcal{D}^{ prime} mid theta) $$Generaly the Bayesian approach to parameter estimation works as follows: . First we need to formulate our knowledge about a situation by defining a distribution model which expresses qualitative aspects of our knowledge about the situation and then specify a prior probability distribution which expresses our subjective beliefs and subjective uncertainty about the unknown parameters, before seeing the data. | Gather data | Obtain posterior knowledge that updates our beliefs by computing the posterior probability distribution which estimates the unknown parameters. | Let apply the bayesian estimation to the coin example in which we have specified the likelihood equal to $ theta^{N_H}(1- theta)^{N_T}$. We only required to specify the prior in which several approches can be used. One of the approach is relay upon lifetime experince of flipping coins in which most coins tend to be fair which implies $p( theta) = 0.5$. We can also use various distribution to specify prior density but in practise a most useful distribution is the beta distribution parameterized by $a , b &gt; 0$ and defined as: . $$ p( theta; a, b) = frac{ Gamma (a + b)}{ Gamma(a) Gamma (b)} theta ^{a-1}(1- theta ^{b - 1}) $$From the above eqution it is clear that the first term (with all $ Gamma$)is just a normalizing constant and thus we can rewrite the beta distribution as: . $$ p( theta; a, b) propto theta ^{a-1}(1- theta) ^{b - 1} $$Note the beta distribution has the following properties . It is centered around $ frac{a}{a + b}$ and it can be shown that if $ theta sim text{Beta}(a,b)$ then $ mathbb{E}( theta)= frac{a}{a + b}$. | It becomes more peaked for larger values of $a$ and $b$ | It become normal distribution when $a = b = 1$ | . Now let compute the posterior and posterior predictive distribution . $$ begin{aligned} p( theta | mathcal{D}) &amp; propto p( theta)p( mathcal{D} | theta) &amp; propto theta^{N_H}(1- theta)^{N_T} theta ^{a-1}(1- theta) ^{b - 1} &amp; = theta ^{a-1+N_H}(1- theta) ^{b - 1 + N_T} end{aligned} $$",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/data%20science/probability/2018/04/02/probabilities-learning.html",
            "relUrl": "/machine%20learning/data%20science/probability/2018/04/02/probabilities-learning.html",
            "date": " ‚Ä¢ Apr 2, 2018"
        }
        
    
  
    
        ,"post10": {
            "title": "Basics of Probability and Information Theory",
            "content": "Introduction . Probability and Information theory are important field that has made significant contribution to deep learning and AI. Probability theory allows us to make uncertain statements and to reason in the presence of uncertainty where information theory enables us to quantify the amount of uncertainty in a probability distribution. . Probability Theory . Probability is a mathematical framework for representing uncertainty. It is very applicable in Machine learning and Artificial Intelligence as it allows to make uncertain statements and reason in the presence of uncertainty. Probability theory allow us to design ML algorithms that take into consideration of uncertain and sometimes stochastic quantities. It further tell us tell us how ML systems should reason in the presence of uncertainty. This is necessary because most things in the world are uncertain, and thus ML systems should reason using probabilistic rules. Probability theory can also be used to analyse the behaviour of ML algorithms probabilistically. Consider evaluating ML classification algorithm using accuracy metric which is the probability that the model will give a correct prediction given an example. . Probability and Probability distribution . Probability is a measure of the likelihood that an event will occur in a random experiment. It is quantified as number between 0 and 1. The mathematical function that maps all possible outcome of a random experiment with its associated probability it is called probability distribution. It describe how likely a random variable or set of random variable is to take on each of its possible state. The probability distribution for discrete random variable is called probability mass function (PMF) which measures the probability $X$ takes on the value $x$, denoted denoted as $P(X=x)$. To be PMF on random variable $X$ a function $P(X)$ must satisfy: . Domain of $P$ equal to all possible states of $X$ | $ forall x in X, 0 leq P(X=x) leq 1$ | $ sum_{x in X} P(x) =1$ | . Popular and useful PMF includes poison, binomial, bernouli, and uniform. Let consider a poison distribution defined as: . $$ P(X=x) = frac{ lambda ^x e^{ - lambda}}{x!} $$$ lambda &gt;0$ is called a parameter of the distribution, and it controls the distribution&#39;s shape. By increasing $ lambda$ , we add more probability to larger values, and conversely by decreasing $ lambda$ we add more probability to smaller values as shown in figure below. . Instead of a PMF, a continuous random variable has a probability density function (pdf) denoted as $f_X(x)$. An example of continuous random variable is a random variable with exponential density. $$ f_X(x mid lambda) = lambda ^x e^{ - lambda} text{, } x geq 0 $$ . To be a probability density function $p(x)$ must satisfy . The domain of $p$ must be the set of all possible state | $ forall x in X, f_X(x) geq 0$ | $ int_{x in X} f_X(x)dx =1$ | . The pdf does not give the probability of a specific state directly. The probability that $x$ is between two point $a, b$ is . $ int_{a}^b f_X(x)dx$ . The probability of intersection of two or more random variables is called joint probability denoted as $ P(X, Y) $ . Suppose we have two random variable $X$ and $Y$ and we know the joint PMF or pdf distribution between these variable. The PMF or pdf corresponding to a single variable is called marginal probability distribution defined as $$ P(x) = sum_{y in Y} P(x, y) $$ . for discrete random variable and $$ p(x) = int p(x)dy $$ . Marginalization allows us to get the distribution of variable $$X$$ ignoring variable $Y$ from the joint distribution $P(X,Y)$. The probability that some event will occur given we know other events is called condition probability denoted as $P(X mid Y)$. The marginal, joint and conditional probability are linked by the following rule $ P(X|Y) = frac{P(X, Y)}{P(Y)} $ . Independence, Conditional Independence and Chain Rule . Two random variables are said to be independent of each other if the probability that one random variables occur in no way affect the probability of the other random variable occurring. $X$ and $Y$ are said to be independent if $P(X,Y) = P(X) cdot P(Y)$ On the other hand two random variable $X$ and $Y$ are conditionally independent given an event $Z$ with $P(Z)&gt;0$ if . $$ P(X,Y mid Z) = P(X mid Y) cdot P(Y mid Z) $$The good example of conditional independence can be found on this link. Any joint probability distribution over many random variables may be decomposed into conditional distributions using chain rule as follows: . $$ P(X_1,X_2, ldots, X_n ) = P(X_1) prod_{i=2}^n P(X_i mid X_i, ldots X_{i-1}) $$Expectation, Variance and Covariance . Expected value of some function $f(x)$ with respect to a probability distribution $P(X)$ is the average or mean value that $f(x)$ takes on when $x$ is drawn from $P$. . $$ mathbb{E}_{x sim P}[f(x)] = sum P(x).f(x) $$for discrete random variable and . $$ mathbb{E}_{x sim P}[f(x)] = int P(x).f(x)dx $$Expectation are linear such that $$ mathbb{E}_{x sim P}[ alpha cdot f(x) + beta cdot g(x)] = alpha mathbb{E}_{x sim P}[f(x)] + beta mathbb{E}_{x sim P}[g(x)] $$ . Variance is a measure of how much the value of a function of random variable $X$ vary as we sample different value of $x$ from its probability distribution. $$ Var(f(x)) = mathbb{E}([f(x)- mathbb{E}[f(x)]^2]) $$ The square root of the variance is know as standard deviation. On the other hand the covarince give some sense of how much two value are linearly related to each other as well as the scale of these value. . $$ Cov(f(x), g(y)) = mathbb{E}[(f(x)- mathbb{E}[f(x)])(g(y)- mathbb{E}[g(y)])] $$ Information theory . Information theory deals with quantification of how much information is present in a signal. In context of machine learning, information theory we apply information theory to: characterize probability distributions and quantify similarities between probability distributions. The following are the key information concepts and their application to machine learning. . Entropy, Cross Entropy and Mutual information . Entropy give measure of uncertainty in a random experiment. It help us quantify the amount of uncertainty in an entire probability distribution. The entropy of a probability distribution is the expected amount of information in an event drawn from that distribution defined as. . $$ H(X) = - mathbb{E}_{x sim P}[ log P(x)] = - sum_{i=1}^n P(x_i)l log P(x_i) $$Entropy is widely used in model selection based on principle of maximum entropy. On the other hand, cross entropy is used to compare two probability distribution. It tell how similar two distribution are. The cross entropy between two probability distribution $P$ and $$Q$ defined over same set of outcome is given by . $$ H(P,Q)= - sum P(x) log Q(x) $$ . Cross entropy loss function is widely used in machine learning for classification problem. The mutual information over two random variables help us gain insight about the information that one random variable carries about the other. . $$ begin{aligned} I(X, Y) &amp;= sum P(x, y) log frac{P(x,y)}{P(x).P(y)} &amp;=H(X)- H(X mid Y) = H(Y) - H(Y mid X) end{aligned} $$From above equation the mutual information give insight about how far $X$ and $Y$ from being independent from each other. Mutual information can be used in feature selection instead of correlation as it capture both linear and non linear dependency. . Kullback-leibler Divergence . Kullback-leibler Divergence measure how one probability distribution diverge from the other. Given two probability distribution $P(x)$ and $Q(X)$ where the former is the modelled/estimated distribution and the later is the actual/expected distribution. The KL divergence is defined as . $$ begin{aligned} D_{KL}(P||Q) &amp; = mathbb{E}_{x sim P} [ log frac{P(x)}{Q(x)}] &amp; = mathbb{E}_{x sim P}[ log P(x)] - mathbb{E}_{x sim P}[ log Q(x)] end{aligned} $$For discrete random distribution . $$ D_{KL}(P||Q) = sum_{i} P(x_i) log frac{P(x_i)}{Q(x_i)} $$And for continuous random variable . $$ D_{KL}(p||q) = int_{x} p(x) log frac{p(x)}{q(x)} $$KL divergence between $P$ and $Q$ tells how much information we lose when trying to approximate data given by $P$ with $Q$. It is non-negative $D_{KL}(P mid mid Q) geq 0$ and $0$ if $P$ and $Q$ are the same (distribution discrete) or equal almost anywhere in the case of continuous distribution. Apart from that KL divergence is not symmetric $D_{KL}(P mid mid Q) neq D_{KL}(P mid mid Q)$ because of this it is not a true distance measure. . Relation between KL divergence and Cross Entropy . $$ begin{aligned} D_{KL}(P||Q) &amp; = mathbb{E}_{x sim P} [ log frac{P(x)}{Q(x)}] &amp; = mathbb{E}_{x sim P}[ log P(x)] - mathbb{E}_{x sim P}[ log Q(x)] &amp; = H(P) - H(P, Q) end{aligned} $$where $ mathbb{E}_{x sim P}[ log P(x)] = H(P)$$ and $$ mathbb{E}_{x sim P}[ log Q(x)] = H(P, Q)$. Thus $H(P,Q) = H(P) - D_{KL}(P||Q)$. This implies that minimizing cross entropy with respect to $Q$ is equivalent to minimizing the KL divergence. KL divergence is used in unsupervised machine learning technique like variational auto-encoder. The KL divergence is also used as objective function in variational bayesian method to find optimal value for approximating distribution. .",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/data%20science/2018/02/01/probabilities.html",
            "relUrl": "/machine%20learning/data%20science/2018/02/01/probabilities.html",
            "date": " ‚Ä¢ Feb 1, 2018"
        }
        
    
  
    
        ,"post11": {
            "title": "Mixture Density Networks.",
            "content": "Introduction . Deep Learning models are widely used in prediction problem which involves learning the mapping from a set of inputs variables x={x1,‚Ä¶,xd} mathbf{x}= {x_1, ldots, x_d }x={x1‚Äã,‚Ä¶,xd‚Äã} to a set of output variables y={y1,‚Ä¶,yc} mathbf{y}= {y_1, ldots,y_c }y={y1‚Äã,‚Ä¶,yc‚Äã}. In this setting, ddd is the size of input features, and ccc is the dimension of the output feature or target. In this case, usually the network is trained using minimization of the sum of squares errors or cross-entropy error function over a set of training data {x1:N,y1:N} { mathbf{x}_{1:N}, mathbf{y}_{1:N} }{x1:N‚Äã,y1:N‚Äã} of the form . L=(y‚àíy^)2¬†where¬†y^1:c=f(x1:d,w,b) mathcal{L} = ( mathbf{y}- hat mathbf{y})^2 text{ where } hat mathbf{y}_{1:c}=f( mathbf{x}_{1:d}, mathbf{w, b})L=(y‚àíy^‚Äã)2¬†where¬†y^‚Äã1:c‚Äã=f(x1:d‚Äã,w,b) . With this approach it is explicitly assumed that there is a deterministic 1‚àíto‚àí11-to-11‚àíto‚àí1 mapping between a given input variables x={x1,‚Ä¶,xd} mathbf{x}= {x_1, ldots, x_d }x={x1‚Äã,‚Ä¶,xd‚Äã} and target variable y={y1,‚Ä¶,yc} mathbf{y}= {y_1, ldots,y_c }y={y1‚Äã,‚Ä¶,yc‚Äã} without any uncertainty. As the result, the output of the network trained by this approach approximates the conditional mean of the output in the training data conditioned on the input vector. For classification problems with a well-chosen target coding scheme, these averages represent the posterior probability of class membership and thus be regarded as optimal. For a problem involving the prediction of a continuous variable, especially the conditional averages is not usually a good description of data and don‚Äôt have power to the modal distribution of output with complex. One way to solve this problem is to model the complete conditional probability density instead. This is the approach used by Mixture Density Networks (MDN). . Mixture Density Network . An MDN, as proposed by Bishop, is a flexible framework for modeling an arbitrary conditional probability distribution p(y‚à£x)p( mathbf{y}| mathbf{x})p(y‚à£x) as a mixture of distributions. It combines a mixture model with DNN in which a DNN is used to parametrize a mixture model consisting of some predefined distributions. Considering gaussian distribution, DNN is used to map a set of input features x1:d mathbf{x}_{1:d}x1:d‚Äã to the parameters of a GMM i.e mixture weights œÄk(x) pi_k( mathbf{x})œÄk‚Äã(x), mean Œºk(x) mu _k( mathbf{x})Œºk‚Äã(x) and the covariance matrices œÉk2(x) sigma_k^2( mathbf{x})œÉk2‚Äã(x) which in turn gives a full probability density function of an output feature y mathbf{y}y conditioned on the input features. . p(y‚à£x)=‚àëk=1MœÄk(x)N(y;Œºk(x),œÉk2(x))p( mathbf{y}| mathbf{x})= sum_{k=1}^M pi_k( mathbf{x}) mathcal{N}( mathbf{y}; mu_k( mathbf{x}), sigma_k^2( mathbf{x}))p(y‚à£x)=k=1‚àëM‚ÄãœÄk‚Äã(x)N(y;Œºk‚Äã(x),œÉk2‚Äã(x)) . where MMM is the number of components in the mixture and . N(y;Œºk(x),œÉk2(x))=1(2œÉk2(x))c/2exp‚Å°[‚à£‚à£y‚àíŒºk(x)‚à£‚à£22œÉk2(x)] mathcal{N}( mathbf{y}; mu_k( mathbf{x}), sigma_k^2( mathbf{x})) = frac{1}{(2 sigma_k^2( mathbf{x}))^{c/2}} exp left[ frac{|| mathbf{y}- mu_k( mathbf{x})||^2}{2 sigma_k^2( mathbf{x})} right]N(y;Œºk‚Äã(x),œÉk2‚Äã(x))=(2œÉk2‚Äã(x))c/21‚Äãexp[2œÉk2‚Äã(x)‚à£‚à£y‚àíŒºk‚Äã(x)‚à£‚à£2‚Äã] . The mixture weights œÄk(x) pi_k( mathbf{x})œÄk‚Äã(x) represents the relative amounts by of each mixture components, which can be interpreted as the probabilities of the k‚àík-k‚àí components for a given observation x mathbf{x}x.If we introduce a latent variable z mathbf{z}z with kkk possible states, then œÄk(x) pi_k( mathbf{x})œÄk‚Äã(x) will represents the probability distribution of these states p(z)p( mathbf{z})p(z). Specifically, the MDN converts the input vector using DNN with an output layer z mathbf{z}z of linear units to obtain the output z^=f(x,Œ∏) hat{ mathbf{z}} = f( mathbf{x}, mathbf{ theta})z^=f(x,Œ∏) . The total number of networks outputs i.e the dimension of z^¬†is¬†(c+2)‚ãÖM hat{ mathbf{z}} text{ is } (c+2) cdot Mz^¬†is¬†(c+2)‚ãÖM compared to the usual ccc outputs for a network used in the conventional manner. In order to guarantee that p(y‚à£x)p( mathbf{y}| mathbf{x})p(y‚à£x) is a probability distribution, the outputs of the networks need to be constrained such that the variance should remain positive and the mixing coefficients lie between zero and one and sum to one. To achieve these constraints: . The mean of the k‚àíthk-thk‚àíth kernel is modeled directly as the network outputs: | . Œºki(x)=zkŒºi¬†where¬†i=1,‚Ä¶,c mu_{k}^i( mathbf{x})=z_{k}^{ mu i} text{ where } i = 1, ldots, cŒºki‚Äã(x)=zkŒºi‚Äã¬†where¬†i=1,‚Ä¶,c . The variances of $$ sigma_k $ is represented by an exponential activation function of the corresponding network output. | . œÉk(x)=exp‚Å°(zkœÉ) sigma_k( mathbf{x}) = exp(z_k^{ sigma})œÉk‚Äã(x)=exp(zkœÉ‚Äã) . The mixing coefficient œÄk(x) pi _k( mathbf{x})œÄk‚Äã(x) is modeled as the softmax transformation of the corresponding output. | . œÄk=exp‚Å°(zkœÄ)‚àëj=1Mexp‚Å°(zjœÄ) pi_k = frac{ exp(z_k^{ pi})}{ sum_{j=1}^M exp(z_j^{ pi})}œÄk‚Äã=‚àëj=1M‚Äãexp(zjœÄ‚Äã)exp(zkœÄ‚Äã)‚Äã . Training MDN . As the generative model, an MDN model can be trained using the backpropagation algorithm under the maximum likelihood criterion. Suppose Œ∏ thetaŒ∏ is the vector of the trainable parameter, and we can redefine our model as a function of x mathbf{x}x parameterized by Œ∏ thetaŒ∏ . p(y‚à£x,Œ∏)=‚àëk=1MœÄk(x,Œ∏)N(y;Œºk(x,Œ∏),œÉk2(x,Œ∏))p( mathbf{y}| mathbf{x}, mathbf{ theta})= sum_{k=1}^M pi_k( mathbf{x}, mathbf{ theta}) mathcal{N}( mathbf{y}; mu_k( mathbf{x}, mathbf{ theta}), sigma_k^2( mathbf{x}, mathbf{ theta}))p(y‚à£x,Œ∏)=k=1‚àëM‚ÄãœÄk‚Äã(x,Œ∏)N(y;Œºk‚Äã(x,Œ∏),œÉk2‚Äã(x,Œ∏)) . Considering a data set D={x1:N,y1:N} mathcal{D}= { mathbf{x}_{1:N}, mathbf{y}_{1:N} }D={x1:N‚Äã,y1:N‚Äã} we want to maximize . p(Œ∏‚à£D)=p(Œ∏‚à£Y,X)p( mathbf{ theta}| mathcal{D}) = p( mathbf{ theta}| mathbf{Y}, mathbf{X})p(Œ∏‚à£D)=p(Œ∏‚à£Y,X) . By Bayes‚Äôs theorem, this is equivalent to . p(Œ∏‚à£Y,X)p(Y)=p(Y,Œ∏‚à£X)=p(Y‚à£X,Œ∏)p(Œ∏)p( mathbf{ theta}| mathbf{Y}, mathbf{X})p( mathbf{Y}) = p( mathbf{Y}, mathbf{ theta} | mathbf{X}) = p( mathbf{Y}| mathbf{X}, mathbf{ theta})p( mathbf{ theta})p(Œ∏‚à£Y,X)p(Y)=p(Y,Œ∏‚à£X)=p(Y‚à£X,Œ∏)p(Œ∏) . which leads to . p(Œ∏‚à£Y,X)=p(Y‚à£X,Œ∏)p(Œ∏)p(Y)‚àùp(Y‚à£X,Œ∏)p(Œ∏)p( mathbf{ theta}| mathbf{Y}, mathbf{X}) = frac{p( mathbf{Y}| mathbf{X}, mathbf{ theta})p( mathbf{ theta})}{p( mathbf{Y})} propto p( mathbf{Y}| mathbf{X}, mathbf{ theta})p( mathbf{ theta})p(Œ∏‚à£Y,X)=p(Y)p(Y‚à£X,Œ∏)p(Œ∏)‚Äã‚àùp(Y‚à£X,Œ∏)p(Œ∏) where p(Y‚à£X,Œ∏)=‚àèn=1Np(yn‚à£xn,Œ∏)p( mathbf{Y}| mathbf{X}, mathbf{ theta})= prod_{n=1}^N p( mathbf{y}_n| mathbf{x}_n, mathbf{ theta})p(Y‚à£X,Œ∏)=‚àèn=1N‚Äãp(yn‚Äã‚à£xn‚Äã,Œ∏) which is simply the product of the conditional densities for each pattern. . To define an error function, the standard approach is the maximum likelihood method, which requires maximisation of the log-likelihood function or, equivalently, minimisation of the negative logarithm of the likelihood. Therefore, the error function for the Mixture Density Network is: . E(Œ∏,D)=‚àílog‚Å°p(Œ∏‚à£Y,X)=‚àílog‚Å°p(Y‚à£X,Œ∏)p(Œ∏)=‚àí(log‚Å°‚àèn=1Np(yn‚à£xn,Œ∏)+log‚Å°p(Œ∏))=‚àí(‚àën=1Nlog‚Å°‚àëk=1MœÄk(x)N(y;Œºk(x),œÉk2(x))+log‚Å°p(Œ∏)) begin{aligned} E( theta, mathcal{D})&amp;=- log p( mathbf{ theta}| mathbf{Y}, mathbf{X})= - log p( mathbf{Y}| mathbf{X}, mathbf{ theta})p( mathbf{ theta}) &amp;= - left( log prod_{n=1}^N p( mathbf{y}_n| mathbf{x}_n, mathbf{ theta}) + log p( mathbf{ theta}) right) &amp;=- left( sum_{n=1}^N log sum_{k=1}^M pi_k( mathbf{x}) mathcal{N}( mathbf{y}; mu_k( mathbf{x}), sigma_k^2( mathbf{x})) + log p( mathbf{ theta}) right) end{aligned}E(Œ∏,D)‚Äã=‚àílogp(Œ∏‚à£Y,X)=‚àílogp(Y‚à£X,Œ∏)p(Œ∏)=‚àí(logn=1‚àèN‚Äãp(yn‚Äã‚à£xn‚Äã,Œ∏)+logp(Œ∏))=‚àí(n=1‚àëN‚Äãlogk=1‚àëM‚ÄãœÄk‚Äã(x)N(y;Œºk‚Äã(x),œÉk2‚Äã(x))+logp(Œ∏))‚Äã . If we assume a non-informative prior of p(Œ∏)=1p( mathbf{ theta})=1p(Œ∏)=1 the error function simplify to . E(Œ∏,D)=‚àí‚àën=1Nlog‚Å°‚àëk=1MœÄk(x)N(y;Œºk(x),œÉk2(x))E( theta, mathcal{D}) = - sum_{n=1}^N log sum_{k=1}^M pi_k( mathbf{x}) mathcal{N}( mathbf{y}; mu_k( mathbf{x}), sigma_k^2( mathbf{x}))E(Œ∏,D)=‚àín=1‚àëN‚Äãlogk=1‚àëM‚ÄãœÄk‚Äã(x)N(y;Œºk‚Äã(x),œÉk2‚Äã(x)) .",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/2018/01/03/mdn.html",
            "relUrl": "/machine%20learning/2018/01/03/mdn.html",
            "date": " ‚Ä¢ Jan 3, 2018"
        }
        
    
  
    
        ,"post12": {
            "title": "Introduction to Machine Learning - Classification.",
            "content": "Introduction . Previously we learned how to predict continuous-valued quantities as a linear function of input values. This post will describe a classification problem where the goal is to learn a mapping from inputs $x$ to target $t$ such that $t in {1 ldots C }$ with $C$ being the number of classes.If $C = 2$, this is called binary classification (in which case we often assume $y in {0, 1}$; if $C &gt; 2$, this is called multiclass classification. . We will first consider binary classification problem in which the target classes $t$ will be generated from 2 class distributions: blue ($t=1$) and red ($t=0$). Samples from both classes are sampled from their respective distributions. These samples are plotted in the figure below. . Note that $X$ is a $N times 2$ matrix of individual input samples $ mathbf{x}_i$, and that $ mathbf{t}$ is a corresponding $N times 1$ vector of target values $t_i$. . Logistic Regression . With logistic regression the goal is to predict the target class $t$ from the input values $x$. The network is defined as having an input $ mathbf{x} = [x_1, x_2]$ which gets transformed by the weights $ mathbf{w} = [w_1, w_2]$ to generate the probability that sample $ mathbf{x}$ belongs to class $t=1$$ This probability $P(t=1 mid mathbf{x}, mathbf{w})$ is represented by the output $y$ of the network computed as $y = sigma( mathbf{x} * mathbf{w}^T)$. $ sigma$ is the logistic function and is defined as: . œÉ(z)=11+e‚àíz sigma(z) = frac{1}{1+e^{-z}}œÉ(z)=1+e‚àíz1‚Äã . which squashes the predictions to be between 0 and 1 such that: . P(t=1‚à£x,w)=y(œÉ(z))P(t=0‚à£x,w)=1‚àíP(t=1‚à£x,w)=1‚àíy(œÉ(z)) begin{aligned} P(t=1| mathbf{x}, mathbf{w}) &amp;= y( sigma(z))P(t=0 mid mathbf{x}, mathbf{w}) &amp;= 1 - P(t=1 mid mathbf{x}, mathbf{w}) = 1 - y( sigma(z)) end{aligned}P(t=1‚à£x,w)‚Äã=y(œÉ(z))P(t=0‚à£x,w)=1‚àíP(t=1‚à£x,w)=1‚àíy(œÉ(z))‚Äã . The loss function for logistic function is called crossentropy and defined as: . LCE(y,t)={‚àílog‚Å°yif¬†t=1‚àílog‚Å°(1‚àíy)if¬†t=0 mathcal{L}_{CE}(y,t)= begin{cases} - log y quad text{if } t = 1 - log (1-y) quad text{if } t = 0 end{cases}LCE‚Äã(y,t)={‚àílogyif¬†t=1‚àílog(1‚àíy)if¬†t=0‚Äã . The crossentropy can be written in other form as: . LCE(y,t)=‚àítlog‚Å°y‚àí(1‚àít)log‚Å°(1‚àíy) mathcal{L}_{CE}(y,t)= -t log y -(1-t) log(1-y)LCE‚Äã(y,t)=‚àítlogy‚àí(1‚àít)log(1‚àíy) . When we combine the logistic activation function with cross-entropy loss, we get logistic regression: . z=wTx+b¬†y=œÉ(z)¬†LCE(y,t)=‚àítlog‚Å°y‚àí(1‚àít)log‚Å°(1‚àíy) begin{aligned} z &amp; = mathbf{w^Tx + b} y &amp; = sigma(z) mathcal{L}_{CE}(y,t) &amp;= -t log y -(1-t) log(1-y) end{aligned}z¬†y¬†LCE‚Äã(y,t)‚Äã=wTx+b=œÉ(z)=‚àítlogy‚àí(1‚àít)log(1‚àíy)‚Äã . The cost function with respect to the model parameters Œ∏ thetaŒ∏ (i.e. the weights and bias) is therefore: . ŒµŒ∏=1N‚àëi=1NLCE(y,t)¬†=1N‚àëi=1N(‚àít(i)log‚Å°y(i)‚àí(1‚àít(i))log‚Å°(1‚àíy(i))) begin{aligned} varepsilon_{ theta} &amp; = frac{1}{N} sum_{i=1}^N mathcal{L}_{CE}(y,t) &amp; = frac{1}{N} sum_{i=1}^N left(-t^{(i)} log y^{(i)} -(1-t^{(i)}) log(1-y^{(i)}) right) end{aligned}ŒµŒ∏‚Äã¬†‚Äã=N1‚Äãi=1‚àëN‚ÄãLCE‚Äã(y,t)=N1‚Äãi=1‚àëN‚Äã(‚àít(i)logy(i)‚àí(1‚àít(i))log(1‚àíy(i)))‚Äã . which can be implemented in python as follows: . # Define the cost function def cost(x, w, t): N, D = np.shape(x) z = z_value(x,w) y = y_value(z) result = np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))/float(N) return -result . Gradient Descent for Logistic Function . To derive the gradient descent updates, we‚Äôll need the partial derivatives of the cost function. We‚Äôll do this by applying the Chain Rule twice: first to compute ‚àÇLCE‚àÇz frac{ partial mathcal{L}_{CE}}{ partial z}‚àÇz‚àÇLCE‚Äã‚Äã and then again to compute $ frac{ partial mathcal{L}_{CE}}{ partial w_j}$ But first, let‚Äôs find $ frac{ partial y}{ partial z}$. . ‚àÇy‚àÇz=e‚àíz(1+e‚àíz)2=y(1‚àíy) frac{ partial y}{ partial z} = frac{e^{-z}}{(1 + e^{-z})^2}= y(1-y)‚àÇz‚àÇy‚Äã=(1+e‚àíz)2e‚àíz‚Äã=y(1‚àíy) . Now for the Chain Rule: . ‚àÇLCE‚àÇz=‚àÇLCE‚àÇy‚àÇy‚àÇz¬†=(‚àíty+1‚àít1‚àíy)y(1‚àíy)¬†=y‚àít begin{aligned} frac{ partial mathcal{L}_{CE}}{ partial z} &amp; = frac{ partial mathcal{L}_{CE}}{ partial y} frac{ partial y}{ partial z} &amp; = left( frac{-t}{y} + frac{1-t}{1-y} right) y(1-y) &amp;= y - t end{aligned}‚àÇz‚àÇLCE‚Äã‚Äã¬†¬†‚Äã=‚àÇy‚àÇLCE‚Äã‚Äã‚àÇz‚àÇy‚Äã=(y‚àít‚Äã+1‚àíy1‚àít‚Äã)y(1‚àíy)=y‚àít‚Äã . Similary: . ‚àÇLCE‚àÇwj=‚àÇLCE‚àÇz‚àÇz‚àÇwj¬†=‚àÇLCE‚àÇzxj¬†=(y‚àít)xj begin{aligned} frac{ partial mathcal{L}_{CE}}{ partial w_j} &amp; = frac{ partial mathcal{L}_{CE}}{ partial z} frac{ partial z}{ partial w_j} &amp; = frac{ partial mathcal{L}_{CE}}{ partial z} x_j &amp;= (y - t)x_j end{aligned}‚àÇwj‚Äã‚àÇLCE‚Äã‚Äã¬†¬†‚Äã=‚àÇz‚àÇLCE‚Äã‚Äã‚àÇwj‚Äã‚àÇz‚Äã=‚àÇz‚àÇLCE‚Äã‚Äãxj‚Äã=(y‚àít)xj‚Äã‚Äã . We can also obtain ‚àÇLCE‚àÇb frac{ partial mathcal{L}_{CE}}{ partial b}‚àÇb‚àÇLCE‚Äã‚Äã as follows: . ‚àÇLCE‚àÇb=‚àÇLCE‚àÇz‚àÇz‚àÇb=(y‚àít) begin{aligned} frac{ partial mathcal{L}_{CE}}{ partial b} &amp;= frac{ partial mathcal{L}_{CE}}{ partial z} frac{ partial z}{ partial b} &amp; = (y-t) end{aligned}‚àÇb‚àÇLCE‚Äã‚Äã‚Äã=‚àÇz‚àÇLCE‚Äã‚Äã‚àÇb‚àÇz‚Äã=(y‚àít)‚Äã . The gradient descent algorithm works by taking the derivative of the cost function ŒµŒ∏ varepsilon_{ theta}ŒµŒ∏‚Äã with respect to the parameters, and updates the parameters in the direction of the negative gradient.The parameter w mathbf{w}w is iteratively updated by taking steps proportional to the negative of the gradient: . wk+1=wk‚àíŒ±‚àÇŒµ‚àÇw mathbf{w_{k+1}} = mathbf{ w_k }- alpha frac{ partial mathbf{ varepsilon}}{ partial mathbf{w}}wk+1‚Äã=wk‚Äã‚àíŒ±‚àÇw‚àÇŒµ‚Äã . where: . ‚àÇLCE‚àÇŒµ=‚àÇŒµ‚àÇLCE‚ãÖ‚àÇLCE‚àÇw=1NxT(y‚àít) begin{aligned} frac{ partial mathcal{L}_{CE}}{ partial varepsilon} &amp;= frac{ partial varepsilon }{ partial mathcal{L}_{CE}} cdot frac{ partial mathcal{L}_{CE}}{ partial mathbf{w}} &amp;= frac{1}{N} mathbf{x^T(y - t)} end{aligned}‚àÇŒµ‚àÇLCE‚Äã‚Äã‚Äã=‚àÇLCE‚Äã‚àÇŒµ‚Äã‚ãÖ‚àÇw‚àÇLCE‚Äã‚Äã=N1‚ÄãxT(y‚àít)‚Äã . which can be implemented in python as follows: . #gradient def gradient(x, w, t): z = z_value(x,w) y = y_value(z) error = y-t dw = x.T.dot(error) return dw.T def solve_gradient(x, t, alpha=0.1, tolerance=1e-2): N, D = np.shape(x) w = np.zeros([D]) iterations = 1 w_cost = [(w, cost(x,w, t))] while True: dw = gradient(x, w, t) w_k = w - alpha * dw w_cost.append((w, cost(x, w, t))) # Stopping Condition if np.sum(abs(w_k - w)) &lt; tolerance: print (&quot;Converged.&quot;) break if iterations % 100 == 0: print (&quot;Iteration: %d - cost: %.4f&quot; %(iterations, cost(x, w, t))) iterations += 1 w = w_k return w . Let us apply the above concept in the following example. Consider the case we want to predict whether a student with a specific pass mark can be admitted or not. . # load dataset admission = pd.read_csv(&#39;data/admission.csv&#39;, names = [&quot;grade1&quot;, &quot;grade2&quot;, &quot;remark&quot;]) admission.head() . The data-preprosessing is done using the following python code: . features = [&#39;grade1&#39;, &#39;grade2&#39;] target = [&#39;remark&#39;] targetVal = admission[target] featureVal = admission[features] y = np.array(targetVal) # Standardize the features for i in range(2): featureVal.iloc[:,i] = (featureVal.iloc[:,i] / featureVal.iloc[:,i].max()) # Add bias term to feature data b = np.ones((featureVal.shape[0], 1)) X = np.hstack((b, featureVal)) # randomly separate data into training and test data from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=50) . We use the solve_gradient function defined before to find the parameter for logistic regression. . w_g = solve_gradient(X_train, y_train, alpha=0.05, tolerance = 1e-9) . Now that you learned the parameters of the model, you can use the model to predict whether a particular student will be admitted. . Let define the prediction function that only 1 or 0 depending on the predicted class. . def predict(x,w): z = z_value(x,w) y = y_value(z) return np.around(y) . To find the accuracy of the model: . p_test = predict(X_test, w_g) p_train = predict(X_train, w_g) print (&#39;Test Accuracy: %f&#39; % ((y_test[np.where(p_test == y_test)].size / float(y_test.size)) * 100.0)) print (&#39;Train Accuracy: %f&#39; % ((y_train[np.where(p_train == y_train)].size / float(y_train.size)) * 100.0)) . After running the above codes, we found that our model performs a training accuracy of 91.2591.2591.25 and a test accuracy of 858585 percents. . Multiclass classification . So far, we‚Äôve talked about binary classification, but most classification problems involve more than two categories. Fortunately, this doesn‚Äôt require any new ideas: everything pretty much works by analogy with the binary case. The first question is how to represent the targets. We could describe them as integers, but it‚Äôs convenient to use indicator vectors or a one-of-K encoding. . Since there are $K$ outputs and DDD inputs, the linear function requires $K times D$ matrix as well as $K$ dimensional bias vector. We use softmax function which is the multivariate generalization given as: . yk=softmax(z1‚Ä¶zk)=ezk‚àëkezky_k = softmax(z_1 ldots z_k) = frac{e^{z_k}}{ sum_k e^{z_k}}yk‚Äã=softmax(z1‚Äã‚Ä¶zk‚Äã)=‚àëk‚Äãezk‚Äãezk‚Äã‚Äã . and can be implemented in python as . def softmax(x,w): z = z_value(x,w) e_x = np.exp(x - np.max(x)) y = np.exp(z - max(z)) / np.sum(np.exp(z - max(z))) return y.reshape(len(y), 1) . Finally, the loss function (cross-entropy) for multiple-output case can be generalized as follows: . LCE(y,t)=‚àí‚àëk=1Ktklog‚Å°yk=‚àítTlog‚Å°y begin{aligned} mathcal{L}_{CE}(y,t) &amp;= - sum_{k=1}^K t_k log y_k &amp;= - mathbf{t^T} log mathbf{y} end{aligned}LCE‚Äã(y,t)‚Äã=‚àík=1‚àëK‚Äãtk‚Äãlogyk‚Äã=‚àítTlogy‚Äã . Combining these things together, we get multiclass logistic regression: . z=wx+by=softmax(z)LCE(y,t)=‚àítTlog‚Å°y begin{aligned} mathbf{z} &amp;= mathbf{wx + b} mathbf{y} &amp;= softmax( mathbf{z}) mathcal{L}_{CE}(y,t) &amp;=- mathbf{t^T} log mathbf{y} end{aligned}zyLCE‚Äã(y,t)‚Äã=wx+b=softmax(z)=‚àítTlogy‚Äã . Gradient Descent for Multiclass Logistic Regression for Multiclass logistic regression: . Let consider the derivative with respect to the loss: . ‚àÇLCE‚àÇwkj=‚àÇ‚àÇwkj(‚àí‚àëltllog‚Å°(yl))=‚àí‚àëltlyl‚àÇyl‚àÇwkj begin{aligned} frac{ partial { mathcal L}_ text{CE}}{ partial w_{kj}} &amp;= frac{ partial }{ partial w_{kj}} left(- sum_l t_l log(y_l) right) &amp;= - sum_l frac{t_l}{y_l} frac{ partial y_l}{ partial w_{kj}} end{aligned}‚àÇwkj‚Äã‚àÇLCE‚Äã‚Äã‚Äã=‚àÇwkj‚Äã‚àÇ‚Äã(‚àíl‚àë‚Äãtl‚Äãlog(yl‚Äã))=‚àíl‚àë‚Äãyl‚Äãtl‚Äã‚Äã‚àÇwkj‚Äã‚àÇyl‚Äã‚Äã‚Äã . Normally in calculus we have the rule: . ‚àÇyl‚àÇwkj=‚àëm‚àÇyl‚àÇzm‚àÇzm‚àÇwkj begin{aligned} frac{ partial y_l}{ partial w_{kj}} &amp;= sum_m frac{ partial y_l}{ partial z_m} frac{ partial z_m}{ partial w_{kj}} end{aligned}‚àÇwkj‚Äã‚àÇyl‚Äã‚Äã‚Äã=m‚àë‚Äã‚àÇzm‚Äã‚àÇyl‚Äã‚Äã‚àÇwkj‚Äã‚àÇzm‚Äã‚Äã‚Äã . But wkjw_{kj}wkj‚Äã is independent of zmz_mzm‚Äã for m‚â†km ne kmÓÄ†‚Äã=k, so . ‚àÇyl‚àÇwkj=‚àÇyl‚àÇzk‚àÇzk‚àÇwkj begin{aligned} frac{ partial y_l}{ partial w_{kj}} &amp;= frac{ partial y_l}{ partial z_k} frac{ partial z_k}{ partial w_{kj}} end{aligned}‚àÇwkj‚Äã‚àÇyl‚Äã‚Äã‚Äã=‚àÇzk‚Äã‚àÇyl‚Äã‚Äã‚àÇwkj‚Äã‚àÇzk‚Äã‚Äã‚Äã . AND . ‚àÇzk‚àÇwkj=xj frac{ partial z_k}{ partial w_{kj}} = x_j‚àÇwkj‚Äã‚àÇzk‚Äã‚Äã=xj‚Äã . Thus . ‚àÇLCE‚àÇwkj=‚àí‚àëltlyl‚àÇyl‚àÇzk‚àÇzk‚àÇwkj=‚àí‚àëltlyl‚àÇyl‚àÇzkxj=xj(‚àí‚àëltlyl‚àÇyl‚àÇzk)=xj‚àÇLCE‚àÇzk begin{aligned} frac{ partial { mathcal L}_ text{CE}}{ partial w_{kj}} &amp;= - sum_l frac{t_l}{y_l} frac{ partial y_l}{ partial z_k} frac{ partial z_k}{ partial w_{kj}} &amp;= - sum_l frac{t_l}{y_l} frac{ partial y_l}{ partial z_k} x_j &amp;= x_j (- sum_l frac{t_l}{y_l} frac{ partial y_l}{ partial z_k}) &amp;= x_j frac{ partial { mathcal L}_ text{CE}}{ partial z_k} end{aligned}‚àÇwkj‚Äã‚àÇLCE‚Äã‚Äã‚Äã=‚àíl‚àë‚Äãyl‚Äãtl‚Äã‚Äã‚àÇzk‚Äã‚àÇyl‚Äã‚Äã‚àÇwkj‚Äã‚àÇzk‚Äã‚Äã=‚àíl‚àë‚Äãyl‚Äãtl‚Äã‚Äã‚àÇzk‚Äã‚àÇyl‚Äã‚Äãxj‚Äã=xj‚Äã(‚àíl‚àë‚Äãyl‚Äãtl‚Äã‚Äã‚àÇzk‚Äã‚àÇyl‚Äã‚Äã)=xj‚Äã‚àÇzk‚Äã‚àÇLCE‚Äã‚Äã‚Äã . Now consider derivative with respect to $z_k$ we can show (onboard) that. . ‚àÇyl‚àÇzk=yk(Ik,l‚àíyl) frac{ partial y_l}{ partial z_k} = y_k (I_{k,l} - y_l)‚àÇzk‚Äã‚àÇyl‚Äã‚Äã=yk‚Äã(Ik,l‚Äã‚àíyl‚Äã) . Where $I_{k,l} = 1$ if $k=l$ and $0$ otherwise. . Therefore . ‚àÇLCE‚àÇzk=‚àí‚àëltlyl(yk(Ik,l‚àíyl))=‚àítkykyk(1‚àíyk)‚àí‚àël‚â†ktlyl(‚àíykyl)=‚àítk(1‚àíyk)+‚àël‚â†ktlyk=‚àítk+tkyk+‚àël‚â†ktlyk=‚àítk+‚àëltlyk=‚àítk+yk‚àëltl=‚àítk+yk=yk‚àítk begin{aligned} frac{ partial { mathcal L}_ text{CE}}{ partial z_k} &amp;= - sum_l frac{t_l}{y_l} (y_k (I_{k,l} - y_l)) &amp;=- frac{t_k}{y_k} y_k(1 - y_k) - sum_{l ne k} frac{t_l}{y_l} (-y_k y_l) &amp;= - t_k(1 - y_k) + sum_{l ne k} t_l y_k &amp;= -t_k + t_k y_k + sum_{l ne k} t_l y_k &amp;= -t_k + sum_{l} t_l y_k &amp;= -t_k + y_k sum_{l} t_l &amp;= -t_k + y_k &amp;= y_k - t_k end{aligned}‚àÇzk‚Äã‚àÇLCE‚Äã‚Äã‚Äã=‚àíl‚àë‚Äãyl‚Äãtl‚Äã‚Äã(yk‚Äã(Ik,l‚Äã‚àíyl‚Äã))=‚àíyk‚Äãtk‚Äã‚Äãyk‚Äã(1‚àíyk‚Äã)‚àílÓÄ†‚Äã=k‚àë‚Äãyl‚Äãtl‚Äã‚Äã(‚àíyk‚Äãyl‚Äã)=‚àítk‚Äã(1‚àíyk‚Äã)+lÓÄ†‚Äã=k‚àë‚Äãtl‚Äãyk‚Äã=‚àítk‚Äã+tk‚Äãyk‚Äã+lÓÄ†‚Äã=k‚àë‚Äãtl‚Äãyk‚Äã=‚àítk‚Äã+l‚àë‚Äãtl‚Äãyk‚Äã=‚àítk‚Äã+yk‚Äãl‚àë‚Äãtl‚Äã=‚àítk‚Äã+yk‚Äã=yk‚Äã‚àítk‚Äã‚Äã . Putting it all together . ‚àÇLCE‚àÇwkj=xj(yk‚àítk) begin{aligned} frac{ partial { mathcal L}_ text{CE}}{ partial w_{kj}} &amp;= x_j (y_k - t_k) end{aligned}‚àÇwkj‚Äã‚àÇLCE‚Äã‚Äã‚Äã=xj‚Äã(yk‚Äã‚àítk‚Äã)‚Äã . In vectorization form it become: . ‚àÇLCE‚àÇW=(y‚àít)xT begin{aligned} frac{ partial mathcal {L}_{CE}}{ partial { mathbf W}} = ( mathbf{y} - mathbf{t}) mathbf{x}^T end{aligned}‚àÇW‚àÇLCE‚Äã‚Äã=(y‚àít)xT‚Äã . Cross-entropy cost function . The cross entropy cost function for multiclass classification is given with respect to the model parameters Œ∏ thetaŒ∏ (i.e. the weights and bias) is therefore: . ŒµŒ∏=1N‚àëi=1NLCE(y,t)=‚àí1N‚àëi=1N‚àëk=1Ktklog‚Å°yk begin{aligned} varepsilon_{ theta} &amp; = frac{1}{N} sum_{i=1}^N mathcal{L}_{CE}(y,t) &amp; = frac{-1}{N} sum_{i=1}^N sum_{k=1}^K t_k log y_k end{aligned}ŒµŒ∏‚Äã‚Äã=N1‚Äãi=1‚àëN‚ÄãLCE‚Äã(y,t)=N‚àí1‚Äãi=1‚àëN‚Äãk=1‚àëK‚Äãtk‚Äãlogyk‚Äã‚Äã . The gradient descent algorithm will be: wk+1=wk‚àíŒ±‚àÇŒµ‚àÇw mathbf{w_{k+1}} = mathbf{ w_k }- alpha frac{ partial mathbf{ varepsilon}}{ partial mathbf{w}}wk+1‚Äã=wk‚Äã‚àíŒ±‚àÇw‚àÇŒµ‚Äã . where: . ‚àÇLCE‚àÇŒµ=‚àÇŒµ‚àÇLCE‚ãÖ‚àÇLCE‚àÇw=1NxT(y‚àít) begin{aligned} frac{ partial mathcal{L}_{CE}}{ partial varepsilon} &amp;= frac{ partial varepsilon }{ partial mathcal{L}_{CE}} cdot frac{ partial mathcal{L}_{CE}}{ partial mathbf{w}} &amp;= frac{1}{N} mathbf{x^T(y - t)} end{aligned}‚àÇŒµ‚àÇLCE‚Äã‚Äã‚Äã=‚àÇLCE‚Äã‚àÇŒµ‚Äã‚ãÖ‚àÇw‚àÇLCE‚Äã‚Äã=N1‚ÄãxT(y‚àít)‚Äã . References . CSC321 Intro to Neural Networks and Machine Learning | Supervised and Unsupervised Machine Learning Algorithms | .",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/2017/07/02/ml-classification.html",
            "relUrl": "/machine%20learning/2017/07/02/ml-classification.html",
            "date": " ‚Ä¢ Jul 2, 2017"
        }
        
    
  
    
        ,"post13": {
            "title": "Learning HMM parameters for Continous Density Models",
            "content": "Introduction . In the previous post, we considered a scenario where observation sequences YYY are discrete symbols. However, for many practical problems, the observation symbols are continuous vectors. As a result, the continuous probability density function (pdf) is used to model the space of the observation signal associated with each state. The most commonly used emission distribution is gaussian distribution and the gaussian mixture models. . Gaussian Distribution and the Gaussian Mixture Models . It is popular to represent the randomness of continuous-valued using the multivariate Gaussian distribution. A vector-valued random variable x mathbf{x}x is said to have a multivariate normal (or Gaussian) distribution with mean Œº=E[x] mu= mathop{ mathbf{E[x]}}Œº=E[x] and covariance matrix Œ£=cov[x] Sigma= mathbf{cov[x]}Œ£=cov[x] if: P(x;Œº,Œ£)=N(x‚à£Œº,Œ£)=1(2œÄ)D/2‚à£Œ£‚à£12exp‚Å°(‚àí12[x‚àíŒº]Œ£‚àí1[x‚àíŒº]T)P( mathbf{x}; mu, Sigma) = mathcal{N( mathbf{x} mid mu, Sigma)}= frac{1}{(2 pi)^{D/2} | Sigma|^ frac{1}{2}} quad exp Big(- frac{1}{2}[ mathbf{x} - mu] Sigma^{-1}[ mathbf{x} - mu]^ mathsf{T} Big)P(x;Œº,Œ£)=N(x‚à£Œº,Œ£)=(2œÄ)D/2‚à£Œ£‚à£21‚Äã1‚Äãexp(‚àí21‚Äã[x‚àíŒº]Œ£‚àí1[x‚àíŒº]T) where DDD is the dimensionality of x mathbf{x}x. The Œº muŒº represents the location where samples are most likely to be generated, and the Œ£ SigmaŒ£ indicates the level to which two variables vary together. . However, a single Gaussian distribution is insufficient to represent the state-dependent observation space for an HMM state st=is_t=ist‚Äã=i. This is because there are large amounts of training data collected from various appliance instances with different modes, distortions, background noises, etc which are used to train the parameters of individual HMM states. In this case, a Gaussian mixture model (GMM) is adopted to represent the state-dependent observation space. . A mixture model is a probabilistic model for density estimation using a mixture distribution and can be regarded as a type of unsupervised learning or clustering. They provide a method of describing more complex probability distributions by combining several probability distributions. The following equation gives a multivariate Gaussian mixture distribution: P(x)=‚àëk=1KœâkN(x‚à£Œºk,Œ£k)P( mathbf{x}) = displaystyle sum_{k=1}^{K} omega_k mathcal{N( mathbf{x} mid mu_k, Sigma_k)}P(x)=k=1‚àëK‚Äãœâk‚ÄãN(x‚à£Œºk‚Äã,Œ£k‚Äã) The parameters œâk omega_kœâk‚Äã are called mixing coefficients, which must fulfill ‚àëk=1Kœâk=1 displaystyle sum_{k=1}^{K} omega_k =1k=1‚àëK‚Äãœâk‚Äã=1 and given N(x‚à£Œºk,Œ£k)‚â•0 mathcal{N( mathbf{x} mid mu_k, Sigma_k)} geq 0N(x‚à£Œºk‚Äã,Œ£k‚Äã)‚â•0 and P(x)‚â•0P( mathbf{x}) geq 0P(x)‚â•0 we also have that 0‚â§œâk‚â•10 leq omega_k geq 10‚â§œâk‚Äã‚â•1. Each Gaussian density N(x‚à£Œºk,Œ£k) mathcal{N( mathbf{x} mid mu_k, Sigma_k)}N(x‚à£Œºk‚Äã,Œ£k‚Äã) is called a component of the mixture and has its own mean Œºk mu_kŒºk‚Äã and covariance Œ£k Sigma_kŒ£k‚Äã. . HMM with Gaussian emission distribution . If the observations are continuous, it is common for the emission probabilities to be a conditional Gaussian such that: P(yt‚à£st=i)=N(yt‚à£Œºi,Œ£i)P( mathbb{y_t} mid s_t =i) = mathcal{N( mathbf{y_t} mid mu_i, Sigma_i)}P(yt‚Äã‚à£st‚Äã=i)=N(yt‚Äã‚à£Œºi‚Äã,Œ£i‚Äã) where Œºi mu_iŒºi‚Äã and Œ£i Sigma_iŒ£i‚Äã are mean vector and covariance matrix associated with state iii. The re-estimation formula for the mean vector and covariance matrix of a state gausian pdf can be derived as: Œº^i=‚àët=1TŒ≥t(i)y(t)‚àët=1TŒ≥t(i)Œ£^i=‚àët=1TŒ≥t(i)[y(t)‚àíŒº^i]‚ãÖ[y(t)‚àíŒº^i]T‚àët=1TŒ≥t(i) begin{aligned} hat{ mu}_i &amp; = frac{ displaystyle sum_{t=1}^{T} gamma_t(i) mathbb{y(t)}}{ displaystyle sum_{t=1}^{T} gamma _t(i)} hat{ Sigma}_i &amp; = frac{ displaystyle sum_{t=1}^{T} gamma_t(i) [ mathbf{y(t)}- hat{ mu}_i] cdot[ mathbf{y(t)}- hat{ mu}_i]^T}{ displaystyle sum_{t=1}^{T} gamma_t(i)} end{aligned}Œº^‚Äãi‚ÄãŒ£^i‚Äã‚Äã=t=1‚àëT‚ÄãŒ≥t‚Äã(i)t=1‚àëT‚ÄãŒ≥t‚Äã(i)y(t)‚Äã=t=1‚àëT‚ÄãŒ≥t‚Äã(i)t=1‚àëT‚ÄãŒ≥t‚Äã(i)[y(t)‚àíŒº^‚Äãi‚Äã]‚ãÖ[y(t)‚àíŒº^‚Äãi‚Äã]T‚Äã‚Äã . HMMs with Gaussian Mixture Model . In HMMs with Gaussian mixture pdf, the emission probabilities is given by P(yt‚à£st=i)=‚àëk=1Mœâ_ikN(yt‚à£Œºik,Œ£ik)P( mathbb{y_t} mid s_t =i) = displaystyle sum_{k=1}^{M} omega _{ik} mathcal{N( mathbb{y_t} mid mu_{ik}, Sigma_{ik})}P(yt‚Äã‚à£st‚Äã=i)=k=1‚àëM‚Äãœâ_ikN(yt‚Äã‚à£Œºik‚Äã,Œ£ik‚Äã) where œâik omega_{ik}œâik‚Äã is the prior probability of the kthk^{th}kth component of the mixture. The posterior probability of state st=is_t=ist‚Äã=i at time ttt and state st+1=js_{t+1}=jst+1‚Äã=j at time t+1t+1t+1 given the model Œª lambdaŒª and the observation sequence YYY is Œ≥t(i,j)=P(st=i,st+1=j‚à£Y,Œª)=Œ±t(i)aij[‚àëk=1MœâikN(yt‚à£Œºik,Œ£ik)]Œ≤t+1(j)‚àëi=1NŒ±T(i) begin{aligned} gamma_t(i,j)&amp; =P(s_t=i, s_{t+1}=j mid Y, lambda) &amp; = frac{ alpha_t(i)a_{ij} Big[ displaystyle sum_{k=1}^{M} omega_{ik} mathcal{N( mathbf{y_t} mid mu_{ik}, Sigma_{ik})} Big] beta_{t+1}(j)}{ displaystyle sum_{i=1}^{N} alpha_T(i)} end{aligned}Œ≥t‚Äã(i,j)‚Äã=P(st‚Äã=i,st+1‚Äã=j‚à£Y,Œª)=i=1‚àëN‚ÄãŒ±T‚Äã(i)Œ±t‚Äã(i)aij‚Äã[k=1‚àëM‚Äãœâik‚ÄãN(yt‚Äã‚à£Œºik‚Äã,Œ£ik‚Äã)]Œ≤t+1‚Äã(j)‚Äã‚Äã . and the posterior probability of state st=is_t=ist‚Äã=i at time ttt given the model Œª lambdaŒª and observation YYY is Œ≥t(i)=Œ±t(i)Œ≤t(i)‚àëi=1NŒ±T(i) gamma_t(i) = frac{ alpha_t(i) beta_t(i)}{ displaystyle sum_{i=1}^{N} alpha _T(i)}Œ≥t‚Äã(i)=i=1‚àëN‚ÄãŒ±T‚Äã(i)Œ±t‚Äã(i)Œ≤t‚Äã(i)‚Äã Let define the joint posterior probability of the state sis_isi‚Äã and the kthk^{th}kth gaussian component pdf of state iii at time ttt . Œæ(i,k)=P(St=si,m(t)=k‚à£Y,Œª)=‚àëj=1NŒ±t(j)aijœâikN(yt‚à£Œºik,Œ£ik)Œ≤t+1(j)‚àëi=1NŒ±T(i) begin{aligned} xi(i,k) &amp;= P(S_t=s_i, m(t)=k mid Y, lambda) &amp;= frac{ displaystyle sum_{j=1}^{N} alpha_t(j) a_{ij} omega_{ik} mathcal{N( mathbf{y_t} mid mu_{ik}, Sigma_{ik})} beta_{t+1}(j)}{ displaystyle sum_{i=1}^{N} alpha _T(i)} end{aligned}Œæ(i,k)‚Äã=P(St‚Äã=si‚Äã,m(t)=k‚à£Y,Œª)=i=1‚àëN‚ÄãŒ±T‚Äã(i)j=1‚àëN‚ÄãŒ±t‚Äã(j)aij‚Äãœâik‚ÄãN(yt‚Äã‚à£Œºik‚Äã,Œ£ik‚Äã)Œ≤t+1‚Äã(j)‚Äã‚Äã . The re-estimation formula for the mixture coefficients, the mean vectors and the covariance matrices of the state mixture gaussian pdf as . œâ^ik=‚àët=1TŒæt(i,k)‚àë_t=0TŒ≥t(i)Œº^ik=‚àë¬†t=1TŒæ¬†t(i,k)yt‚àët=1TŒæt(i,k)Œ£^ik=‚àët=1TŒæt(i,k)[yt‚àíŒº^ik]‚ãÖ[yt‚àíŒº^ik]T‚àët=1TŒæt(i,k) begin{aligned} hat{ omega}_{ik} &amp;= frac{ displaystyle sum_{t=1}^{T} xi_t(i,k)}{ displaystyle sum _{t=0}^{T} gamma_t(i)} hat{ mu}_{ik} &amp;= frac{ displaystyle sum _{t=1}^{T} xi _t(i,k) mathbf{y_t}}{ displaystyle sum_{t=1}^{T} xi_t(i,k)} hat{ Sigma}_{ik}&amp;= frac{ displaystyle sum_{t=1}^{T} xi_t(i,k)[ mathbf{y_t}- hat{ mu}_{ik}] cdot[ mathbf{y_t}- hat{ mu}_{ik}]^T}{ displaystyle sum_{t=1}^{T} xi_t(i,k)} end{aligned}œâ^ik‚ÄãŒº^‚Äãik‚ÄãŒ£^ik‚Äã‚Äã=‚àë_t=0TŒ≥t‚Äã(i)t=1‚àëT‚ÄãŒæt‚Äã(i,k)‚Äã=t=1‚àëT‚ÄãŒæt‚Äã(i,k)‚àë¬†t=1T‚ÄãŒæ¬†t‚Äã(i,k)yt‚Äã‚Äã=t=1‚àëT‚ÄãŒæt‚Äã(i,k)t=1‚àëT‚ÄãŒæt‚Äã(i,k)[yt‚Äã‚àíŒº^‚Äãik‚Äã]‚ãÖ[yt‚Äã‚àíŒº^‚Äãik‚Äã]T‚Äã‚Äã . Limitation of Baum‚ÄìWelch algorithm . When applying the Baum‚ÄìWelch algorithm in real data, we need to consider some heuristics in the ML EM algorithm. . How to provide initial parameter values. This is always an important question, and it is usually resolved by using a simple algorithm (e.g., K-means clustering or random initialization). | How to avoid instability in the parameter estimation (especially covariance parameter estimation) due to data sparseness. For example, some mixture components or hidden states cannot have sufficient data assigned in the Viterbi or forward-backward algorithm. This can be heuristically avoided by setting a threshold to update these parameters or setting minimum threshold values for covariance parameters. | Bayesian approaches can solve the above two problems. . References . Saeed V. Vaseghi, Advanced Digital Signal Processing, and Noise Reduction. John Wiley &amp; Sons, 2008. | Kevin P. Murphy, Machine Learning: A Probabilistic Perspective. The MIT Press Cambridge, Massachusetts, 2012. |",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/2017/06/06/hmm-gausian.html",
            "relUrl": "/machine%20learning/2017/06/06/hmm-gausian.html",
            "date": " ‚Ä¢ Jun 6, 2017"
        }
        
    
  
    
        ,"post14": {
            "title": "Introduction to Machine Learning",
            "content": "Introduction . Machine learning is a set of algorithms that automatically detect patterns in data and use the uncovered pattern to make inferences or predictions. It is a subfield of artificial intelligence that aims to enable computers to learn on their own. Any machine learning algorithms involve the necessary three steps: first, you identify a pattern from data, build (train) model that best explains the pattern and the world (unseen data), and lastly, use the model to predict or make an inference. Model training (building) can be seen as a learning process where the model is exposed to new, unfamiliar data step by step. . Machine learning is an exciting and fast-moving field of computer science with many new applications. Applications where machine learning algorithms are regularly deployed includes: . Computer vision: Object Classification in Photograph, image captioning. | Speech recognition, Automatic Machine Translation. | Detecting anomalies (e.g. Security, credit card fraud) | Speech recognition. | Communication systemsref | Robots learning complex behaviors | Recommendations services like in Amazo or Netflix where intelligent machine learning algorithms analyze your activity and compare it to the millions of other users to determine what you might like to buy or binge watch nextref. | . Machine learning algorithms that learn to recognize what they see have been the heart of Apple, Google, Amazon, Facebook, Netflix, Microsoft, etc. . Why Machine learning . For many problems such as recognizing people and objects and understanding human speech, it‚Äôs difficult to program the correct behavior by hand. However, with machine learning, these tasks are easier. Other reasons we might want to use machine learning to solve a given problem: . A system might need to adapt to a changing environment. For instance, spammers are always trying to figure out ways to trick our e-mail spam classifiers, so the classification algorithms will need to adapt continually. | A learning algorithm might be able to perform better than its human programmers. Learning algorithms have become world champions at a variety of games, from checkers to chess to Go. This would be impossible if the programs were only doing what they were explicitly told to do. | We may want an algorithm to behave autonomously for privacy or fairness reasons, such as with ranking search results or targeting ads. | . Types of Machine Learning . Machine learning is usually divided into three major types: Supervised Learning, Unsupervised Learning and . Supervised Learning: Supervised learning is where you have input variables x and an output variable y, and use an algorithm to learn the mapping function from the input to the outputref. For instance, if we‚Äôre trying to train a machine-learning algorithm to distinguish cars and trucks, we would collect car and truck images and label each one as a car or a truck. Supervised learning problems can be further grouped into regression and classification problems. . A regression problem: is when the output variable is a real value, such as ‚Äúdollars‚Äù or ‚Äúweight‚Äù e.g Linear regression and Random forest. | Classification: A classification problem is when the output variable is a category, such as ‚Äúred‚Äù or ‚Äúblue‚Äù or ‚Äúdisease‚Äù and ‚Äúno disease‚Äù, e.g. Support vector machines, random forest, and logistic regression. Some famous examples of supervised machine learning algorithms are: | . Unsupervised Learning : Unsupervised learning is where you only have input data (X) and no corresponding output variables. We just have a bunch of data and want to look for patterns in the data. For instance, maybe we have lots of examples of patients with autism and want to identify different subtypes of the condition. The most important types of unsupervised learning include: . Distribution modeling where one has an unlabeled dataset (such as a collection of images or sentences), and the goal is to learn a probability distribution that matches the dataset as closely as possible. | Clustering where the aim is to discover the inherent groupings in the data, such as grouping customers by purchasing behavior. | . Reinforcement Learning: is learning best actions based on reward or punishment. It involves learning what actions to take in a given situation, based on rewards and penalties. For example, a robot takes a big step forward, then falls. The next time, it takes a smaller stage and is able to hold its balance. The robot tries variations like this many times; eventually, it learns the right size of steps to take and walks steadily. It has succeeded. . There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. Action is what an agent can do in each state. When a robot takes action in a state, it receives a reward, feedback from the environment. A reward can be positive or negative (penalties). . Typical ML task: Linear Regression . In regression, we are interested in predicting a scalar-valued target, such as the price of a stock. By linear, we mean that the target must be predicted as a linear function of the inputs. This is a kind of supervised learning algorithm; recall that, in supervised learning, we have a collection of training examples labeled with the correct outputs. Example applications of linear regression include weather forecasting, house pricing prediction, student performance (GPA) prediction, just to mention a few. . Linear Regression: Formulating a learning problem . To formulate a learning problem mathematically, we need to define two things: a model (hypothesis)** and a *loss function. After defining model and loss function, we solve an optimization problem with the aim to find the model parameters that best fit the data. . Model (Hypothesis): It is the set of allowable hypotheses or functions that compute predictions from the inputs. In the case of linear regression, the model simply consists of linear functions given by: . y=‚àëjwjxj+by = sum_j w_jx_j + by=j‚àë‚Äãwj‚Äãxj‚Äã+b . where www is the weights, and bbb is an intercept term, which we‚Äôll call the bias. These two terms are called model parameters denoted as Œ∏ thetaŒ∏. . Loss function: It defines how well the model fit the data and thus show how far off the prediction yyy is from the target ttt and given as: . L(y,t)=12(y‚àít)2 mathcal{L(y,t)} = frac{1}{2}(y - t)^2L(y,t)=21‚Äã(y‚àít)2 . Since the loss function show how far off the prediction is from the target for one data point. We also need to define a cost function. The cost function is simply the loss, averaged over all the training examples. . J(w1‚Ä¶wD,b)=1N‚àëi=1NL(y(i),t(i))=12N‚àëi=1N(y(i)‚àít(i))2=12N‚àëi=1N(‚àëjwjxj(i)+b‚àít(i)) begin{aligned} J (w_1 ldots w_D,b) &amp; = frac{1}{N} sum_{i=1}^N mathcal{L}(y^{(i)},t^{(i)}) &amp; = frac{1}{2N} sum_{i=1}^N (y^{(i)} - t^{(i)})^2 &amp;= frac{1}{2N} sum_{i=1}^N left( sum_j w_jx_j^{(i)} + b -t^{(i)} right) end{aligned}J(w1‚Äã‚Ä¶wD‚Äã,b)‚Äã=N1‚Äãi=1‚àëN‚ÄãL(y(i),t(i))=2N1‚Äãi=1‚àëN‚Äã(y(i)‚àít(i))2=2N1‚Äãi=1‚àëN‚Äã(j‚àë‚Äãwj‚Äãxj(i)‚Äã+b‚àít(i))‚Äã . In vectorized form: . J=12N‚à•y‚àít‚à•2=12N(y‚àít)T(y‚àít)wherey=wTx mathbf{J} = frac{1}{2N} lVert mathbf{y-t} lVert^2 = frac{1}{2N} mathbf{(y - t)^T(y-t)} quad text{where} quad mathbf{y = w^Tx}J=2N1‚Äã‚à•y‚àít‚à•2=2N1‚Äã(y‚àít)T(y‚àít)wherey=wTx . The python implementation of the cost function (vectorized) is shown below. . def loss(x, w, t): N, D = np.shape(x) y = np.matmul(x,w.T) loss = (y - t) return loss . def cost(x,w, t): &#39;&#39;&#39; Evaluate the cost function in a vectorized manner for inputs `x` and targets `t`, at weights `w1`, `w2` and `b`. N, D = np.shape(x) return (loss(x, w,t) **2).sum() / (2.0 * N) . Combine our model and loss function, we get an optimization problem, where we are trying to minimize a cost function concerning the model parameters Œ∏ thetaŒ∏ (i.e. the weights and bias). . Solving the optimization problem . We now want to find the choice of model parameters Œ∏w1‚Ä¶wD,b theta _{w_1 ldots w_D,b}Œ∏w1‚Äã‚Ä¶wD‚Äã,b‚Äã that minimizes J(w1‚Ä¶wD,b)J (w_1 ldots w_D,b)J(w1‚Äã‚Ä¶wD‚Äã,b) as given in the cost function above.There are two methods that we can use: direct solution and gradient descent. . Direct Solution . One way to compute the minimum of a function is to set the partial derivatives to zero. For simplicity, let‚Äôs assume the model doesn‚Äôt have a bias term, as shown in the equation below. . JŒ∏=12N‚àëi=1N(‚àëjwjxj(i)‚àít(i))J_ theta = frac{1}{2N} sum_{i=1}^N left( sum_j w_jx_j^{(i)} -t^{(i)} right)JŒ∏‚Äã=2N1‚Äãi=1‚àëN‚Äã(j‚àë‚Äãwj‚Äãxj(i)‚Äã‚àít(i)) . In vectorized form . J=12N‚à•y‚àít‚à•212N(y‚àít)T(y‚àít)wherey=wx mathbf{J} = frac{1}{2N} lVert mathbf{y-t} rVert ^2 frac{1}{2N} mathbf{(y - t)^T(y-t)} quad text{where} quad mathbf{y = wx}J=2N1‚Äã‚à•y‚àít‚à•22N1‚Äã(y‚àít)T(y‚àít)wherey=wx . For matrix differentiation we need the following results: . ‚àÇAx‚àÇx=AT‚àÇ(xTAx)‚àÇx=2ATx begin{aligned} frac{ partial mathbf{Ax}}{ partial mathbf{x}} &amp; = mathbf{A}^T frac{ partial ( mathbf{x}^T mathbf{Ax})}{ partial mathbf{x}} &amp; = 2 mathbf{A}^T mathbf{x} end{aligned}‚àÇx‚àÇAx‚Äã‚Äã=AT‚àÇx‚àÇ(xTAx)‚Äã=2ATx‚Äã . Setting the partial derivatives of cost function in vectorized form to zero we obtain: . ‚àÇJ‚àÇw=12N‚àÇ(wTxTxw‚àí2tTwx+tTt)‚àÇw=12N(2xTxw‚àí2xTt)w=(xTx)‚àí1xTt begin{aligned} frac{ partial mathbf{J}}{ partial mathbf{w}} &amp; = frac{1}{2N} frac{ partial left( mathbf{w^Tx^Tx w} -2 mathbf{t^Twx} + mathbf{t^Tt} right)}{ partial mathbf{w}} &amp;= frac{1}{2N} left(2 mathbf{x}^T mathbf{xw} -2 mathbf{x}^T mathbf{t} right) mathbf{w} &amp;= ( mathbf{x^Tx})^{-1} mathbf{x^Tt} end{aligned}‚àÇw‚àÇJ‚Äãw‚Äã=2N1‚Äã‚àÇw‚àÇ(wTxTxw‚àí2tTwx+tTt)‚Äã=2N1‚Äã(2xTxw‚àí2xTt)=(xTx)‚àí1xTt‚Äã . In python, this result can be implemented as follows: . def direct method(x, t): &#39;&#39;&#39; Solve linear regression exactly. (fully vectorized) Given `x` - NxD matrix of inputs `t` - target outputs Returns the optimal weights as a D-dimensional vector &#39;&#39;&#39; N, D = np.shape(x) A = np.matmul(x.T, x) c = np.dot(x.T, t) return np.matmul(linalg.inv(A), c) . Gradient Descent . The optimization algorithm commonly used to train machine learning is the gradient descent algorithm. It works by taking the derivative of the cost function JJJ with respect to the parameters at a specific position on this cost function and updates the parameters in the direction of the negative gradient. The entries of the gradient vector are simply the partial derivatives with respect to each of the variables: . ‚àÇJ‚àÇw=(‚àÇJ‚àÇw1‚ãÆ‚àÇJ‚àÇwD) frac{ partial mathbf{J}}{ partial mathbf{w}} = begin{pmatrix} frac{ partial J}{ partial w_1} vdots frac{ partial J}{ partial w_D} end{pmatrix}‚àÇw‚àÇJ‚Äã=‚éù‚éú‚éú‚éõ‚Äã‚àÇw1‚Äã‚àÇJ‚Äã‚ãÆ‚àÇwD‚Äã‚àÇJ‚Äã‚Äã‚é†‚éü‚éü‚éû‚Äã . The parameter w mathbf{w}w is iteratively updated by taking steps proportional to the negative of the gradient: . wt+1=wt‚àíŒ±‚àÇJ‚àÇw=wt‚àíŒ±NxT(y‚àít) mathbf{w_{t+1}} = mathbf{ w_t }- alpha frac{ partial mathbf{J}}{ partial mathbf{w}} = mathbf{w_t} - mathbf{ frac{ alpha}{N}x^T(y-t)}wt+1‚Äã=wt‚Äã‚àíŒ±‚àÇw‚àÇJ‚Äã=wt‚Äã‚àíNŒ±‚ÄãxT(y‚àít) . In coordinate systems this is equivalent to: . wt+1=wt‚àíŒ±1N‚àëi=1Nxt(y(i)‚àít(i))w_{t+1} = w_t - alpha frac{1}{N} sum_{i=1}^{N} x_t (y^{(i)}-t^{(i)})wt+1‚Äã=wt‚Äã‚àíŒ±N1‚Äãi=1‚àëN‚Äãxt‚Äã(y(i)‚àít(i)) . The python implementation of gradient descent is shown below: . def getGradient(x, w, t): N, D = np.shape(x) gradient = (1.0/ float(N)) * np.matmul(np.transpose(x), loss(x,w,t)) return gradient . def gradientDescentMethod(x, t, alpha=0.1, tolerance=1e-2): N, D = np.shape(x) #w = np.random.randn(D) w = np.zeros([D]) # Perform Gradient Descent iterations = 1 w_cost = [(w, cost(x,w, t))] while True: dw = getGradient(x, w, t) w_k = w - alpha * dw w_cost.append((w, cost(x, w, t))) # Stopping Condition if np.sum(abs(w_k - w)) &lt; tolerance: print (&quot;Converged.&quot;) break if iterations % 100 == 0: print (&quot;Iteration: %d - cost: %.4f&quot; %(iterations, cost(x, w, t))) iterations += 1 w = w_k return w, w_cost . Generalization . The goal of a learning algorithm is not only to make correct predictions on the training examples but also to be generalized to patterns not seen before. The average squared error on new examples is known as the generalization error, and we‚Äôd like this to be as small as possible. In practice, we normally tune model parameters by partitioning the dataset into three different subsets: . The training set is used to train the model. | The validation set is used to estimate the generalization error of each hyperparameter setting. | The test set is used at the very end, to estimate the generalization error of the final model, once all hyperparameters have been chosen. | .",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/2017/06/01/ml-introduction.html",
            "relUrl": "/machine%20learning/2017/06/01/ml-introduction.html",
            "date": " ‚Ä¢ Jun 1, 2017"
        }
        
    
  
    
        ,"post15": {
            "title": "Learning HMM parameters with Discrete Observation Models",
            "content": "Introduction . In the previous post, we discussed the basics of HMM modeling given model parameters Œª lambdaŒª and compute the likelihood values etc., efficiently based on the forward, backward, and Viterbi algorithms. In the like manner, we can efficiently train the HMM to obtain the model parameter Œª^ hat{ lambda}Œª^ from data. In this post, we will discuss different methods for training HMM models. . This is the solution to Problem 3, which involves determining a method to learn model parameters Œª^ hat{ lambda}Œª^ given the sequence of observation variables YYY. Given the observation sequences YYY as training data, there is no optimal way of estimating the model parameters. However, using iterative procedure we can choose Œª^=(A^,B^,œÄ^) hat{ lambda} = ( hat{A}, hat{B}, hat{ pi})Œª^=(A^,B^,œÄ^) such that P(Y‚à£Œª)P(Y mid lambda)P(Y‚à£Œª) is locally maximized.The most common procedure which has been employed to his problem is the Baum-Welch method. . Baum-Welch Methods . This method assumes an initial model parameter of Œª lambdaŒª, which should be adjusted to increase P(Y‚à£Œª)P(Y mid lambda)P(Y‚à£Œª). The initial parametrs can be constructed in any way or employ the first five procedure of the Segmental K-means algorithm. The optimazation criteria is called the maximum likelihood criteria.The function P(Y‚à£Œª)P(Y mid lambda)P(Y‚à£Œª) is called the likelihood function. . The E-M Auxilliary Function . Let Œª lambdaŒª represent the current model and Œª^ hat{ lambda}Œª^ represent the candidate models. The learning objective is to make: P(Y‚à£Œª^)‚â•P(Y‚à£Œª)P(Y mid hat{ lambda}) geq P(Y mid lambda)P(Y‚à£Œª^)‚â•P(Y‚à£Œª) which is equivalently to log‚Å°[P(Y‚à£Œª^)]‚â•log‚Å°[P(Y‚à£Œª)] log[P(Y mid hat{ lambda})] geq log [P(Y mid lambda)]log[P(Y‚à£Œª^)]‚â•log[P(Y‚à£Œª)] . Let also define the auxilliary function Q(Œª^‚à£Œª)Q( hat{ lambda} mid lambda)Q(Œª^‚à£Œª) such that: Q(Œª^‚à£Œª)=E[log‚Å°P(Y,S‚à£Œª^)‚à£Y,Œª]=‚àësP(S‚à£Y,Œª)‚ãÖlog‚Å°[P(Y,S‚à£Œª^)] begin{aligned} Q( hat{ lambda} mid lambda) &amp; = mathbb{E} Big[ log P(Y,S mid hat{ lambda}) mid Y, lambda Big] &amp; = sum_s P(S mid Y, lambda) cdot log [P(Y,S mid hat{ lambda})] end{aligned}Q(Œª^‚à£Œª)‚Äã=E[logP(Y,S‚à£Œª^)‚à£Y,Œª]=s‚àë‚ÄãP(S‚à£Y,Œª)‚ãÖlog[P(Y,S‚à£Œª^)]‚Äã . The Maximum Likelihood Estimation (MLE) of the model parameter Œª lambdaŒª for complete data YYY and hidden state SSS is; . Œª^=arg‚Å°max‚Å°Œª‚àësP(Y,S‚à£Œª) hat{ lambda} = arg max _{ lambda} sum_s P(Y, S mid lambda)Œª^=argŒªmax‚Äãs‚àë‚ÄãP(Y,S‚à£Œª) . However due to the presence of several stochatsic constraints it turns out to be easier to mximize uxilliary function Q(Œª^‚à£Œª)Q( hat{ lambda} mid lambda)Q(Œª^‚à£Œª) rather than directly maximize P(Y‚à£Œª^)P(Y mid hat{ lambda})P(Y‚à£Œª^). Thus the MLE of the model parameter Œª lambdaŒª for complete data YYY and hidden state SSS become: . Œª^=arg‚Å°max‚Å°ŒªQ(Œª^‚à£Œª) hat{ lambda} = arg max _{ lambda} Q( hat{ lambda} mid lambda)Œª^=argŒªmax‚ÄãQ(Œª^‚à£Œª) . It can be shown that the parameter estimated by the EM procedure, Q(Œª^‚à£Œª)Q( hat{ lambda} mid lambda)Q(Œª^‚à£Œª), always increases the likelihood value. You may concert reference 2 chapter 3 for details on the prove. . Expectation step . To find ML estimates of HMM parameters, we first expand the auxiliary function rewrite it by substituting the joint distribution of complete data likelihood. . Q(Œª^‚à£Œª)=E[log‚Å°P(Y,S‚à£Œª^)‚à£Y,Œª]=‚àësP(S‚à£Y,Œª)‚ãÖlog‚Å°[P(Y,S‚à£Œª^)]=‚àësP(S‚à£Y,Œª)‚ãÖ[log‚Å°œÄ^1+log‚Å°b^1(y1)+‚àët=2T(log‚Å°a^ij+log‚Å°b^i(yt))] begin{aligned} Q( hat{ lambda} mid lambda) &amp; = mathbf{E} Big[ log P(Y,S mid hat{ lambda}) mid Y, lambda Big] &amp; = sum_s P(S mid Y, lambda) cdot log [P(Y,S mid hat{ lambda})] &amp; = sum_s P(S mid Y, lambda) cdot Big[ log hat{ pi}_1 + log hat{b}_1(y_1) + sum _{t=2}^T big( log hat{a} _{ij} + log hat{b}_i({y_t}) big) Big] end{aligned}Q(Œª^‚à£Œª)‚Äã=E[logP(Y,S‚à£Œª^)‚à£Y,Œª]=s‚àë‚ÄãP(S‚à£Y,Œª)‚ãÖlog[P(Y,S‚à£Œª^)]=s‚àë‚ÄãP(S‚à£Y,Œª)‚ãÖ[logœÄ^1‚Äã+logb^1‚Äã(y1‚Äã)+t=2‚àëT‚Äã(loga^ij‚Äã+logb^i‚Äã(yt‚Äã))]‚Äã . We have three term to solve: . The initial probability œÄ^ hat{ pi}œÄ^ , | State transition probability A^=a^ij hat{A} = hat{a}_{ij}A^=a^ij‚Äã and | Emission probability B^=b^i(yt) hat{B} = hat{b}_i(y_t)B^=b^i‚Äã(yt‚Äã). | . Let first define important parameters that we will use. For t=1,2...Tt = 1,2...Tt=1,2...T, 1‚â§i‚â•N1 leq i geq N1‚â§i‚â•N and 1‚â§j‚â•N1 leq j geq N1‚â§j‚â•N, we define: . Œæt(i,j)=P(st=i,st+1=j‚à£Y,Œª) xi_t(i,j)=P(s_t=i, s_{t+1}=j mid Y, lambda)Œæt‚Äã(i,j)=P(st‚Äã=i,st+1‚Äã=j‚à£Y,Œª) . an expected transition probability from st=is_t=ist‚Äã=i to , st+1=js_{t+1}=jst+1‚Äã=j. The probability of being in state sis_isi‚Äã at time ttt and state sjs_jsj‚Äã at time t+1t+1t+1 given the model Œª lambdaŒª and observation sequences YYY. . Œæt(i,j) xi_t(i,j)Œæt‚Äã(i,j) can be written in terms of forward Œ±t(i) alpha_t(i)Œ±t‚Äã(i) and backward Œ≤t+1(j) beta_{t+1}(j)Œ≤t+1‚Äã(j) variables as: . Œæt(i,j)=Œ±t(i)aijbi(yt+1)Œ≤t+1(j)P(Y‚à£Œª)=Œ±t(i)aijbi(yt+1)Œ≤t+1(i)‚àëi=1N‚àëj=1NŒ±t(i)aijbj(yt+1)Œ≤t+1(j) begin{aligned} xi_t(i,j) &amp;= frac{ alpha_t(i)a_{ij}b_i(y_{t+1}) beta_{t+1}(j)}{P(Y mid lambda)} &amp;= frac{ alpha_t(i)a_{ij}b_i(y_{t+1}) beta_{t+1}(i)}{ displaystyle sum_{i=1}^{N} displaystyle sum_{j=1}^{N} alpha_t(i)a_{ij}b_j(y_{t+1}) beta_{t+1}(j)} end{aligned}Œæt‚Äã(i,j)‚Äã=P(Y‚à£Œª)Œ±t‚Äã(i)aij‚Äãbi‚Äã(yt+1‚Äã)Œ≤t+1‚Äã(j)‚Äã=i=1‚àëN‚Äãj=1‚àëN‚ÄãŒ±t‚Äã(i)aij‚Äãbj‚Äã(yt+1‚Äã)Œ≤t+1‚Äã(j)Œ±t‚Äã(i)aij‚Äãbi‚Äã(yt+1‚Äã)Œ≤t+1‚Äã(i)‚Äã‚Äã . where the numerator term is just P(St=si,St+1=sj‚à£Y,Œª)P(S_t=s_i, S_{t+1}=s_j mid Y, lambda)P(St‚Äã=si‚Äã,St+1‚Äã=sj‚Äã‚à£Y,Œª) and the division by P(Y‚à£Œª)P(Y mid lambda)P(Y‚à£Œª) gives the desire probability measures. . We have previosly difined Œ≥t(i)=Œ±t(i)Œ≤t(i)P(Y‚à£Œª) gamma_t(i) = frac{ alpha_t(i) beta_t(i)}{P(Y mid lambda)}Œ≥t‚Äã(i)=P(Y‚à£Œª)Œ±t‚Äã(i)Œ≤t‚Äã(i)‚Äã as the probability of being in state sis_isi‚Äã at time ttt given the observation sequence and model parameter. Œ≥t(i) gamma_t(i)Œ≥t‚Äã(i) relate to Œæt(i,j) xi_t(i,j)Œæt‚Äã(i,j) as follows: . Œ≥t(i)=‚àëj=1NŒæt(i,j) gamma_t(i) = displaystyle sum_{j=1}^{N} xi_t(i,j)Œ≥t‚Äã(i)=j=1‚àëN‚ÄãŒæt‚Äã(i,j) . It follows that: . ‚àët=1T‚àí1Œ≥t(i)= displaystyle sum_{t=1}^{T-1} gamma_t(i)=t=1‚àëT‚àí1‚ÄãŒ≥t‚Äã(i)= Expected number of transitions from state iii . . ‚àët=1T‚àí1Œæt(i,j)= displaystyle sum_{t=1}^{T-1} xi_t(i,j)=t=1‚àëT‚àí1‚ÄãŒæt‚Äã(i,j)= Expected number of transitions from state iii to state jjj. . We provide a solution for each term. Considering the first term Q(œÄ^‚à£œÄ)Q( hat{ pi} mid pi)Q(œÄ^‚à£œÄ) we define the following auxiliary function for œÄi pi _iœÄi‚Äã as: . Q(œÄ^‚à£œÄ)=‚àësP(S‚à£Y,Œª)‚ãÖlog‚Å°œÄ^s1Q( hat{ pi} mid pi) = sum_s P(S mid Y, lambda) cdot log hat{ pi}_{s_1}Q(œÄ^‚à£œÄ)=s‚àë‚ÄãP(S‚à£Y,Œª)‚ãÖlogœÄ^s1‚Äã‚Äã . Since œÄ^s1 hat{ pi}_{s_1}œÄ^s1‚Äã‚Äã only depends on s1s_1s1‚Äã, it clear that: . P(S‚à£Y,Œª)=P(s1‚à£Y,Œª)P(S mid Y, lambda) = P(s_1 mid Y, lambda)P(S‚à£Y,Œª)=P(s1‚Äã‚à£Y,Œª) . Therefore Q(œÄ^‚à£pi)Q( hat{ pi} mid pi)Q(œÄ^‚à£pi) can be rewritten as: . Q(œÄ^‚à£œÄ)=‚àës1P(s1‚à£Y,Œª)‚ãÖlog‚Å°œÄ^s1=‚àëi=1NP(s1=i‚à£Y,Œª)‚ãÖlog‚Å°œÄ^i=‚àëi=1NŒ≥t(i)log‚Å°œÄ^i begin{aligned} Q( hat{ pi} mid pi) &amp;= sum_{s_1}P(s_1 mid Y, lambda) cdot log hat{ pi}_{s_1} &amp;= sum_{i=1}^N P(s_1=i mid Y, lambda) cdot log hat{ pi}_{i} &amp; = sum_{i=1}^N gamma_t(i) log hat{ pi}_{i} end{aligned}Q(œÄ^‚à£œÄ)‚Äã=s1‚Äã‚àë‚ÄãP(s1‚Äã‚à£Y,Œª)‚ãÖlogœÄ^s1‚Äã‚Äã=i=1‚àëN‚ÄãP(s1‚Äã=i‚à£Y,Œª)‚ãÖlogœÄ^i‚Äã=i=1‚àëN‚ÄãŒ≥t‚Äã(i)logœÄ^i‚Äã‚Äã . Next, we focus on the second term Q(A^‚à£A)Q( hat{A} mid A)Q(A^‚à£A) . Q(A^‚à£A)=‚àësP(S‚à£Y,Œª)‚ãÖ‚àët=2Tlog‚Å°a^st,st+1Q( hat{A} mid A) = sum_s P(S mid Y, lambda) cdot sum _{t=2}^T log hat{a}_{s_t,s_{t+1}}Q(A^‚à£A)=s‚àë‚ÄãP(S‚à£Y,Œª)‚ãÖt=2‚àëT‚Äãloga^st‚Äã,st+1‚Äã‚Äã . Similar to Q(œÄ^‚à£œÄ)Q( hat{ pi} mid pi)Q(œÄ^‚à£œÄ) , we obtain P(S‚à£Y,Œª)=P(s1‚à£Y,Œª)=P(st,st+1‚à£Y,Œª)P(S mid Y, lambda) = P(s_1 mid Y, lambda) = P(s_t,s_{t+1} mid Y, lambda)P(S‚à£Y,Œª)=P(s1‚Äã‚à£Y,Œª)=P(st‚Äã,st+1‚Äã‚à£Y,Œª) . Therefore . Q(A^‚à£A)=‚àët=1T‚àí1‚àësP(st,st+1‚à£Y,Œª)log‚Å°a^st,st+1=‚àët=1T‚àí1‚àëi=1N‚àëj=1NP(st=i,st+1=j‚à£Y,Œª)log‚Å°a^ij=‚àët=1T‚àí1‚àëi=1N‚àëj=1NŒæt(i,j)log‚Å°a^ij begin{aligned} Q( hat{A} mid A) &amp; = sum _{t=1}^{T-1} sum_s P(s_t,s_{t+1} mid Y, lambda) log hat{a}_{s_t,s_{t+1}} &amp; = sum _{t=1}^{T-1} sum_{i=1}^N sum_{j=1}^N P(s_t=i,s_{t+1}=j mid Y, lambda) log hat{a}_{ij} &amp; = sum _{t=1}^{T-1} sum_{i=1}^N sum_{j=1}^N xi_t(i,j) log hat{a}_{ij} end{aligned}Q(A^‚à£A)‚Äã=t=1‚àëT‚àí1‚Äãs‚àë‚ÄãP(st‚Äã,st+1‚Äã‚à£Y,Œª)loga^st‚Äã,st+1‚Äã‚Äã=t=1‚àëT‚àí1‚Äãi=1‚àëN‚Äãj=1‚àëN‚ÄãP(st‚Äã=i,st+1‚Äã=j‚à£Y,Œª)loga^ij‚Äã=t=1‚àëT‚àí1‚Äãi=1‚àëN‚Äãj=1‚àëN‚ÄãŒæt‚Äã(i,j)loga^ij‚Äã‚Äã . Finally, we focus on the last term Q(B^‚à£B)Q( hat{B} mid B)Q(B^‚à£B) . Q(B^‚à£B)=‚àësP(S‚à£Y,Œª)‚ãÖ‚àët=1Tlog‚Å°b^i(yt)Q( hat{B} mid B) = sum_s P(S mid Y, lambda) cdot sum _{t=1}^T log hat{b}_{i}(y_t)Q(B^‚à£B)=s‚àë‚ÄãP(S‚à£Y,Œª)‚ãÖt=1‚àëT‚Äãlogb^i‚Äã(yt‚Äã) . Similary P(S‚à£Y,Œª)=P(st=i‚à£Y,Œª)P(S mid Y, lambda) = P(s_t = i mid Y, lambda)P(S‚à£Y,Œª)=P(st‚Äã=i‚à£Y,Œª). Therefore . Q(B^‚à£B)=‚àët=1T‚àësP(st=i‚à£Y,Œª)log‚Å°b^i(yt)=‚àët=1T‚àëi=1NŒ≥t(i)log‚Å°b^i(yt) begin{aligned} Q( hat{B} mid B) &amp;= sum _{t=1}^T sum_s P(s_t = i mid Y, lambda) log hat{b}_{i}(y_t) &amp; = sum _{t=1}^T sum_{i=1}^N gamma_t(i) log hat{b}_{i}(y_t) end{aligned}Q(B^‚à£B)‚Äã=t=1‚àëT‚Äãs‚àë‚ÄãP(st‚Äã=i‚à£Y,Œª)logb^i‚Äã(yt‚Äã)=t=1‚àëT‚Äãi=1‚àëN‚ÄãŒ≥t‚Äã(i)logb^i‚Äã(yt‚Äã)‚Äã . Thus, we summarize the auxiliary function. . Q(Œª^‚à£Œª)=Q(œÄ^‚à£œÄ)+Q(A^‚à£A)+Q(B^‚à£B)Q( hat{ lambda} mid lambda)= Q( hat{ pi} mid pi)+ Q( hat{A} mid A) + Q( hat{B} mid B)Q(Œª^‚à£Œª)=Q(œÄ^‚à£œÄ)+Q(A^‚à£A)+Q(B^‚à£B) . Maximization step . In the maximization step, we aim to maximize Q(œÄ^‚à£œÄ)Q( hat{ pi} mid pi)Q(œÄ^‚à£œÄ), Q(A^‚à£A)Q( hat{A} mid A)Q(A^‚à£A) and Q(B^‚à£B)Q( hat{B} mid B)Q(B^‚à£B) with respect to œÄ^ hat{ pi}œÄ^, A^ hat{A}A^ and B^ hat{B}B^ under the following constraints. . ‚àë_i=1NœÄ^=1,¬†and¬†‚àëi=1NA^=1 sum _{i=1}^N hat{ pi} = 1, text{ and } sum_{i=1}^N hat{A} = 1‚àë_i=1NœÄ^=1,¬†and¬†i=1‚àëN‚ÄãA^=1 . Considering the estimation of initial state probabilities œÄ^=œÄ^i mathbf{ hat{ pi}} = { hat{ pi}_i}œÄ^=œÄ^i‚Äã , we construct a Lagrange function (or Lagrangian): . Q‚àó(œÄ^‚à£œÄ)=‚àëi=1NŒ≥1(i)log‚Å°œÄ^i+Œ∑(‚àëi=1NœÄ^‚àí1)Q^*( hat{ pi} mid pi) = sum_{i=1}^N gamma_1(i) log hat{ pi}_{i} + eta left( sum_{i=1}^N hat{ pi} - 1 right)Q‚àó(œÄ^‚à£œÄ)=i=1‚àëN‚ÄãŒ≥1‚Äã(i)logœÄ^i‚Äã+Œ∑(i=1‚àëN‚ÄãœÄ^‚àí1) . Differentiating this Lagrangian with respect to individual probability parameter œÄ^i hat{ pi}_iœÄ^i‚Äã and set it to zero we obtain. . ‚àÇQ‚àó(œÄ^‚à£œÄ)‚àÇœÄ^i=Œ≥1(i)1œÄ^i+Œ∑=0œÄ^i=‚àí1Œ∑Œ≥1(i) begin{aligned} frac{ partial Q^*( hat{ pi} mid pi)}{ partial hat{ pi}_i } &amp; = gamma_1(i) frac{1}{ hat{ pi}_i} + eta = 0 hat{ pi}_i &amp;= - frac{1}{ eta} gamma_1(i) end{aligned}‚àÇœÄ^i‚Äã‚àÇQ‚àó(œÄ^‚à£œÄ)‚ÄãœÄ^i‚Äã‚Äã=Œ≥1‚Äã(i)œÄ^i‚Äã1‚Äã+Œ∑=0=‚àíŒ∑1‚ÄãŒ≥1‚Äã(i)‚Äã . Substituting the above equation into ‚àëi=1NœÄ^=1 sum_{i=1}^N hat{ pi} = 1‚àëi=1N‚ÄãœÄ^=1 constraint, we obtain: . ‚àëi=1NœÄ^=‚àëi=1N‚àí1Œ∑Œ≥1(i)=1‚áíŒ∑=‚àí‚àëi=1NŒ≥1(i) begin{aligned} sum_{i=1}^N hat{ pi} &amp;= sum_{i=1}^N - frac{1}{ eta} gamma_1(i) = 1 Rightarrow eta &amp;= - sum_{i=1}^N gamma_1(i) end{aligned}i=1‚àëN‚ÄãœÄ^‚áíŒ∑‚Äã=i=1‚àëN‚Äã‚àíŒ∑1‚ÄãŒ≥1‚Äã(i)=1=‚àíi=1‚àëN‚ÄãŒ≥1‚Äã(i)‚Äã . The ML estimate of new initial state probability is obtained by substituting the above equation into œÄ^i=‚àí1Œ∑Œ≥1(i) hat{ pi}_i = - frac{1}{ eta} gamma_1(i)œÄ^i‚Äã=‚àíŒ∑1‚ÄãŒ≥1‚Äã(i): . œÄ^i=Œ≥1(i)‚àëi=1NŒ≥1(i)=Œ≥1(i) hat{ pi}_i = frac{ gamma_1(i)}{ sum _{i=1}^N gamma_1(i)} = gamma_1(i)œÄ^i‚Äã=‚àëi=1N‚ÄãŒ≥1‚Äã(i)Œ≥1‚Äã(i)‚Äã=Œ≥1‚Äã(i) . In the same manner, we can derive the ML estimates of new state transition probability and new emission probability, which can be shown to be: . a^ij=‚àët=1T‚àí1Œæt(i,j)‚àët=1T‚àí1Œ≥t(i) hat{a}_{ij} = frac{ displaystyle sum_{t=1}^{T-1} xi_t(i,j)}{ displaystyle sum_{t=1}^{T-1} gamma_t(i)}a^ij‚Äã=t=1‚àëT‚àí1‚ÄãŒ≥t‚Äã(i)t=1‚àëT‚àí1‚ÄãŒæt‚Äã(i,j)‚Äã . And . b^i(k)=‚àët=1TœÑŒ≥t(i)‚àët=1TŒ≥t(i) hat{b}_i(k) = frac{ displaystyle sum_{t=1}^{T} tau gamma_t(i)}{ displaystyle sum_{t=1}^{T} gamma_t(i)}b^i‚Äã(k)=t=1‚àëT‚ÄãŒ≥t‚Äã(i)t=1‚àëT‚ÄãœÑŒ≥t‚Äã(i)‚Äã . where . œÑ={1¬†if¬†yt=k,0¬†otherwise¬† tau = begin{cases} 1 text{ if } y_t = k, 0 text{ otherwise } end{cases}œÑ={1¬†if¬†yt‚Äã=k,0¬†otherwise¬†‚Äã . If we denote the initial model Œª{ lambda}Œª and the re-estimation model by Œª^=(œÄ^i,a^ij,b^j(k)) hat{ lambda}=( hat{ pi}_i, hat{a}_{ij}, hat{b}_j(k))Œª^=(œÄ^i‚Äã,a^ij‚Äã,b^j‚Äã(k)). Then i t can be shown that either: . The initial model Œª lambdaŒª is a critical point of the likelihood in which case Œª^=Œª hat{ lambda}= lambdaŒª^=Œª or | P(Y‚à£Œª^)‚â§P(Y‚à£Œª)P(Y mid hat lambda) leq P(Y mid lambda)P(Y‚à£Œª^)‚â§P(Y‚à£Œª), i.e we have find the better model from which the observation sequence Y=y1,‚Ä¶YTY=y_1, ldots Y_TY=y1‚Äã,‚Ä¶YT‚Äã is more likely to be produced. | Hence we can go on iteractively computing until P(Y‚à£Œª^)P(Y mid hat{ lambda})P(Y‚à£Œª^) is maximazed. . The Baum-Welch Algorithm can be summarized as: . Require: Œª‚ÜêŒªinit lambda leftarrow lambda ^{init}Œª‚ÜêŒªinit . repeat | Compute the forward variable Œ±t(i) alpha _t(i)Œ±t‚Äã(i) from the forward algorithm | Compute the backward variable Œ≤t(i) beta _t(i)Œ≤t‚Äã(i) from the backward algorithm | Compute the occupation probabilities Œ≥t(i) gamma _t(i)Œ≥t‚Äã(i), and Œæt(i,j) xi _t(i,j)Œæt‚Äã(i,j) | Estimate the new HMM parameters Œª^ hat{ lambda}Œª^ | Update the HMM parameters Œª‚ÜêŒª^ lambda leftarrow hat{ lambda}Œª‚ÜêŒª^ | until Convergence | References . L. R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proceedings of the IEEE, Vol. 77, No. 2, February 1989. | Shinji Watanabe, Jen-Tzung Chien, Bayesian Speech and Language Processing, Cambridge University Press, 2015. | Viterbi Algorithm in Speech Enhancement and HMM | Nikolai Shokhirev, Hidden Markov Models |",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/2017/05/29/hmm-discrete.html",
            "relUrl": "/machine%20learning/2017/05/29/hmm-discrete.html",
            "date": " ‚Ä¢ May 29, 2017"
        }
        
    
  
    
        ,"post16": {
            "title": "The Basic of Hidden Markov Model",
            "content": "Introduction . HMM is a Markov model whose states are not directly observed; instead, each state is characterized by a probability distribution function. The probability distribution model the observation corresponding to that state. HMM has been extensively used in temporal pattern recognition such as speech, handwriting, gesture recognition, robotics, biological sequences, and recently in energy disaggregation. This tutorial will introduce the basic concept of HMM. . There are two variables in HMM: observed variables and hidden variables. The sequences of hidden variables form a Markov process, as shown in the figure below. In the context of NILM, the hidden variables are used to model states(ON, OFF, standby etc) of individual appliances, and the observed variables are used to model the electric usage. HMMs have been widely used in most of the recently proposed NILM approach. This is because HMM represents well the individual appliance internal states which are not directly observed in the targeted energy consumption. . A typical HMM is characterised by the following: . The finite set of hidden states SSS (e.g ON, stand-by, OFF, etc.) of an appliance, S={s1,s2....,sN}S = {s_1, s_2....,s_N }S={s1‚Äã,s2‚Äã....,sN‚Äã}. | The finite set of MMM observable symbol YYY per states (power consumption) observed in each state, Y={y1,y2....,yM}Y = {y_1, y_2....,y_M }Y={y1‚Äã,y2‚Äã....,yM‚Äã}. The observable symbol YYY can be discrete or a continuous set. | The transition matrix A={aij,1‚â§i,j‚â•N} mathbf{A}= {a_{ij},1 leq i,j geq N }A={aij‚Äã,1‚â§i,j‚â•N} represents the probability of moving from state st‚àí1=is_{t-1}=ist‚àí1‚Äã=i to st=js_t =jst‚Äã=j such that: aij=P(st=j‚à£st‚àí1=i)a_{ij} = P(s_{t} =j mid s_{t-1}=i)aij‚Äã=P(st‚Äã=j‚à£st‚àí1‚Äã=i), with aij‚â§0a_{ij} leq 0aij‚Äã‚â§0 and where sts_tst‚Äã denotes the state occupied by the system at time ttt. The matrix A mathbf{A}A is NxNN x NNxN. | The emission matrix B={bj(k)} mathbf{B} = {b_j(k) }B={bj‚Äã(k)} representing the probability of emission of symbol kkk œµ epsilonœµ YYY when system state is st=js_t=jst‚Äã=j such that: bj(k)=p(yt=k‚à£st=j)b_j(k) = p(y_t = k mid s_t=j)bj‚Äã(k)=p(yt‚Äã=k‚à£st‚Äã=j) The matrix B mathbf{B}B is an NxMN x MNxM. The emission probability can be discrete or continous distribution. If the emission is descrete a multinomial distribution is used and multivariate Gaussian distribution is usually used for continous emission. | And the initial state probability distribution œÄ={œÄi} mathbf{ pi} = { pi_i }œÄ={œÄi‚Äã} indicating the probability of each state of the hidden variable at t=1t = 1t=1 such that, œÄi=P(q1=si),1‚â§i‚â•N pi _i = P(q_1 = s_i), 1 leq i geq NœÄi‚Äã=P(q1‚Äã=si‚Äã),1‚â§i‚â•N. | . For brief introduction of optuna, you can watch this video . The complete HMM specification requires; . Finite set of hidden states NNN and observation symbols MMM | Length of observation seqences TTT and | Specification of three probability measures A,B mathbf{A}, mathbf{B}A,B and œÄ mathbf{ pi}œÄ | The set of all HMM model parameters is represented by Œª=(œÄ,A,B) mathbf{ lambda} =( pi, A, B)Œª=(œÄ,A,B). . Since $S$ is not observed, the likelihood function of YYY is given by the joint distribution of $Y$ and $S$ over all possible state. . P(Y‚à£Œª)=‚àëP(Y,S‚à£Œª)P(Y mid lambda) = sum P(Y, S mid lambda)P(Y‚à£Œª)=‚àëP(Y,S‚à£Œª) . where . P(Y,S‚à£Œª)=P(Y‚à£S,Œª)P(S‚à£Œª)P(Y,S mid lambda) = P(Y mid S, lambda)P(S mid lambda)P(Y,S‚à£Œª)=P(Y‚à£S,Œª)P(S‚à£Œª) . Note that yty_tyt‚Äã is independent and identically distributed given state sequence S={s1,s2....,sN}S = {s_1, s_2....,s_N }S={s1‚Äã,s2‚Äã....,sN‚Äã}. Also each state at time ttt depend on the state at its previous time t‚àí1t-1t‚àí1. Then . P(Y‚à£S,Œª)=‚àèt=1TP(yt‚à£st)P(Y mid S, lambda) = prod_{t=1}^T P(y_t mid s_t)P(Y‚à£S,Œª)=t=1‚àèT‚ÄãP(yt‚Äã‚à£st‚Äã) . Similary . P(S‚à£Œª)=œÄs1‚àèt=2TaijP(S mid lambda) = pi _{s_1} prod _{t=2}^T a_{ij}P(S‚à£Œª)=œÄs1‚Äã‚Äãt=2‚àèT‚Äãaij‚Äã . The joint probability is therefore: . P(Y‚à£Œª)=œÄs1P(y1‚à£s1)‚àë‚àèt=2TaijP(yt‚à£st)P(Y mid lambda) = pi _{s_1}P(y_1 mid s_1) sum prod_{t=2}^T a_{ij} P(y_t mid s_t)P(Y‚à£Œª)=œÄs1‚Äã‚ÄãP(y1‚Äã‚à£s1‚Äã)‚àët=2‚àèT‚Äãaij‚ÄãP(yt‚Äã‚à£st‚Äã) . Three main problems in HMMs . When applying HMM to a real-world problem, three crucial issues must be solved. . Evaluation Problem: Given HMM parameters Œª lambdaŒª and the observation sequence Y={Y1,Y2....,YM}Y = {Y_1, Y_2....,Y_M }Y={Y1‚Äã,Y2‚Äã....,YM‚Äã}, find P(Y‚à£Œª)P(Y mid lambda)P(Y‚à£Œª) the likelihood of the observation sequence YYY given the model Œª lambdaŒª. This problem gives a score on how well a given model matches a given observation and allows you to choose the model that best matches the observation. | Decoding Problem: Given HMM parameters Œª lambdaŒª and the observation seqence Y={Y1,Y2....,YM}Y = {Y_1, Y_2....,Y_M }Y={Y1‚Äã,Y2‚Äã....,YM‚Äã}, find an optimal state sequense S={S1,S2....,SN}S = {S_1, S_2....,S_N }S={S1‚Äã,S2‚Äã....,SN‚Äã} which best explain the observation.This problem attempt to cover the hidden part of the model. | Learning Problem: Given the obseravtion seqence Y={Y1,Y2....,YM}Y = {Y_1, Y_2....,Y_M }Y={Y1‚Äã,Y2‚Äã....,YM‚Äã}, find the model parameters Œª lambdaŒª that maximize P(Y‚à£Œª)P(Y mid lambda)P(Y‚à£Œª).This problem attempt to optimize the model parameters so as to describe the model. | The first and the second problem can be solved by the dynamic programming algorithms known as the Viterbi algorithm and the Forward-Backward algorithm, respectively. The last one can be solved by an iterative Expectation-Maximization (EM) algorithm, known as the Baum-Welch algorithm. We will discuss the first and the second problem in this post. . Solution to Problem 1 . A straight forward way to solve this problem is to find P(Y‚à£S,Œª)P(Y mid S, lambda)P(Y‚à£S,Œª) for fixed state sequences S={s1,...sT}S = {s_1,...s_T }S={s1‚Äã,...sT‚Äã} and then sum up over all possible states. This is generally infeasible since it requires about 2TNT2TN^T2TNT multiplications. Nevertheless, this problem can be efficiently solved by using the forward algorithm as follows: . The forward-backward Algorithm . Let us define the forward variable . Œ±t(i)=P(y1,‚Ä¶yt,st=i‚à£Œª) alpha _t(i)=P(y_1, ldots y_t, s_t=i mid lambda)Œ±t‚Äã(i)=P(y1‚Äã,‚Ä¶yt‚Äã,st‚Äã=i‚à£Œª) . the probability of the partial observation sequences y1‚Ä¶yty_1 ldots y_ty1‚Äã‚Ä¶yt‚Äã up to time ttt and the state st=is_t =ist‚Äã=i at time ttt given the model Œª{ lambda}Œª. We also define an emission probability given HMM state iii at time ttt as bi(yt)b_i(y_t)bi‚Äã(yt‚Äã). . Forward-Algorithm . Initialization . Let . Œ±1(i)=P(y1,s1=i‚à£Œª)=P(y1‚à£s1=i,Œª)P(s1=i‚à£Œª)=œÄibi(y1)¬†for¬†1‚â§i‚â•N begin{aligned} alpha _1(i)&amp;=P(y_1, s_1=i mid lambda) &amp; = P(y_1 mid s_1=i, lambda)P(s_1=i mid lambda) &amp;= pi _i b_i(y_1) text{ for } 1 leq i geq N end{aligned}Œ±1‚Äã(i)‚Äã=P(y1‚Äã,s1‚Äã=i‚à£Œª)=P(y1‚Äã‚à£s1‚Äã=i,Œª)P(s1‚Äã=i‚à£Œª)=œÄi‚Äãbi‚Äã(y1‚Äã)¬†for¬†1‚â§i‚â•N‚Äã . Induction . For t=2,3...Tt=2,3...Tt=2,3...T and 1‚â§i‚â•N1 leq i geq N1‚â§i‚â•N, compute: . Œ±t(i)=P(y1‚Ä¶yt,st=i‚à£Œª)=‚àëj=1NP(y1‚Ä¶yt,st‚àí1=j,st=i‚à£Œª)=‚àëj=1NP(yt‚à£st=i,y1,‚Ä¶yt‚àí1,st‚àí1=j,Œª)√óP(st=i‚à£y1‚Ä¶yt‚àí1‚Ä¶,st‚àí1=j,Œª)√óP(y1‚Ä¶yt‚àí1,st‚àí1=j,Œª)=P(yt‚à£st=i,Œª)‚àëj=1NP(st=i‚à£st‚àí1=j)‚ãÖP(y1,‚Ä¶yt‚àí1,st‚àí1)=bi(yt)‚àëj=1NŒ±t‚àí1(i)aij begin{aligned} alpha _{t}(i) &amp; = P(y_1 ldots y_t, s_t=i mid lambda) &amp;= displaystyle sum_{j=1}^{N} P(y_1 ldots y_{t}, s_{t-1}=j,s_t=i mid lambda) &amp;= displaystyle sum_{j=1}^{N} P(y_t mid s_t=i, y_1, ldots y_{t-1}, s_{t-1}=j, lambda) &amp; times P(s_t=i mid y_1 ldots y_{t-1} ldots , s_{t-1}=j, lambda) &amp; times P(y_1 ldots y_{t-1}, s_{t-1}=j, lambda) &amp; = P(y_t mid s_t=i, lambda) displaystyle sum_{j=1}^{N} P(s_t=i mid s_{t-1}=j) cdot P(y_1, ldots y_{t-1}, s_{t-1}) &amp; = b_i(y_{t}) displaystyle sum_{j=1}^{N} alpha _{t-1}(i)a_{ij} end{aligned}Œ±t‚Äã(i)‚Äã=P(y1‚Äã‚Ä¶yt‚Äã,st‚Äã=i‚à£Œª)=j=1‚àëN‚ÄãP(y1‚Äã‚Ä¶yt‚Äã,st‚àí1‚Äã=j,st‚Äã=i‚à£Œª)=j=1‚àëN‚ÄãP(yt‚Äã‚à£st‚Äã=i,y1‚Äã,‚Ä¶yt‚àí1‚Äã,st‚àí1‚Äã=j,Œª)√óP(st‚Äã=i‚à£y1‚Äã‚Ä¶yt‚àí1‚Äã‚Ä¶,st‚àí1‚Äã=j,Œª)√óP(y1‚Äã‚Ä¶yt‚àí1‚Äã,st‚àí1‚Äã=j,Œª)=P(yt‚Äã‚à£st‚Äã=i,Œª)j=1‚àëN‚ÄãP(st‚Äã=i‚à£st‚àí1‚Äã=j)‚ãÖP(y1‚Äã,‚Ä¶yt‚àí1‚Äã,st‚àí1‚Äã)=bi‚Äã(yt‚Äã)j=1‚àëN‚ÄãŒ±t‚àí1‚Äã(i)aij‚Äã‚Äã . Termination . From Œ±t(i)=P(y1,...yt,st=i‚à£Œª) alpha _t(i)=P(y_1,...y_t, s_t=i mid lambda)Œ±t‚Äã(i)=P(y1‚Äã,...yt‚Äã,st‚Äã=i‚à£Œª), it cear that: . P(Y‚à£Œª)=‚àëi=1NP(y1,‚Ä¶yT,sT=i‚à£Œª)=‚àëi=1NŒ±T(i) begin{aligned} P(Y mid lambda) &amp;= displaystyle sum_{i=1}^{N} P(y_1, ldots y_T, s_T = i mid lambda) &amp;= displaystyle sum_{i=1}^{N} alpha _T(i) end{aligned}P(Y‚à£Œª)‚Äã=i=1‚àëN‚ÄãP(y1‚Äã,‚Ä¶yT‚Äã,sT‚Äã=i‚à£Œª)=i=1‚àëN‚ÄãŒ±T‚Äã(i)‚Äã . The forward algorithm only requires about N2TN^2TN2T multiplications and is it can be implemented in Python as follows. . def forward(obs_seq): T = len(obs_seq) N = A.shape[0] alpha = np.zeros((T, N)) alpha[0] = pi*B[:,obs_seq[0]] for t in range(1, T): alpha[t] = alpha[t-1].dot(A) * B[:, obs_seq[t]] return alpha def likelihood(obs_seq): # returns log P(Y mid model) # using the forward part of the forward-backward algorithm return forward(obs_seq)[-1].sum() . Backward Algorithm . This is the same as the forward algorithm discussed in the previous sectionexcept that it start at the end and works backward toward the beginning. We first define the backward variable Œ≤t(i)=P(yt+1,yt+2‚Ä¶yT‚à£st=i,Œª) beta_t(i)=P(y_{t+1},y_{t+2} ldots y_{T} mid s_t=i, { lambda})Œ≤t‚Äã(i)=P(yt+1‚Äã,yt+2‚Äã‚Ä¶yT‚Äã‚à£st‚Äã=i,Œª): probability of the partial observed sequence from t+1t+1t+1 to the end at TTT given state iii at time ttt and the model Œª lambdaŒª. . Then Œ≤t(i) beta_t(i)Œ≤t‚Äã(i) can be recursively computed as follows. . Initialization . Let Œ≤T(i)=1 beta_{T}(i)= 1Œ≤T‚Äã(i)=1, for 1‚â§i‚â•N1 leq i geq N1‚â§i‚â•N . Induction . For t=T‚àí1,T‚àí2,‚Ä¶1t =T-1, T-2, ldots1t=T‚àí1,T‚àí2,‚Ä¶1 for 1‚â§i‚â•N1 leq i geq N1‚â§i‚â•N and by using the sum and product rules, we can rewrite Œ≤t(j) beta_t(j)Œ≤t‚Äã(j) as: . Œ≤t(i)=P(yt+1,‚Ä¶yT‚à£st=j,Œª)=‚àëi=1NP(yt+1‚Ä¶yT,st+1=i‚à£st=j,Œª)=‚àëi=1NP(yt+1‚Ä¶yT,st+1=i,st=j,Œª)‚ãÖP(st+1=i‚à£st=j)=‚àëi=1NP(yt+2‚Ä¶yT,st+1=i,Œª)‚ãÖP(yt+1‚à£st+1=i,Œª)‚ãÖP(st+1=i‚à£st=j)=‚àëi=1Naijbi(yt+1)Œ≤t+1(i) begin{aligned} beta_t(i)&amp;=P(y_{t+1}, ldots y_{T} mid s_t=j, { lambda}) &amp;= displaystyle sum_{i=1}^{N} P(y_{t+1} ldots y_T, s_{t+1}=i mid s_t=j, lambda) &amp; = displaystyle sum_{i=1}^{N} P(y_{t+1} ldots y_T, s_{t+1}=i, s_t=j, lambda) cdot P(s_{t+1}=i mid s_t=j) &amp;= displaystyle sum_{i=1}^{N} P(y_{t+2} ldots y_T, s_{t+1}=i, lambda) cdot P(y_{t+1} mid s_{t + 1}=i, lambda) cdot P(s_{t+1}=i mid s_t=j) &amp; = displaystyle sum_{i=1}^{N} a_{ij}b_i(y_{t+1}) beta _{t+1}(i) end{aligned}Œ≤t‚Äã(i)‚Äã=P(yt+1‚Äã,‚Ä¶yT‚Äã‚à£st‚Äã=j,Œª)=i=1‚àëN‚ÄãP(yt+1‚Äã‚Ä¶yT‚Äã,st+1‚Äã=i‚à£st‚Äã=j,Œª)=i=1‚àëN‚ÄãP(yt+1‚Äã‚Ä¶yT‚Äã,st+1‚Äã=i,st‚Äã=j,Œª)‚ãÖP(st+1‚Äã=i‚à£st‚Äã=j)=i=1‚àëN‚ÄãP(yt+2‚Äã‚Ä¶yT‚Äã,st+1‚Äã=i,Œª)‚ãÖP(yt+1‚Äã‚à£st+1‚Äã=i,Œª)‚ãÖP(st+1‚Äã=i‚à£st‚Äã=j)=i=1‚àëN‚Äãaij‚Äãbi‚Äã(yt+1‚Äã)Œ≤t+1‚Äã(i)‚Äã . Termination . Œ≤0=P(Y‚à£Œª)=‚àëi=1NP(y1,‚Ä¶yT,s1=i)=‚àëi=1NP(y1,‚Ä¶yT‚à£s1=i)‚ãÖP(s1=i)=‚àëi=1NP(y1‚à£s1=i)‚ãÖP(y2,‚Ä¶yT‚à£s1=i)‚ãÖP(s1=i)=‚àëi=1NœÄibi(y1)Œ≤1(i) begin{aligned} beta_{0} &amp; = P(Y mid lambda) &amp; = displaystyle sum_{i=1}^{N} P(y_1, ldots y_T, s_1=i) &amp;= displaystyle sum_{i=1}^{N} P(y_1, ldots y_T mid s_1=i) cdot P(s_1=i) &amp; = displaystyle sum_{i=1}^{N} P(y_1 mid s_1=i) cdot P(y_2, ldots y_T mid s_1=i) cdot P(s_1=i) &amp; = displaystyle sum_{i=1}^{N} pi _i b_i(y_1) beta _1(i) end{aligned}Œ≤0‚Äã‚Äã=P(Y‚à£Œª)=i=1‚àëN‚ÄãP(y1‚Äã,‚Ä¶yT‚Äã,s1‚Äã=i)=i=1‚àëN‚ÄãP(y1‚Äã,‚Ä¶yT‚Äã‚à£s1‚Äã=i)‚ãÖP(s1‚Äã=i)=i=1‚àëN‚ÄãP(y1‚Äã‚à£s1‚Äã=i)‚ãÖP(y2‚Äã,‚Ä¶yT‚Äã‚à£s1‚Äã=i)‚ãÖP(s1‚Äã=i)=i=1‚àëN‚ÄãœÄi‚Äãbi‚Äã(y1‚Äã)Œ≤1‚Äã(i)‚Äã . Python implementation of the forward algorithm is as shown below; . def backward(obs_seq): N = A.shape[0] T = len(obs_seq) beta = np.zeros((N,T)) beta[:,-1:] = 1 for t in reversed(range(T-1)): for n in range(N): beta[n,t] = np.sum(beta[:,t+1] * A[n,:] * B[:, obs_seq[t+1]]) return beta . Posterior Probability . The forward variable Œ±t(i) alpha _t(i)Œ±t‚Äã(i) and backward variable Œ≤t(i) beta _t(i)Œ≤t‚Äã(i) are used to calculate the posterior probability of a specific case. Now for t=1...Tt=1...Tt=1...T and i=1..Ni=1..Ni=1..N, let define posterior probability Œ≥t(i)=P(st=i‚à£Y,Œª) gamma_t(i)=P(s_t=i mid Y, lambda)Œ≥t‚Äã(i)=P(st‚Äã=i‚à£Y,Œª) the probability of being in state st=is_t = ist‚Äã=i at time ttt given the observation YYY and the model Œª lambdaŒª. . Œ≥t(i)=P(st=1,Y‚à£Œª)P(Y‚à£Œª)=P(y1,‚Ä¶yt,st=1,‚à£Œª)P(Y‚à£Œª) begin{aligned} gamma_t(i) &amp; = frac{P(s_t=1, Y mid lambda)}{P(Y mid lambda)} &amp;= frac{P(y_1, ldots y_t, s_t=1, mid lambda)}{P(Y mid lambda)} end{aligned}Œ≥t‚Äã(i)‚Äã=P(Y‚à£Œª)P(st‚Äã=1,Y‚à£Œª)‚Äã=P(Y‚à£Œª)P(y1‚Äã,‚Ä¶yt‚Äã,st‚Äã=1,‚à£Œª)‚Äã‚Äã . Consider: . P(y1,‚Ä¶yt,st=1,‚à£Œª)=P(y1,‚Ä¶yt‚à£st=1,Œª)‚ãÖP(yt+1,‚Ä¶yT‚à£st=1,Œª)‚ãÖP(st=i‚à£Œª)=Œ±t(i)‚ãÖŒ≤t(i) begin{aligned} P(y_1, ldots y_t, s_t=1, mid lambda) &amp; = P(y_1, ldots y_t mid s_t=1, lambda) cdot P(y_{t+1}, ldots y_T mid s_t=1, lambda) cdot P(s_t =i mid lambda) &amp; = alpha _t(i) cdot beta _t(i) end{aligned}P(y1‚Äã,‚Ä¶yt‚Äã,st‚Äã=1,‚à£Œª)‚Äã=P(y1‚Äã,‚Ä¶yt‚Äã‚à£st‚Äã=1,Œª)‚ãÖP(yt+1‚Äã,‚Ä¶yT‚Äã‚à£st‚Äã=1,Œª)‚ãÖP(st‚Äã=i‚à£Œª)=Œ±t‚Äã(i)‚ãÖŒ≤t‚Äã(i)‚Äã . Thus . Œ≥t(i)=Œ±t(i)‚ãÖŒ≤t(i)P(Y‚à£Œª) gamma_t(i) = frac{ alpha _t(i) cdot beta _t(i)}{P(Y mid lambda)}Œ≥t‚Äã(i)=P(Y‚à£Œª)Œ±t‚Äã(i)‚ãÖŒ≤t‚Äã(i)‚Äã . where . P(Y‚à£Œª)=‚àëi=1NŒ±T(i)P(Y mid { lambda}) = displaystyle sum_{i=1}^{N} alpha _T(i)P(Y‚à£Œª)=i=1‚àëN‚ÄãŒ±T‚Äã(i) . In python: . def gamma(obs_seq): alpha = forward(obs_seq) beta = backward(obs_seq) obs_prob = likelihood(obs_seq) return (np.multiply(alpha,beta.T) / obs_prob) . We can use Œ≥t(i) gamma_t(i)Œ≥t‚Äã(i) to find the most likely state at time ttt which is the state st=is_t=ist‚Äã=i for which Œ≥t(i) gamma_t(i)Œ≥t‚Äã(i) is maximum. This algorithm works fine in the case when HMM is ergodic i.e., there is the transition from any state to any other state. If applied to an HMM of another architecture, this approach could give a sequence that may not be a legitimate path because some transitions are not permitted. To avoid this problem, Viterbi algorithm is the most common decoding algorithm used. . Viterbi Algorithm . Viterbi is a kind of dynamic programming algorithm that makes uses of a dynamic programming trellis. . The virtebi algorithm offer an efficient way of finding the single best state sequence.Let define the highest probability along a single path, at time ttt, which accounts for the first ttt observations and ends in state jjj using a new notation: . Œ¥t(i)=max‚Å°s1,‚Ä¶st‚àí1P(s1,‚Ä¶st=1,y1,‚Ä¶yt‚à£Œª) begin{aligned} delta_t(i) &amp; = max_{s_1, ldots s_{t-1}} P(s_1, ldots s_t =1, y_1, ldots y_t mid lambda) end{aligned}Œ¥t‚Äã(i)‚Äã=s1‚Äã,‚Ä¶st‚àí1‚Äãmax‚ÄãP(s1‚Äã,‚Ä¶st‚Äã=1,y1‚Äã,‚Ä¶yt‚Äã‚à£Œª)‚Äã . By induction, a recursive formula of Œ¥t+1(i) delta_{t+1}(i)Œ¥t+1‚Äã(i) from Œ¥t(i) delta_t(i)Œ¥t‚Äã(i) is derived to calculate this probability as follows: . Consider the joint distribution appearing in Œ¥t+1(i) delta_{t+1}(i)Œ¥t+1‚Äã(i), which can be rewritten when st+1=is_{t+1}=ist+1‚Äã=i and st=js_t = jst‚Äã=j as: . P(s1,‚Ä¶,st=j,st+1=i,y1,‚Ä¶yt,yt+1‚à£Œª)=P(s1‚Ä¶st=j,y1,‚Ä¶yt‚à£Œª)√óP(st+1=i,yt+1‚à£s1,‚Ä¶st,y1,‚Ä¶yt,Œª)=P(s1‚Ä¶st=j,y1,‚Ä¶yt‚à£Œª)‚ãÖP(st+1‚à£st,Œª)√óP(yt+1‚à£st+1,Œª)=P(s1‚Ä¶st=j,y1,‚Ä¶yt‚à£Œª)‚ãÖaijbi(yt+1) begin{aligned} P(s_1, ldots, s_t=j,s_{t+1}=i, y_1, ldots y_t, y_{t+1} mid lambda) &amp; = P(s_1 ldots s_t=j, y_1, ldots y_t mid lambda) &amp; times P(s_{t+1}=i,y_{t+1} mid s_1, ldots s_t, y_1, ldots y_t, lambda) &amp; = P(s_1 ldots s_t=j, y_1, ldots y_t mid lambda) cdot P(s_{t+1} mid s_t, lambda) &amp; times P(y_{t+1} mid s_{t+1}, lambda) &amp; = P(s_1 ldots s_t=j, y_1, ldots y_t mid lambda) cdot a_{ij}b_i(y_{t+1}) end{aligned}P(s1‚Äã,‚Ä¶,st‚Äã=j,st+1‚Äã=i,y1‚Äã,‚Ä¶yt‚Äã,yt+1‚Äã‚à£Œª)‚Äã=P(s1‚Äã‚Ä¶st‚Äã=j,y1‚Äã,‚Ä¶yt‚Äã‚à£Œª)√óP(st+1‚Äã=i,yt+1‚Äã‚à£s1‚Äã,‚Ä¶st‚Äã,y1‚Äã,‚Ä¶yt‚Äã,Œª)=P(s1‚Äã‚Ä¶st‚Äã=j,y1‚Äã,‚Ä¶yt‚Äã‚à£Œª)‚ãÖP(st+1‚Äã‚à£st‚Äã,Œª)√óP(yt+1‚Äã‚à£st+1‚Äã,Œª)=P(s1‚Äã‚Ä¶st‚Äã=j,y1‚Äã,‚Ä¶yt‚Äã‚à£Œª)‚ãÖaij‚Äãbi‚Äã(yt+1‚Äã)‚Äã . Thus Œ¥t+1(i) delta_{t+1}(i)Œ¥t+1‚Äã(i) is computed recursively from Œ¥t+1(j) delta_{t+1}(j)Œ¥t+1‚Äã(j) as: . Œ¥t+1(i)=max‚Å°s1,‚Ä¶st=jP(s1‚Ä¶st=j,y1,‚Ä¶yt‚à£Œª)‚ãÖaijbi(yt+1)=max‚Å°j[Œ¥t(j)aij]‚ãÖbi(yt+1) begin{aligned} delta_{t+1}(i) &amp;= max_{s_1, ldots s_{t}=j} P(s_1 ldots s_t=j, y_1, ldots y_t mid lambda) cdot a_{ij}b_i(y_{t+1}) &amp; = max_{j} Big[ delta_t(j) a_{ij} Big] cdot b_i(y_{t+1}) end{aligned}Œ¥t+1‚Äã(i)‚Äã=s1‚Äã,‚Ä¶st‚Äã=jmax‚ÄãP(s1‚Äã‚Ä¶st‚Äã=j,y1‚Äã,‚Ä¶yt‚Äã‚à£Œª)‚ãÖaij‚Äãbi‚Äã(yt+1‚Äã)=jmax‚Äã[Œ¥t‚Äã(j)aij‚Äã]‚ãÖbi‚Äã(yt+1‚Äã)‚Äã . Therefore, we need to keep track of the state that maximizes the above equation to backtrack to the single best state sequence in the following Viterbi algorithm: . Initialization . For 1‚â§i‚â•N1 leq i geq N1‚â§i‚â•N, let: . Œ¥1(i)=œÄsibi(y1)Œò1(i)=0 begin{aligned} delta _1(i)&amp;= pi _{s_i}b_i(y_1) Theta _1(i)&amp;=0 end{aligned}Œ¥1‚Äã(i)Œò1‚Äã(i)‚Äã=œÄsi‚Äã‚Äãbi‚Äã(y1‚Äã)=0‚Äã . Recursion . Calculate the ML (maximum likelihood) state sequences and their probabilities. For t=2,3,...Tt=2,3,...Tt=2,3,...T and 1‚â§i‚â•N1 leq i geq N1‚â§i‚â•N . Œ¥t(i)=max‚Å°jœµ1,..N[Œ¥t‚àí1(j)aij]‚ãÖbi(yt)Œòt(i)=arg‚Å°max‚Å°j[Œ¥t‚àí1(j)aij] begin{aligned} delta_t(i) &amp; = displaystyle max_{j epsilon{1,..N}} Big[ delta_{t-1}(j)a_{ij} Big] cdot b_i(y_t) Theta_t(i) &amp; = arg max_j Big[ delta_{t-1}(j)a_{ij} Big] end{aligned}Œ¥t‚Äã(i)Œòt‚Äã(i)‚Äã=jœµ1,..Nmax‚Äã[Œ¥t‚àí1‚Äã(j)aij‚Äã]‚ãÖbi‚Äã(yt‚Äã)=argjmax‚Äã[Œ¥t‚àí1‚Äã(j)aij‚Äã]‚Äã . Termination: . Retrieve the most likely final state . P^=max‚Å°jœµ1,..N[Œ¥T(j)]S^T=arg‚Å°max‚Å°j[Œ¥T(j)] begin{aligned} hat{P} &amp;= displaystyle max_{j epsilon{1,..N}}[ delta_T(j)] hat{S}_T &amp; = arg max_j [ delta_T(j)] end{aligned}P^S^T‚Äã‚Äã=jœµ1,..Nmax‚Äã[Œ¥T‚Äã(j)]=argjmax‚Äã[Œ¥T‚Äã(j)]‚Äã . State sequence backtracking: . Retrieve the most likely state sequences (Viterbi path) . S^t=Œòt+1(S^t+1),¬†where¬†t=T‚àí1,T‚àí2,‚Ä¶1 hat{S}_t = Theta_{t+1}( hat{S}_{t+1}) text{, where } t=T-1,T-2, ldots1S^t‚Äã=Œòt+1‚Äã(S^t+1‚Äã),¬†where¬†t=T‚àí1,T‚àí2,‚Ä¶1 . The Viterbi algorithm uses the same schema as the Forward algorithm except for two differences: . It uses maximization in place of summation at the recursion and termination steps. | It keeps track of the arguments that maximize Œ¥t(i) delta_t(i)Œ¥t‚Äã(i) for each ttt and iii, storing them in the N by T matrix Œò ThetaŒò. This matrix is used to retrieve the optimal state sequence at the backtracking step. | Python implementation of virtebi algorithm . def viterbi(obs_seq): # returns the most likely state sequence given observed sequence x # using the Viterbi algorithm T = len(obs_seq) N = A.shape[0] delta = np.zeros((T, N)) psi = np.zeros((T, N)) delta[0] = pi*B[:,obs_seq[0]] for t in range(1, T): for j in range(N): delta[t,j] = np.max(delta[t-1]*A[:,j]) * B[j, obs_seq[t]] psi[t,j] = np.argmax(delta[t-1]*A[:,j]) # backtrack states = np.zeros(T, dtype=np.int32) states[T-1] = np.argmax(delta[T-1]) for t in range(T-2, -1, -1): states[t] = psi[t+1, states[t+1]] return states . To summarize, we can compute the following from HMM: . The marginalized likelihood function P(Y‚à£Œª)P(Y mid lambda)P(Y‚à£Œª) from the forward or backward algorithm. | The posterior probability Œ≥t(i)=P(st=i‚à£Y,Œª) gamma_t(i) = P(s_t=i mid Y, lambda)Œ≥t‚Äã(i)=P(st‚Äã=i‚à£Y,Œª) from the forward‚Äìbackward algorithm. | The optimal state sequence S^=max‚Å°sP(S‚à£Y,Œª)=max‚Å°sP(S,Y‚à£Œª) hat{S} = max_{s} P(S mid Y, lambda) = max_{s} P(S, Y mid lambda)S^=maxs‚ÄãP(S‚à£Y,Œª)=maxs‚ÄãP(S,Y‚à£Œª)from the Viterbi algorithm. | The segmental joint likelihood function P(S^,Y‚à£Œª)P( hat{S},Y mid lambda)P(S^,Y‚à£Œª) from the Viterbi algorithm. | These values are used in the decoding step, and the training step of estimating model parameters Œª lambdaŒª. . Example 1 . Consider the Bob-Alice example as described here. Two friends, Alice and Bob, who live far apart and talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no specific information about the weather where Bob lives, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been. . Alice believes that the weather operates as a discrete Markov chain. There are two states, ‚ÄúRainy‚Äù and ‚ÄúSunny‚Äù, but she cannot observe them directly; that is, they are hidden from her. On each day, there is a pure chance that Bob will perform one of the following activities, depending on the weather: ‚Äúwalk‚Äù, ‚Äúshop‚Äù, or ‚Äúclean‚Äù. Since Bob tells Alice about his actions, those are the observations. . states = (&#39;Rainy&#39;, &#39;Sunny&#39;) observations = (&#39;walk&#39;, &#39;shop&#39;, &#39;clean&#39;) pi = np.array([0.6, 0.4]) #initial probability A = np.array([[0.7, 0.3],[0.4, 0.6]]) #Transmission probability B = np.array([[0.1, 0.4, 0.5],[0.6, 0.3, 0.1]]) #Emission probability . Suppose Bob says walk, clean, shop, shop, clean, walk. What will Alice hears. . bob_says = np.array([0, 2, 1, 1, 2, 0]) alice_hears=viterbi(bob_says) print(&quot;Bob says:&quot;, &quot;, &quot;,list(map(lambda y: observ_bob[y], bob_says))) print(&quot;Alice hears:&quot;, &quot;, &quot;, list(map(lambda s: states_bob[s], alice_hears))) . (&#39;Bob says:&#39;, &#39;walk, clean, shop, shop, clean, walk&#39;) (&#39;Alice hears:&#39;, &#39;Sunny, Rainy, Rainy, Rainy, Rainy, Sunny&#39;) The notebook with codes for the above example can be found in here . References . L. R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proceedings of the IEEE, Vol. 77, No. 2, February 1989. | Shinji Watanabe, Jen-Tzung Chien, Bayesian Speech and Language Processing, Cambridge University Press, 2015. | Viterbi Algorithm in Speech Enhancement and HMM | Nikolai Shokhirev, Hidden Markov Models |",
            "url": "https://sambaiga.github.io/sambaiga/machine%20learning/2017/05/03/hmm-intro.html",
            "relUrl": "/machine%20learning/2017/05/03/hmm-intro.html",
            "date": " ‚Ä¢ May 3, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Anthony Faustine is a Data Scientist at CeADAR (UCD), Dublin, Ireland with over four years of successful experience in data analytics and Artificial Intelligence techniques for multiple applications. He effectively researches techniques for novel approaches to problems and develops prototypes to assess their viability. Although Anthony is a person who takes the initiative, he has a strong team-work spirit with experience of working in a highly international environment. . At CeADER, Anthony is devising and implementing data analytics/AI technical solutions for multiple application domains. He is also involved in the research and development of the applicability of Artificial Intelligence for Earth Observation (AI4EO). . Mr Faustine, received the B.sc. Degree in Electronics Science and Communication from the University of Dar es Salaam, Tanzania, and the M.sc. Degree in Telecommunications Engineering from the University of Dodoma, Tanzania, in 2010. From 2010 to 2017, he worked as an assistant lecturer at the University of Dodoma, Tanzania, where he was involved in several research projects within the context of ICT4D. . In 2017, Anthony joined IDLab, imec research group of the University of Ghent, in Belgium as a Ph.D. Machine learning researcher advised by Tom Dhaene and Dirk Deschrijver. His research focused on machine learning techniques applied to energy smart-meter data. He develops methods to identify active appliances and extract their corresponding power consumption from aggregate power (energy-disaggregation) in residential and industrial buildings. . His research interests lie in the intersections between Computational Sustainability and Artificial Intelligence. He works towards bridging the gap between research and real-world applicability of machine learning for sustainable development. .",
          "url": "https://sambaiga.github.io/sambaiga/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "",
          "url": "https://sambaiga.github.io/sambaiga/project/",
          "relUrl": "/project/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Resources",
          "content": "",
          "url": "https://sambaiga.github.io/sambaiga/resources.html",
          "relUrl": "/resources.html",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
      ,"page9": {
          "title": "Talk",
          "content": "",
          "url": "https://sambaiga.github.io/sambaiga/talks/",
          "relUrl": "/talks/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://sambaiga.github.io/sambaiga/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}