<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://sambaiga.github.io/sambaiga/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sambaiga.github.io/sambaiga/" rel="alternate" type="text/html" /><updated>2020-03-09T16:48:07-05:00</updated><id>https://sambaiga.github.io/sambaiga/feed.xml</id><title type="html">sambaiga</title><entry><title type="html">Vectorization and Distribution shapes in Pyro</title><link href="https://sambaiga.github.io/sambaiga/jupyter/2020/03/04/ppl-pyro-two.html" rel="alternate" type="text/html" title="Vectorization and Distribution shapes in Pyro" /><published>2020-03-04T00:00:00-06:00</published><updated>2020-03-04T00:00:00-06:00</updated><id>https://sambaiga.github.io/sambaiga/jupyter/2020/03/04/ppl-pyro-two</id><content type="html" xml:base="https://sambaiga.github.io/sambaiga/jupyter/2020/03/04/ppl-pyro-two.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-03-04-ppl-pyro-two.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;Introduction&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In the previous post we introduced pyro and its building blocks such as schotastic function, primitive sample and param primitive statement, model and guide. We also defined pyro model and use it to generate data, learn from data and predict future observations.&lt;/p&gt;
&lt;p&gt;In this section, we will learn in details about inference in Pyro, how to use Pyro primitives and the effect handling library (pyro.poutine) to build custom tools for analysis.&lt;/p&gt;
&lt;p&gt;Consider a previous poison regression model&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro.distributions&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dist&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributions&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constraints&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_rng_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manual_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;matplotlib&lt;/span&gt; inline
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;slope&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;intercept&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;count_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Poisson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Plate-statement&quot;&gt;Plate statement&lt;a class=&quot;anchor-link&quot; href=&quot;#Plate-statement&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;From the given  model above , &lt;strong&gt;pyro.param&lt;/strong&gt; designate model parameters that we would like to optimize. Observations are denoted by the obs= keyword argument to pyro.sample. This specifies the likelihood function. Instead of log transforming the data, we use a LogNormal distribution. The observations are conditionally independent given the latent random variable slope and intercept. To explicitly mark this in Pyro, &lt;strong&gt;plate&lt;/strong&gt; statement is used to construct conditionally independent sequences of variables.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subsample_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# ...do conditionally independent stuff with ind...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However compared to &lt;code&gt;range()&lt;/code&gt; each invocation of &lt;strong&gt;plate&lt;/strong&gt; requires the user to provide a unique name. The &lt;strong&gt;plate&lt;/strong&gt;  statement can be used either sequentially as a generator or in parallel as a context manager. Sequential plate is similar to &lt;code&gt;range()&lt;/code&gt;in that it generates a sequence of values.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# This version declares sequential independence and subsamples data:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subsample_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
         &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Control flow in this example prevents vectorization.&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;obs_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Vectorized plate is similar to &lt;code&gt;torch.arange()&lt;/code&gt; in that it yields an array of indices by which other tensors can be indexed. However, unlike  &lt;code&gt;torch.arange()&lt;/code&gt; &lt;strong&gt;plate&lt;/strong&gt;  also informs inference algorithms that the variables being indexed are conditionally independent.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# This version declares vectorized independence:&lt;/span&gt;
     &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;obs&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Additionally, plate can take advantage of the conditional independence assumptions by subsampling the indices and informing inference algorithms to scale various computed values. This is typically used to subsample minibatches of data:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subsample_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;You can additionally nest plates, e.g. if you have per-pixel independence:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;x_axis&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;320&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# within this context, batch dimension -1 is independent&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;y_axis&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# within this context, batch dimensions -2 and -1 are independent&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finaly you can declare multiple plates and use them as reusable context managers. For example if you want to mix and match plates for e.g. noise that depends only on x, some noise that depends only on y, and some noise that depends on both&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;x_axis&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;y_axis&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# within this context, batch dimension -2 is independent&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# within this context, batch dimension -3 is independent&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# within this context, batch dimensions -3 and -2 are independent&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;slope&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;intercept&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;                        
        &lt;span class=&quot;n&quot;&gt;log_y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LogNormal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Distribution-shapes&quot;&gt;Distribution shapes&lt;a class=&quot;anchor-link&quot; href=&quot;#Distribution-shapes&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Unlike PyTorch Tensors which have  a single .shape attribute, pyro Distributions have two shape &lt;strong&gt;batch_shape&lt;/strong&gt; and &lt;strong&gt;event_shape&lt;/strong&gt;. These two combine to define the total shape of a sample. The batch_shape denote conditionally independent random variables, whereas .event_shape denote dependent random variables (ie one draw from a distribution). Because the dependent random variables define probability together, the .log_prob() method only produces a single number for each event of shape .event_shape.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Bernoulli&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;event_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;torch.Size([])
torch.Size([])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Distributions can be batched by passing in batched parameters.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Bernoulli&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;event_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;torch.Size([50])
torch.Size([])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([50])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;From the two examples above, we observe that univariate distributions have empty event shape (because each number is an independent event). Let also consider multivariate distribution.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MultivariateNormal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;event_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;torch.Size([])
torch.Size([3])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([3])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can also create batched multivariate distribution as follows.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MultivariateNormal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;event_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;torch.Size([50])
torch.Size([3])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([50, 3])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Because Multivariate distributions have nonempty &lt;strong&gt;.event_shape&lt;/strong&gt;, the shapes of .sample() and .log_prob(x) differ:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([50])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;strong&gt;Distribution.sample()&lt;/strong&gt; method also takes a sample_shape parameter that indexes over independent identically   distributed (iid) random varables, such that:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;event_shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_sample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([10, 50, 3])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Reshaping-distributions&quot;&gt;Reshaping distributions&lt;a class=&quot;anchor-link&quot; href=&quot;#Reshaping-distributions&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;You can treat a univariate distribution as multivariate by calling the &lt;code&gt;.to_event(n)&lt;/code&gt; property where &lt;strong&gt;n&lt;/strong&gt; is the number of batch dimensions (from the right) to declare as dependent.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Bernoulli&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_event&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;event_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;torch.Size([50])
torch.Size([3])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;While working with distributions in pyro it is essential to note that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Samples have shape batch_shape + event_shape, &lt;/li&gt;
&lt;li&gt;&lt;code&gt;.log_prob(x)&lt;/code&gt; values have shape batch_shape. &lt;/li&gt;
&lt;li&gt;You’ll need to ensure that &lt;code&gt;batch_shape&lt;/code&gt; is carefully controlled by either trimming it down with &lt;code&gt;.to_event(n)&lt;/code&gt; or by declaring dimensions as independent via &lt;code&gt;pyro.plate&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Often in Pyro we’ll declare some dimensions as dependent even though they are in fact independent. This allows us to easily swap in a MultivariateNormal distribution later, but aslo it simplifies the code as  we don’t need a plate. Consider the following two codes&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;x&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_event&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([10])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;y_plate&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# .expand([10]) is automatic&lt;/span&gt;
  
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([10])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;From the two code examples, the second version with plate informs Pyro that it can make use of conditional independence information when estimating gradients, whereas in the first version Pyro must assume they are dependent (even though the normals are in fact conditionally independent).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sambaiga.github.io/sambaiga/images/ppl-pyro-intro.png" /><media:content medium="image" url="https://sambaiga.github.io/sambaiga/images/ppl-pyro-intro.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Probabilistic Programming with Pyro</title><link href="https://sambaiga.github.io/sambaiga/jupyter/2020/03/01/ppl-pyro-intro.html" rel="alternate" type="text/html" title="Probabilistic Programming with Pyro" /><published>2020-03-01T00:00:00-06:00</published><updated>2020-03-01T00:00:00-06:00</updated><id>https://sambaiga.github.io/sambaiga/jupyter/2020/03/01/ppl-pyro-intro</id><content type="html" xml:base="https://sambaiga.github.io/sambaiga/jupyter/2020/03/01/ppl-pyro-intro.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-03-01-ppl-pyro-intro.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Intro-to-Pyro&quot;&gt;Intro to Pyro&lt;a class=&quot;anchor-link&quot; href=&quot;#Intro-to-Pyro&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://pyro.ai/&quot;&gt;Pyro&lt;/a&gt; is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. It enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling.&lt;/p&gt;
&lt;h3 id=&quot;Models-and-Probability-distributions&quot;&gt;Models and Probability distributions&lt;a class=&quot;anchor-link&quot; href=&quot;#Models-and-Probability-distributions&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Models&lt;/strong&gt; are the basic unit of probabilistic programs in pyro, they represent simplified or abstract descriptions of a process by which data are generated. Models in pyro are expressed as &lt;em&gt;stochastic functions&lt;/em&gt; which implies that models can be composed, reused, imported, and serialized just like regular Python callables. &lt;strong&gt;Probability distributions&lt;/strong&gt; (pimitive stochastic functions) are important class of models (stochastic functions) used explicitly to compute the probability of the outputs given the inputs.  Pyro uses &lt;a href=&quot;https://pytorch.org/docs/master/distributions.html&quot;&gt;PyTorch’s distribution library&lt;/a&gt; which contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. Each probability distributions are equipped with several  methods such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;prob()&lt;/strong&gt;: $\log p(\mathbf{x} \mid \theta ^{*})$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mean&lt;/strong&gt;: $\mathbb{E}_{p(\mathbf{x} \mid \theta ^{*})}[\mathbf{x}]$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sample&lt;/strong&gt;: $\mathbf{x}^{*} \sim  {p(\mathbf{x} \mid \theta ^{*})}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also create custom distributions using &lt;a href=&quot;https://pytorch.org/docs/master/distributions.html#module-torch.distributions.transforms&quot;&gt;transforms&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;: Let define the unit normal distribution $\mathcal{N}(0,1)$, draw  sample $x$ and  compute the log probability according to the distribution.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro.distributions&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dist&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributions&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constraints&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_rng_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manual_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_printoptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;matplotlib&lt;/span&gt; inline
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# draw a sample from N(1,1)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;sample&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#To compute the log probability according to the distribution&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;prob&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# score the sample from N(1,1)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;sample -1.3905061483383179
prob 0.15172401070594788
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Sample-and-Param-statements&quot;&gt;Sample and Param statements&lt;a class=&quot;anchor-link&quot; href=&quot;#Sample-and-Param-statements&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Pyro simplifies the process of sampling from distributions with the use of  &lt;strong&gt;pyro.sample&lt;/strong&gt; statement. The &lt;strong&gt;pyro.sample&lt;/strong&gt; statement  call stochastic functions or models with a &lt;em&gt;unique name&lt;/em&gt; as identifier. Pyro’s backend uses these names to uniquely identify sample statements and change their behavior at runtime depending on how the enclosing stochastic function is being used. Using &lt;strong&gt;pyro.sample&lt;/strong&gt; statement, Pyro can implement  various manipulations that underlie inference algorithms.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;name – name of sample&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;fn – distribution class or function&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;obs – observed datum (optional; should only be used in context of inference) optionally specified in kwargs&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;: Let sample from previous normal distribution created in example 1.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;my_sample&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;tensor(-0.815)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The above code generate a random value and records it in the Pyro runtime.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;my_sample&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;2
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;/opt/miniconda3/lib/python3.7/site-packages/pyro/primitives.py:86: RuntimeWarning: trying to observe a value outside of inference at my_sample
  RuntimeWarning)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The above code conditions a stochatsic function on observed data. This should run on inference.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Pyro use  &lt;strong&gt;pyro.param&lt;/strong&gt; statement to  saves the variable as a parameter in the param store. To interact with the param store. The &lt;strong&gt;pyro.param&lt;/strong&gt; statement is used by pyro to  declares a learnable parameter.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constraints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;name – name of param&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;init_value – initial value&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;constraint – torch constraint&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Example 3&lt;/strong&gt;: Let create theta parameter&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;theta&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constraint&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constraints&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;positive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Simple-PPL-model&quot;&gt;Simple PPL model&lt;a class=&quot;anchor-link&quot; href=&quot;#Simple-PPL-model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Consider the following Poison Regression model
\begin{align}
y(t) &amp;amp;\sim \lambda \exp(-\lambda)\\
\lambda &amp;amp;\sim \exp(c + m(t))\\
c &amp;amp;\sim \mathcal{N}(1, 1)\\
m &amp;amp;\sim \mathcal{N}(0, 1)
\end{align}&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;slope&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;intercept&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;count_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Poisson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Given a pyro model.
We can&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Generate data from model&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn parameters of the model from data&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use the model to predict future observation&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Generate-data-from-model&quot;&gt;Generate data from model&lt;a class=&quot;anchor-link&quot; href=&quot;#Generate-data-from-model&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Running a Pyro model will generate a sample from the prior.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_rng_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# We pass counts = [None, ..., None] to indicate time duration.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;true_slope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true_intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true_counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lineplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Learn-parameters-of-the-model-from-data&quot;&gt;Learn parameters of the model from data&lt;a class=&quot;anchor-link&quot; href=&quot;#Learn-parameters-of-the-model-from-data&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;To learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data. Inference algorithms in Pyro us arbitrary stochastic functions as approximate posterior distributions. that s. These functions are  called guide functions or &lt;strong&gt;guides&lt;/strong&gt;  and contains &lt;strong&gt;pyro.sample&lt;/strong&gt; and &lt;strong&gt;pyro.param&lt;/strong&gt; statement. It is a stochastic function that represents a probability distribution over the latent (unobserved) variables. The guide can be arbitrary python code just like the model, but with a few requirements:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;All unobserved sample statements that appear in the model appear in the guide. &lt;/li&gt;
&lt;li&gt;The guide has the same input signature as the model (i.e. takes the same arguments).&lt;/li&gt;
&lt;li&gt;There are no pyro.sample statements with the obs keyword in the guide. These are exclusive to the model.&lt;/li&gt;
&lt;li&gt;There are pyro.param statements, which are exclusive to the guide. These provide differentiation for the inputs to the pay_probs sample in the guide vs. the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example if the model contains a random variable &lt;em&gt;z_1&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;z_1&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;then the guide needs to have a matching sample statement&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;guide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;z_1&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Once a guide has been specified, we can then perform learning and inference which is an optimization problem of maximizing the evidence lower bound (ELBO). The ELBO, is a function of both $\theta$ and $\phi$, defined as an expectation w.r.t. to samples from the guide:&lt;/p&gt;
$${\rm ELBO} \equiv \mathbb{E}_{q_{\phi}({\bf z})} \left [
\log p_{\theta}({\bf x}, {\bf z}) - \log q_{\phi}({\bf z})
\right]$$&lt;p&gt;The &lt;strong&gt;SVI&lt;/strong&gt; class is unified interface for stochastic variational inference in Pyro. To use this class you need to provide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the model, &lt;/li&gt;
&lt;li&gt;the guide, and an &lt;/li&gt;
&lt;li&gt;optimizer which is a wrapper a for a PyTorch optimizer as discusseced in below&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro.infer&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Trace_ELBO&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;svi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;guide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Trace_ELBO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The SVI object provides two methods, &lt;strong&gt;step()&lt;/strong&gt; and &lt;strong&gt;evaluate_loss()&lt;/strong&gt;,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The method step() takes a single gradient step and returns an estimate of the loss (i.e. minus the ELBO). &lt;/li&gt;
&lt;li&gt;The method evaluate_loss() returns an estimate of the loss without taking a gradient step.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both of these methods  accept an optional argument: &lt;strong&gt;num_particles&lt;/strong&gt;, which denotes the number of samples used to compute the loss  and gradient.&lt;/p&gt;
&lt;p&gt;The module &lt;strong&gt;pyro.optim&lt;/strong&gt; provides support for optimization in Pyro. In particular it provides &lt;strong&gt;PyroOptim&lt;/strong&gt;, which is used to wrap PyTorch optimizers and manage optimizers for dynamically generated parameters. &lt;strong&gt;PyroOptim&lt;/strong&gt; takes two arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a constructor for PyTorch optimizers &lt;em&gt;optim_constructor&lt;/em&gt; and &lt;/li&gt;
&lt;li&gt;a specification of the optimizer &lt;em&gt;arguments optim_args&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro.optim&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;adam_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;lr&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;betas&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.999&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adam_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Thus to learn model parameters we pass the model to an inference algorithm and let the algorithm guess what the model is doing based on observed data (here true_counts).&lt;/p&gt;
&lt;p&gt;For the above example we will use Autoguide pyro inference algorithm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AutoLaplaceApproximation:Laplace approximation (quadratic approximation) approximates the posterior log𝑝(𝑧|𝑥) by a multivariate normal distribution in the unconstrained space.&lt;/li&gt;
&lt;li&gt;Autodelta: This implementation of AutoGuide uses Delta distributions to construct a MAP guide over the entire latent space. &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro.infer.autoguide&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoDelta&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro.infer&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Trace_ELBO&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro.optim&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;guide&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoDelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;svi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;guide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;lr&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Trace_ELBO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;svi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# true_counts is passed as argument to model()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;loss = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;loss = 87295.88946688175
loss = 64525.8595520854
loss = 80838.8460238576
loss = 33014.93229973316
loss = 13704.865498423576
loss = 6232.828522503376
loss = 2017.9879159331322
loss = 631.3558134436607
loss = 170.10323333740234
loss = 198.57187271118164
loss = 207.4590385556221
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;true_slope = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_slope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;true_intercept = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;guess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;guide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;guess = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;guess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;true_slope = 0.15409961342811584
true_intercept = -0.293428897857666
guess = {&amp;#39;slope&amp;#39;: tensor(0.147, grad_fn=&amp;lt;ExpandBackward&amp;gt;), &amp;#39;intercept&amp;#39;: tensor(-0.054, grad_fn=&amp;lt;ExpandBackward&amp;gt;)}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Use-model-to-predict-future-observation&quot;&gt;Use model to predict future observation&lt;a class=&quot;anchor-link&quot; href=&quot;#Use-model-to-predict-future-observation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;A third way to use a Pyro model is to predict new observed data by guiding the model. This uses two of Pyro's effects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;trace records guesses made by the guide, and&lt;/li&gt;
&lt;li&gt;replay conditions the model on those guesses, allowing the model to generate conditional samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Traces&lt;/strong&gt; are directed graphs whose nodes represent primitive calls or input/output, and whose edges represent conditional dependence relationships between those primitive calls. It return a handler that records the inputs and outputs of primitive calls and their dependencies.&lt;/p&gt;
&lt;p&gt;We can record its execution using trace and use the resulting data structure to compute the log-joint probability of all of the sample sites in the execution or extract all parameters.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyro&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poutine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pprint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s1&quot;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;s1&quot;&gt;&amp;#39;prob&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;fn&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nodes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;props&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;type&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;sample&amp;#39;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;{&amp;#39;intercept&amp;#39;: {&amp;#39;prob&amp;#39;: tensor(0.250), &amp;#39;value&amp;#39;: tensor(-0.966)},
 &amp;#39;slope&amp;#39;: {&amp;#39;prob&amp;#39;: tensor(2.818), &amp;#39;value&amp;#39;: tensor(0.083)}}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_prob_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;tensor(0.705)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here, the trace feature will collect values every time they are sampled with sample and store them with the corresponding string name (that’s why we give each sample a name). With a little cleanup, we can print out the value and probability of each random variable’s value, along with the joint probability of the entire trace.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Replay&lt;/strong&gt; return a callable that runs the original, reusing the values at sites in trace at those sites in the new trace.  makes sample statements behave as if they had sampled the values at the corresponding sites in the trace&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyro&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;poutine&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forecast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forecast_steps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true_counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forecast_steps&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# observed data + blanks to fill in&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;guide_trace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;poutine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;guide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;poutine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;guide_trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counts&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can now call forecast() multiple times to generate samples.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;full_counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forecast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;forecast_counts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;forecast&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;k-&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;truth&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Pyro-ducomentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/pyro-ppl/sandbox/&quot;&gt;PPL models for timeseries forecasting&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sambaiga.github.io/sambaiga/images/ppl-pyro-intro.png" /><media:content medium="image" url="https://sambaiga.github.io/sambaiga/images/ppl-pyro-intro.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Stochastic Variational Inference (SVI)</title><link href="https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html" rel="alternate" type="text/html" title="Stochastic Variational Inference (SVI)" /><published>2019-05-02T00:00:00-05:00</published><updated>2019-05-02T00:00:00-05:00</updated><id>https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi</id><content type="html" xml:base="https://sambaiga.github.io/sambaiga/jupyter/2019/05/02/svi.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-05-02-svi.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;a href=&quot;&quot;&gt;previous post&lt;/a&gt;  introduced the basic principle of Variational Inference (VI) as one of the approach used to approximate difficult probability distribution, derived the ELBO function and discussed about Mean Field Variational Inference (MFVI) and the Coordinate Ascent Variational Inference (CAVI) algorithms. This post introduce another  stochastic gradient based algorithm (SVI) used in practise to do VI under mean filed assumptions. It also present two important tricks re-parametrization trick and amortized inference that are useful when using SVI in solving problems.&lt;/p&gt;
&lt;h2 id=&quot;Stochastic-Variational-Inference-(SVI)&quot;&gt;Stochastic Variational Inference (SVI)&lt;a class=&quot;anchor-link&quot; href=&quot;#Stochastic-Variational-Inference-(SVI)&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Consider the graphical model of the observations $\mathbf{x}$ and latent variable $\mathbf{z}=\{\theta, z\}$ in figure 1 where $\theta$ is the global variable and $z = \{z_1, \ldots z_n\}$  is the local (per-data-point) variable such that:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/sambaiga/images/copied_from_nb/my_icons/VI.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
$$
p(\mathbf{x},\mathbf{z}) = p(\theta|\alpha)\prod_{i=1}^N p(x_i|z_i, \theta)\cdot p(z_i|\alpha)
$$&lt;p&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Similarly the variational parameters are given by $\lambda = \{\gamma, \phi\} $ where the variational parameter $\gamma$ correspond to latent variable  and  $\phi$ denote set of local variational parameters. The variational distribution $q(\mathbf{z}\mid \phi)$ is given by&lt;/p&gt;
$$
q(\mathbf{z}\mid \phi) = q(\theta|\gamma)\prod_{i=1}^N q(z_i|\phi_i, \alpha)
$$&lt;p&gt;which  also depend on hyper-parameter $\alpha$. The ELBO of this graphical model $\mathcal{L}_{VI}(q) = \mathbb{E}_q[ \log p(\mathbf{x},\mathbf{z}, \alpha) -\log q(\mathbf{z}, \gamma)]$ has the following form:&lt;/p&gt;
$$
\begin{split}
\mathcal{L}_{VI}(q) &amp;amp;= \mathbb{E}_q[ \log p(\theta|\alpha)- \log q(\theta|\gamma)] \\
&amp;amp;+ \sum_{i=1}^{N}\mathbb{E}_q[\log p(z_i|\theta) 
+ \log p(x_i|z_i, \theta)-\log q(z_i|\phi_i)]
\end{split}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The equation above could be optimized by CAVI algorithm discussed in previous post which is expensive for large data sets. The CAVI algorithm scales with $N$ as it require to optimize the local variational parameters for each data point before re-estimating the global variational parameters.&lt;/p&gt;
&lt;p&gt;Unlike CAI, SVI uses stochastic optimization to fit the global variational parameters by repeatedly sub-sample the data to form stochastic estimate of ELBO. In every iteration one randomly selects mini-batches of size $b_{sz}$  to obtain a stochastic estimate of ELBO.&lt;/p&gt;
$$
\begin{split}
\mathcal{L}_{VI}(q) &amp;amp;= \mathbb{E}_q[ \log p(\theta|\alpha)- \log q(\theta|\gamma)] \\
&amp;amp;+ \frac{N}{b_{sz}}\sum_{s=1}^{b_{sz}}\mathbb{E}_q[\log p(z_{i_s}|\theta) + \log p(x_{i_s}|z_{i_s}, \theta)-\log q(z_{i_s}|\phi_{i_s})]
\end{split}
$$&lt;p&gt;SVI algorithms follow noisy estimates of the gradient with a decreasing step size which is often  cheaper to compute than the true gradient. Following such noisy estimates allows SVI to escape  shallow local optima of complex objective functions.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Natural-Gradient-for-SVI&quot;&gt;Natural Gradient for SVI&lt;a class=&quot;anchor-link&quot; href=&quot;#Natural-Gradient-for-SVI&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;To solve the optimization problem standard gradient-based methods such as SGD, Adam or Adagrad can be used. However, for SVI these gradient based methods cause slow convergence or converge to inferior local models. This is because, gradient based methods use the following update&lt;/p&gt;
$$
\theta^{t+1}=\theta^t + \alpha \frac{\partial \mathcal{L}_{VI}(q)}{\partial \theta} 
$$&lt;p&gt;where&lt;/p&gt;
$$
\frac{\partial \mathcal{L}_{VI}(q)}{\partial \theta} =\frac{\partial \mathcal{L}_{VI}(q)}{\partial \theta_1}, \ldots \frac{\partial \mathcal{L}_{VI}(q)}{\partial \theta_k}
$$&lt;p&gt;&lt;/p&gt;
&lt;p&gt;is the the gradient vector which point in the direction where the function increases most quickly while the changes in the function are measured with respect to euclidean distance. As the result, if the euclidean distance between the variational parameter being optimized is not good measure of variation in objective function then gradient descent will move suboptimal through the parameter value.&lt;/p&gt;
&lt;p&gt;Consider the following  two set of gausian distributions $$\{d_{(1)}=\mathcal{N}(-2, 3), d_{(2)}=\mathcal{N}(2, 3)\}$$ and $$\{d_{(1)}=\mathcal{N}(-2, 1), d_{(2)}=\mathcal{N}(2, 1)\}$$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The euclidean distance between the two distributions $d_{}=\sqrt{(\mu_1-\mu_2)^2+ (\sigma^2_1-\sigma^2_2)^2}=4$ It clear that, considering only the euclidean distance the two images are the same. However, when we consider the shape of the distribution, the distance is different in the first and second image. In the first image, the KL-divergence should be lower as there is more overlap between between the two distribution unlike the second image where their support barely overlap. The reason for this difference is that probability distribution do not naturally fit in euclidean space rather it fit on a statistical manifold also called &lt;a href=&quot;https://en.wikipedia.org/wiki/Riemannian_manifold&quot;&gt;Riemannian manifold&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Statistical manifold give a natural way of measuring distances between distribution that euclidean distance use in SGD. A common Riemannian metric for statistical manifold is the &lt;a href=&quot;https://wiseodd.github.io/techblog/2018/03/11/fisher-information/&quot;&gt;fisher information matrix&lt;/a&gt; defined by&lt;/p&gt;
$$
F_{\lambda} = \mathbb{E}_{p(x;\lambda)}[\nabla \log p(x;\lambda) (\nabla \log p(x;\theta))^T ]
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;It can be showed that the fisher information matrix $F_{\lambda}$ is  the second derivative of the KL divergence between two distributions.&lt;/p&gt;
$$
F_{\theta} = \nabla^2_{\theta} KL(q(x;\lambda)||p(x;\theta))
$$&lt;p&gt;Thus for SVI, the standard gradients descent techniques can be replaced  with the natural gradient as follows:&lt;/p&gt;
$$
        \tilde{\nabla_{q}} \mathcal{L}(q) = F^{-1} \nabla{q}\mathcal{L}_{VI}(q)
$$&lt;p&gt;The update procedure for natural gradient can be summarized as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Compute the loss $\mathcal{L}_{VI}(q)$&lt;/li&gt;
&lt;li&gt;Compute the gradient of the loss $\nabla{q}\mathcal{L}_{VI}(q)$&lt;/li&gt;
&lt;li&gt;Compute the Fisher Information Matrix F.&lt;/li&gt;
&lt;li&gt;Compute the natural gradient $\tilde{\nabla_{q}} \mathcal{L}_{VI}(q)$&lt;/li&gt;
&lt;li&gt;Update the parameter $q^{t+1} =q^t - \alpha \tilde{\nabla_{\theta}}\mathcal{L}_{VI}(q)$&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Using natural gradient instead of standard gradients simplify SVI gradient update. However the same conditions for convergence as standard SDG have to be fulfilled. First, the mini-batch indices must be drawn uniformly at random size where the size $b_{sz}$ of the mini-batch must satisfies $1\leq b_{sz} \leq N$ The learning rate $\alpha$ needs to decrease with iterations $t$ satisying the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_approximation&quot;&gt;Robbins Monro conditions&lt;/a&gt; $\sum_{t=1}^{\infty} \alpha_t =\infty$ and $\sum_{t=1}^{\infty} \alpha_t^2 &amp;lt;\infty$ This guarantee that every point in the parameter space can be reached while the gradient noise decreases quickly enough to ensure convergence.&lt;/p&gt;
&lt;p&gt;The next section presents two important tricks namely re-parametrization trick and amortized inference that are useful when using SVI in solving problems.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Re-parametrization-trick&quot;&gt;Re-parametrization trick&lt;a class=&quot;anchor-link&quot; href=&quot;#Re-parametrization-trick&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Consider the graphical model presented in figure 1, where gradient based stochastic optimization is used to learn the variational parameter $\phi$. For example; for Gaussian distribution  $q_{\phi}(z|x)=\mathcal{N}(\mu_{\phi}(x), \Sigma_{\phi}(x))$&lt;/p&gt;
&lt;p&gt;To maximize the likelihood of the data, we need to back propagate the loss to the parameter $\phi$ across the distribution of $z$ or across sample $z\sim q_\phi(z \mid x) $ However, it is difficulty to back-propagate through random variable. To address this problem, the re-parametrization trick is used.&lt;/p&gt;
&lt;p&gt;First let consider the &lt;a href=&quot;https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician&quot;&gt;Law of the Unconscious Statistician (LOTUS)&lt;/a&gt;, that is used to calculate the expected value of a function $g(\epsilon)$ of a random variable $\epsilon$ when only the probability distribution $p(\epsilon)$ of $\epsilon$ is known. The Law state that:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;To compute the expectation of a measurable function $g(.)$ of a random variable $\epsilon$, we have to integrate $g(\epsilon)$ with respect to the distribution function of $\epsilon$, that is: $$\mathbb{E}(g(\epsilon)) = \int g(\epsilon)dF_{\epsilon}(\epsilon)
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, to compute the expectation of $z =g(\epsilon)$ we only need to know $g(.)$ and the distribution of $\epsilon$. We do not need to explicitly know the distribution of $z$. Thus the above equation can be expression in the convenient alternative notation:&lt;/p&gt;
$$
\mathbb{E}_{\epsilon \sim p(\epsilon)}(g(\epsilon)) = \mathbb{E}_{z \sim p(z)} (z)
$$&lt;p&gt;Therefore the reparameteriztaion trick states that:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A random variable $z$ with distribution $q_{\phi}(z, \phi)$ which is independent to $\phi$ can be expressed as transformation of random variable $\epsilon \sim p(\epsilon)$ that come from noise distribution such as uniform or gaussian such that $z = g(\phi, \epsilon)$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For instance for Gaussian variable $z$ in the above example&lt;/p&gt;
$$
z = \mu(\phi) + \sigma^2(\phi)\cdot \epsilon
$$&lt;p&gt;where $\epsilon \sim \mathcal{N}(0, 1)$. Since $p(\epsilon)$ is independent of the parameter of $q_{\phi}(z, \phi)$, we can apply the change of variables in integral theory to compute any expectation over $z$ or any expectation over  $\phi$. The SDG estimator can therefore be estimated by pulling the gradient into expectations and approximating it by samples from the noise 
distribution such that  for any measurable function $f_{\theta}(.)$: $$\Delta_{\phi}\mathbb{E}_{z\sim p_{\phi}(z)} = \frac{1}{M}\sum_{i=1}^M \Delta f(g(\phi, \epsilon_i)) 
$$&lt;/p&gt;
&lt;p&gt;where $\epsilon_i\sim p(\epsilon)$ , $f_{\theta}(.)$ must be differentiable w.r.t  its input $z$ and $g(\phi, \epsilon_i)$ must exist and be differentiable with respect to $\phi$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Amortized-Variational-Inference&quot;&gt;Amortized Variational Inference&lt;a class=&quot;anchor-link&quot; href=&quot;#Amortized-Variational-Inference&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Consider the graphical model presented in figure 1 where ecah data point $x_i$ is governed by its latent variable $z_i$ with variational parameter $phi_i$ such that&lt;/p&gt;
$$
q(\mathbf{z}\mid \phi) = q(\theta|\gamma)\prod_{i=1}^N q(z_i|\phi_i, \alpha)
$$&lt;p&gt;Using traditional SVI make it necessary to optimize $\phi_i$ for each data point $x_i$. As the results the number parameters to be optimized will grows with the number of observations $x$. This is not ideal for larger datasets. Apart from that, it requires one to re-run the optimization procedure in case of new observation or when we have to perform inference. To address these problem amortized VI introduce a parametrized function that maps from observation space to the parameter of the approximate posterior distribution.&lt;/p&gt;
&lt;p&gt;Amortized VI try to learn from past inference/pre-computation so that future inferences run faster. Instead of approximating separate variables for each data point $x_i$, amortized VI assume that the local variational parameter $\phi$ can be predicted by a parametrized function $f_{\phi}(.)$ of data whose parameters are shared across all data points. Thus instead of introducing local variational parameter, we learn a single parametric function and work with a variational distribution that has the form&lt;/p&gt;
$$
q(\mathbf{z}\mid \phi) = q(\theta|\gamma)\prod_{i=1}^N q(z_i|f_{\phi}(.))
$$&lt;p&gt;where $f_{\phi}(.)$ is the deep neural net function of $z$&lt;/p&gt;
&lt;p&gt;Deep neural network used in this context are called &lt;a href=&quot;&quot;&gt;inference networks&lt;/a&gt;. Therefore amortized inference with inference networks combines probabilistic modelling with representation power of deep learning. Using amortized VI instead of traditional VI, has two important advantages. First the number of variational parameters remain constant with respect to the data size. We only need to specify the parameter of the neural networks which is independent to the number of observations. Second, for new observation or during inference all we need to do is to call the inference network. As the result, we can invest time upfront optimizing the inference network and during inference we use the trained network for fast inference.&lt;/p&gt;
&lt;h2 id=&quot;Reference&quot;&gt;Reference&lt;a class=&quot;anchor-link&quot; href=&quot;#Reference&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.05597&quot;&gt;[Cheng Zhang,(2017)]&lt;/a&gt;:
Advances in Variational Inference.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.05735&quot;&gt;[Daniel Ritchie,(2016)]&lt;/a&gt;:Deep Amortized Inference for Probabilistic Programs.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.05735&quot;&gt;[Andrew Miller,(2016)]&lt;/a&gt;:Natural Gradients and Stochastic Variational Inference.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.shakirm.com/papers/VITutorial.pdf&quot;&gt;Shakir Mohamed&lt;/a&gt;:Variational Inference  for Machine Learning. &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://emtiyaz.github.io/teaching/ds3_2018/ds3.html&quot;&gt;DS3 workshop&lt;/a&gt;:Approximate Bayesian Inference: Old and New.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/philschulz/VITutorial&quot;&gt;Variational Inference and Deep Generative Models&lt;/a&gt;:Variational Inference for NLP audiences&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sambaiga.github.io/sambaiga/images/svi.jpg" /><media:content medium="image" url="https://sambaiga.github.io/sambaiga/images/svi.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Learning from probabilistic models</title><link href="https://sambaiga.github.io/sambaiga/jupyter/2018/04/02/probabilities-learning.html" rel="alternate" type="text/html" title="Learning from probabilistic models" /><published>2018-04-02T00:00:00-05:00</published><updated>2018-04-02T00:00:00-05:00</updated><id>https://sambaiga.github.io/sambaiga/jupyter/2018/04/02/probabilities-learning</id><content type="html" xml:base="https://sambaiga.github.io/sambaiga/jupyter/2018/04/02/probabilities-learning.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-04-02-probabilities-learning.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;Introduction&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Given some data $ \mathbf{x}=[x_1\ldots x_m]$  that come from some probability density function characterized by
an unknown parameter $ \theta$. How can we find $ \hat{\theta}$  that is the best estimator of $ \theta$. For example suppose we have flipped a particular coin $ 100$  times and landed head $ N_H = 55$  times and tails $ N_T = 45$  times. We are interested to know what is the probability that it will come-up head if we flip it again. In this case the behavior of the coin can be summerized with parameter  $ \theta$  the probability that a flip land head (H), which in this case is independent and identically distributed Bernoulli distribution. The key question is, how do we find parameter  $ \hat{\theta}$  of this distribution that fits the data. This is called parameter estimation, in which three approaches can be used:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Maximum-Likehood estimation&lt;/li&gt;
&lt;li&gt;Bayesian parameter estimation and &lt;/li&gt;
&lt;li&gt;Maximum a-posterior approximation&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;Like-hood-and-log-likehood-function&quot;&gt;Like-hood and log-likehood function&lt;a class=&quot;anchor-link&quot; href=&quot;#Like-hood-and-log-likehood-function&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Before discussing the above learning approach, let firts define the &lt;strong&gt;like-hood function&lt;/strong&gt; $L(\theta)$ which is the probability of the observed data as function of $\theta$ given as:&lt;/p&gt;
$$
L(\theta) = P(x_1,\ldots x_m; \theta) = \prod_i^m P(x_i;\theta)
$$&lt;p&gt;The like-hood function indicates how likely each value of the parameter is to have generated the data. In the case of coin example above, the like-lihood is the probability of particular seqeuence of H and T generated:&lt;/p&gt;
&lt;p&gt;$$
 L(\theta) = \theta ^{N_H}(1 - \theta ^{N_T})
 $$&lt;/p&gt;
&lt;p&gt;We also define the &lt;strong&gt;log-likelihood function&lt;/strong&gt; $\mathcal{L}(\theta)$ which is the log of the likelihood function $L(\theta)$.&lt;/p&gt;
$$
\begin{aligned}
\mathcal{L}(\theta) &amp;amp;= \log L(\theta) \\
 &amp;amp; = \log \prod_i^m P(x_i;\theta) \\
  &amp;amp; = \sum_i^M P(x_i;\theta)
\end{aligned}
$$&lt;p&gt;For the above coin example the log-likelihood is&lt;/p&gt;
$$ 
\mathcal{L}(\theta)= N_H\log\theta + N_T\log(1-\theta)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Maximum-Likelihood-Estimation&quot;&gt;Maximum-Likelihood Estimation&lt;a class=&quot;anchor-link&quot; href=&quot;#Maximum-Likelihood-Estimation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The main objective of maximum likelihood estimation (MLE) is to determine the value of $\theta$ that is most likely to have generated the vector of observed data, $\mathbf{x}$ where $\theta$ is assumed to be  fixed point (point-estimation). MLE achieve this by finding the parameter that maximize the probability of the observed data. The parameter $\hat{\theta}$ is selected such that it maximize $  \mathcal{L}(\theta)$:&lt;/p&gt;
&lt;p&gt;$
\hat{\theta}=\arg\max_{\theta} \mathcal{L}(\theta)
$&lt;/p&gt;
&lt;p&gt;For the coin example the MLE is :&lt;/p&gt;
$$
\begin{aligned}
\frac{\partial \mathcal{L}(\theta)}{\partial \theta} &amp;amp; = \frac{\partial }{\partial \theta}(N_H\log\theta + N_T\log(1-\theta) \\
 &amp;amp;= \frac{N_H}{\theta} - \frac{N_T}{1-\theta}
\end{aligned}
$$&lt;p&gt;Set $\frac{\partial \mathcal{L}(\theta)}{\partial \theta} = 0$  and  solve for $\theta$ we obtain the MLE:&lt;/p&gt;
&lt;p&gt;$
\hat{\theta} =\frac{N_H}{N_H + N_T}
$&lt;/p&gt;
&lt;p&gt;which is simply the fraction of flips that cameup head.&lt;/p&gt;
&lt;p&gt;Now suppose we are observing power-meta data which can be modelled as gaussian ditribution with mean $$\mu$$ and standard deviation $\sigma$. We can use MLE to estimate $\hat{\mu}$ and $\hat{\sigma}$. The log-likehood for gausian distribution is given as&lt;/p&gt;
$$
\begin{aligned}
\mathcal{L}(\theta) &amp;amp;= \sum_{i=1}^M \log \left[ \frac{1}{\sqrt{2} \pi \sigma} \exp \frac{-(x_i - \mu)}{2\sigma ^2}\right] \\
 &amp;amp; = -\frac{M}{2}\log 2\pi - M\log \sigma - \frac{1}{2\sigma^2} \sum_i^M (x_i - \mu)^2
\end{aligned}
$$&lt;p&gt;Let find $ \frac{\partial \mathcal{L}(\theta)}{\partial \mu} $ and $\frac{\partial \mathcal{L}(\theta)}{\partial \sigma} $ and set  equal to zero.&lt;/p&gt;
$$
\begin{aligned}
\frac{\partial \mathcal{L}(\theta)}{\partial \mu} &amp;amp;=  -\frac{1}{2\sigma^2} \sum_i^M \frac{\partial}{\partial \mu}(x_i - \mu)^2 \\
&amp;amp; = \sum_i^M (x_i - \mu) = 0 \\
&amp;amp;\Rightarrow \hat{\mu} = \frac{1}{M} \sum_{i=1}^M x_i
\end{aligned}
$$&lt;p&gt;which is the mean of the observed values. Similary:&lt;/p&gt;
$$
\begin{aligned}
\frac{\partial \mathcal{L}(\theta)}{\partial \sigma} &amp;amp;=  \frac{M}{\sigma} + \frac{1}{\sigma^3}\sum_i^M (x_i - \mu)^2 \\
&amp;amp;\Rightarrow \hat{\sigma} = \sqrt{\frac{1}{M} \sum_{i=1}^M (x_i - \mu)^2}
\end{aligned}
$$&lt;p&gt;In the two examples above  we manged to obtain the exact maximum likelihood solution analytically. But this is not always the case, let’s consider how to compute the maximum likelihood estimate of the parameters of the gamma distribution, whose PDF is defined as:&lt;/p&gt;
$$
P(x) = \frac{b^a}{\Gamma(a)}x^{x-1}\exp(-bx)
$$&lt;p&gt;where $\Gamma (a)$ is the gamma function which is the generalization of the factorial function to continous values given as:&lt;/p&gt;
&lt;p&gt;$
\Gamma(t) = \int_0^{-\infty} x^{t-1}\exp(-x) \,dx
$&lt;/p&gt;
&lt;p&gt;The model parameters for gamma distribution is $a$ and $b$ both of which are $\geq 0$. the log-likelihood is therefore:&lt;/p&gt;
&lt;p&gt;$
\begin{aligned} \mathcal{L}( a, b) &amp;amp; = \sum_{i=1}^M a\log b -\log \Gamma (a) + (a -1) \log x_i - bx_i \\
 &amp;amp; = Ma\log b - M \log \Gamma (a) + (a - 1) \sum_{i=1}^M \log x_i - b \sum_{i=1}^M x_i
\end{aligned}
$&lt;/p&gt;
&lt;p&gt;To get MLE we need employ gradient descent which consists of computing the derivatives:
$
\frac{\partial \mathcal{L}}{\partial a}
$ and $
\frac{\partial \mathcal{L}}{\partial b}
$ and then updating; $
a_{k+1}= a_k + \alpha \frac{\partial \mathcal{L}}{\partial a}
$
and 
$
b_{k+1}= b_k + \alpha \frac{\partial \mathcal{L}}{\partial b}
$&lt;/p&gt;
&lt;p&gt;where $\alpha$ is the learning rate.&lt;/p&gt;
&lt;h3 id=&quot;Limitation-of-MLE&quot;&gt;Limitation of MLE&lt;a class=&quot;anchor-link&quot; href=&quot;#Limitation-of-MLE&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Despite the fact that MLE is very powerful technique, it has a pitfall for little training data which can lead into seriously overfit. The most painful issue is when it assign a $0$ probability to items that were never seen in the training data but which still might actually happen. Take an example if we flipped  a coin twice and $N_H = 2$, the MLE of $\theta$, the probability of H would be $1$. This imply that we are considering it impossible for the coin to come up T. This problem is knowas &lt;em&gt;data sparsity&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Bayesian-Parameter-Estimation&quot;&gt;Bayesian Parameter Estimation&lt;a class=&quot;anchor-link&quot; href=&quot;#Bayesian-Parameter-Estimation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Unlike MLE which treat only the observation $\mathbf{x}$ as random variable and the parameter $\theta$ as a fixed point, the bayesian approach treat the parameter $\theta $ as random varibale as well with some known prior distribution. Let define the model for joint distribution $$p(\theta, \mathcal{D})$$ over parameter  $\theta$ and data $\mathcal{D}$. To further define this joint distribution we aslo need the following two distribution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A distribution of $P(\theta)$ knowas &lt;strong&gt;prior distribution&lt;/strong&gt; which is the probability of paratemeter $\theta$ availabe beforehand, and before making any additional observations. It account for everything you believed about the parameter $\theta$ before observing the data. In practise choose prior that is computational convinient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;likelihood&lt;/strong&gt; $P(\mathcal{D}\mid \theta)$ which is the probability of data given the parameter like in maximum likelihood.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this two distributions, we can compute the posterior distribution and the posterior predictive distribution. The posterior distribution $P(\theta \mid \mathcal{D})$ which correspond to uncertainty about $\theta$ after observing the data given by:&lt;/p&gt;
$$
\begin{aligned}
P(\theta  \mid  \mathcal{D}) &amp;amp;= \frac{P(\theta)p(\mathcal{D}  \mid  \theta)}{P(\mathcal{D})} &amp;amp;= \frac{P(\theta)P(\mathcal{D} \mid  \theta)}{ \displaystyle \int P(\theta ^ {\prime} ) P(\mathcal{D} \mid  \theta ^{\prime})}
\end{aligned} 
$$&lt;p&gt;The denominator is usually considered as a normalizing constant and thus the posterior distribution become:&lt;/p&gt;
$$
P(\theta  \mid  \mathcal{D}) \propto P(\theta)P(\mathcal{D}  \mid \theta)
$$&lt;p&gt;On the other hand the posterior predictive distribution $P(\mathcal{D}^{\prime}\mid)\mathcal{D}$ is the distribution of future observation given past observation defined by:&lt;/p&gt;
$$
P(\mathcal{D}^{\prime} \mid \mathcal{D} )= \int P(\theta\mid \mathcal{D}) P(\mathcal{D}^{\prime} \mid \theta)
$$&lt;p&gt;Generaly the Bayesian approach to parameter estimation works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First we need to formulate our knowledge about a situation by defining a distribution model which expresses qualitative aspects of our knowledge about the situation and then specify a prior probability distribution which expresses our subjective
beliefs and subjective uncertainty about the unknown parameters, before seeing the data.&lt;/li&gt;
&lt;li&gt;Gather data&lt;/li&gt;
&lt;li&gt;Obtain posterior knowledge that updates our beliefs by computing the posterior probability distribution which estimates the unknown
parameters. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let apply the bayesian estimation to the coin example in which we have specified the likelihood equal to $\theta^{N_H}(1-\theta)^{N_T}$. We only required to specify the prior in which several approches can be used. One of the approach is relay upon lifetime experince of flipping coins in which most coins tend to be fair which implies $p(\theta) = 0.5$. We can also use various distribution to specify prior density but in practise a most useful distribution is the &lt;strong&gt;beta distribution&lt;/strong&gt; parameterized by $a , b &amp;gt; 0$ and defined as:&lt;/p&gt;
$$
p(\theta; a, b) = \frac{\Gamma (a + b)}{\Gamma(a) \Gamma (b)} \theta ^{a-1}(1-\theta ^{b - 1})
$$&lt;p&gt;From the above eqution it is clear that the first term (with all $\Gamma$)is just a normalizing constant and thus we can rewrite the beta distribution as:&lt;/p&gt;
$$
p(\theta; a, b) \propto \theta ^{a-1}(1-\theta) ^{b - 1}
$$&lt;p&gt;Note the beta distribution has the following properties&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is centered around $\frac{a}{a + b}$ and it can be shown that if $\theta \sim \text{Beta}(a,b)$ then $\mathbb{E}(\theta)=\frac{a}{a + b}$.&lt;/li&gt;
&lt;li&gt;It becomes more peaked for larger values of $a$ and $b$&lt;/li&gt;
&lt;li&gt;It become normal distribution when $a = b = 1$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let compute the posterior and posterior predictive distribution&lt;/p&gt;
$$
\begin{aligned}
p(\theta | \mathcal{D}) &amp;amp; \propto p(\theta)p(\mathcal{D} |\theta) \\
&amp;amp; \propto \theta^{N_H}(1-\theta)^{N_T}\theta ^{a-1}(1-\theta) ^{b - 1} \\
&amp;amp; = \theta ^{a-1+N_H}(1-\theta) ^{b - 1 + N_T}
\end{aligned}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sambaiga.github.io/sambaiga/images/pmf.png" /><media:content medium="image" url="https://sambaiga.github.io/sambaiga/images/pmf.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Basics of Probability and Information Theory</title><link href="https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html" rel="alternate" type="text/html" title="Basics of Probability and Information Theory" /><published>2018-02-01T00:00:00-06:00</published><updated>2018-02-01T00:00:00-06:00</updated><id>https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities</id><content type="html" xml:base="https://sambaiga.github.io/sambaiga/jupyter/2018/02/01/probabilities.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-02-01-probabilities.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;Introduction&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Probability and Information theory are important field that has made significant contribution to deep learning and AI. Probability theory allows us to make &lt;em&gt;uncertain statements&lt;/em&gt; and to &lt;em&gt;reason&lt;/em&gt; in the presence of &lt;em&gt;uncertainty&lt;/em&gt; where information theory enables us to &lt;em&gt;quantify&lt;/em&gt; the amount of &lt;em&gt;uncertainty&lt;/em&gt; in a probability distribution.&lt;/p&gt;
&lt;h2 id=&quot;Probability-Theory&quot;&gt;Probability Theory&lt;a class=&quot;anchor-link&quot; href=&quot;#Probability-Theory&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Probability is a mathematical framework for representing uncertainty. It is very applicable in Machine learning and Artificial Intelligence as it allows to make &lt;em&gt;uncertain statements&lt;/em&gt; and  &lt;em&gt;reason&lt;/em&gt; in the presence of &lt;em&gt;uncertainty&lt;/em&gt;. Probability theory allow us to design ML algorithms that take into consideration  of &lt;em&gt;uncertain&lt;/em&gt; and sometimes &lt;em&gt;stochastic&lt;/em&gt;  quantities. It further tell us tell us how ML systems should &lt;em&gt;reason&lt;/em&gt; in the presence of uncertainty. This  is necessary because most things in the world  are uncertain, and thus  ML systems should reason using probabilistic rules. Probability theory can also be used to analyse the behaviour of  ML algorithms probabilistically. Consider evaluating ML classification algorithm using  accuracy metric which is  the probability that the model will give a correct prediction  given an example.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Probability-and-Probability-distribution&quot;&gt;Probability and Probability distribution&lt;a class=&quot;anchor-link&quot; href=&quot;#Probability-and-Probability-distribution&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Probability&lt;/strong&gt; is a measure of the likelihood that an event will occur in a random experiment. It is quantified as number between 0 and 1. The mathematical function that maps all possible outcome of a random experiment with its associated probability it is called &lt;strong&gt;probability distribution&lt;/strong&gt;. It describe how likely a random variable or set of random variable is to take on each of its possible state. The probability distribution for discrete random variable is called probability mass function (PMF) which measures the probability $X$ takes on the value $x$, denoted denoted as $P(X=x)$.
To be PMF on random variable $X$  a function $P(X)$ must satisfy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Domain of $P$ equal to all possible states of $X$&lt;/li&gt;
&lt;li&gt;$\forall x \in X, 0\leq P(X=x) \leq 1$&lt;/li&gt;
&lt;li&gt;$\sum_{x \in X} P(x) =1$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Popular and useful PMF includes poison, binomial, bernouli, and uniform. Let consider a poison  distribution defined as:&lt;/p&gt;
$$
P(X=x) = \frac{\lambda ^x e^{ -\lambda}}{x!}
$$&lt;p&gt;$\lambda &amp;gt;0$ is called a parameter of the distribution, and it controls the distribution's shape. By increasing $\lambda$ , we add more probability to larger values, and conversely by decreasing $\lambda$  we add more probability to smaller values as shown in figure below.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Instead of a PMF, a continuous random variable has a probability density function (pdf) denoted as $f_X(x)$. An example of continuous random variable is a random variable with exponential density. 
$$
f_X(x\mid \lambda) = \lambda ^x e^{ -\lambda} \text{,  } x \geq 0
$$&lt;/p&gt;
&lt;p&gt;To be a probability density function $p(x)$ must satisfy&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The domain of $p$ must be the set of all possible state&lt;/li&gt;
&lt;li&gt;$\forall x \in X, f_X(x) \geq 0$&lt;/li&gt;
&lt;li&gt;$\int_{x \in X} f_X(x)dx =1$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The pdf does not give the probability of a specific state directly. The probability that $x$ is between two point $a, b$ is&lt;/p&gt;
&lt;p&gt;$\int_{a}^b f_X(x)dx$&lt;/p&gt;
&lt;p&gt;The probability of intersection of two or more random variables is called &lt;em&gt;joint probability&lt;/em&gt; denoted  as $
P(X, Y)
$&lt;/p&gt;
&lt;p&gt;Suppose we have two random variable $X$ and $Y$ and we know the joint PMF or pdf distribution between these variable. The PMF or pdf  corresponding to a single variable is called &lt;em&gt;marginal probability distribution&lt;/em&gt; defined as
$$
P(x) = \sum_{y\in Y} P(x, y)
$$&lt;/p&gt;
&lt;p&gt;for discrete random variable and 
$$
p(x) = \int p(x)dy
$$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Marginalization allows us to get the distribution of variable $$X$$ ignoring variable $Y$ from the joint distribution $P(X,Y)$. The probability that some event will occur given we know other events is called condition probability denoted as $P(X\mid Y)$. The  marginal, joint and conditional probability are linked by the following rule 
$
P(X|Y) = \frac{P(X, Y)}{P(Y)}
$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Independence, Conditional Independence and Chain Rule&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Two random variables are said to be independent of each other if the probability that one random variables occur in no way affect the probability of the other random variable occurring. $X$ and $Y$ are said to be independent if $P(X,Y) = P(X)\cdot P(Y)$
On the other hand two random variable $X$ and $Y$ are conditionally independent given an event $Z$ with $P(Z)&amp;gt;0$ if&lt;/p&gt;
$$
P(X,Y\mid Z) = P(X\mid Y)\cdot P(Y\mid Z)
$$&lt;p&gt;The good example of conditional independence can be found on this &lt;a href=&quot;&quot;&gt;link&lt;/a&gt;. Any joint probability distribution over many random variables may be decomposed into conditional distributions using &lt;em&gt;chain rule&lt;/em&gt; as follows:&lt;/p&gt;
$$
P(X_1,X_2, \ldots, X_n ) = P(X_1)\prod_{i=2}^n P(X_i\mid X_i, \ldots X_{i-1})
$$&lt;p&gt;&lt;strong&gt;Expectation, Variance and Covariance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Expected value of some function $f(x)$ with respect to a probability distribution $P(X)$ is the average or mean value that $f(x)$ takes on when $x$ is drawn from $P$.&lt;/p&gt;
$$
\mathbb{E}_{x\sim P}[f(x)] = \sum P(x).f(x)
$$&lt;p&gt;for discrete random variable and&lt;/p&gt;
$$
\mathbb{E}_{x\sim P}[f(x)] = \int P(x).f(x)dx
$$&lt;p&gt;Expectation are linear such that 
$$
\mathbb{E}_{x\sim P}[\alpha \cdot f(x) + \beta \cdot g(x)] = \alpha \mathbb{E}_{x\sim P}[f(x)] + \beta \mathbb{E}_{x\sim P}[g(x)]
$$&lt;/p&gt;
&lt;p&gt;Variance is a measure of how much the value of a function of random variable $X$ vary as we sample different value of $x$ from its probability distribution.
$$
Var(f(x)) =\mathbb{E}([f(x)-\mathbb{E}[f(x)]^2])
$$
The square root of the variance is know as standard deviation. On the other hand the covarince give some sense of how much two value are linearly related to each other as well as the scale of these value.&lt;/p&gt;
$$
Cov(f(x), g(y)) = \mathbb{E}[(f(x)- \mathbb{E}[f(x)])(g(y)- \mathbb{E}[g(y)])]
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Information-theory&quot;&gt;Information theory&lt;a class=&quot;anchor-link&quot; href=&quot;#Information-theory&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Information theory deals with quantification of how much information is present in a signal. In context of machine learning, information theory we apply information theory to: &lt;em&gt;characterize probability distributions&lt;/em&gt; and &lt;em&gt;quantify similarities between probability distributions&lt;/em&gt;. The following are the key information concepts and their application to machine learning.&lt;/p&gt;
&lt;h3 id=&quot;Entropy,-Cross-Entropy-and-Mutual-information&quot;&gt;Entropy, Cross Entropy and Mutual information&lt;a class=&quot;anchor-link&quot; href=&quot;#Entropy,-Cross-Entropy-and-Mutual-information&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Entropy give measure of uncertainty in a random experiment. It help us  quantify the amount of uncertainty in an entire probability distribution. The entropy of a probability distribution is the expected amount of information in an event drawn from that distribution defined as.&lt;/p&gt;
$$
H(X) = -\mathbb{E}_{x \sim P}[\log P(x)] = -\sum_{i=1}^n P(x_i)l\log P(x_i)
$$&lt;p&gt;Entropy is widely used in model selection based on principle of maximum entropy. On the other hand, cross entropy is used to compare two probability distribution. It tell how similar two distribution are. The cross entropy between two probability distribution $P$ and $$Q$ defined over same set of outcome is given by&lt;/p&gt;
$$
H(P,Q)= -\sum P(x)\log Q(x)
$$&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Cross entropy loss function is widely used in machine learning for classification problem. The mutual information over two random variables help us gain insight about the information that one random variable carries about the other.&lt;/p&gt;
$$
\begin{aligned}
I(X, Y) &amp;amp;= \sum P(x, y)\log \frac{P(x,y)}{P(x).P(y)}\\
        &amp;amp;=H(X)- H(X\mid Y) = H(Y) - H(Y\mid X)
\end{aligned}
$$&lt;p&gt;From above equation the mutual information  give insight about how far $X$ and $Y$ from being independent from each other. Mutual information can be used in feature selection instead of correlation as it capture both linear and non linear dependency.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Kullback-leibler-Divergence&quot;&gt;Kullback-leibler Divergence&lt;a class=&quot;anchor-link&quot; href=&quot;#Kullback-leibler-Divergence&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Kullback-leibler Divergence&lt;/strong&gt; measure how one probability distribution diverge from the other. Given two probability distribution $P(x)$ and $Q(X)$ where the former is the modelled/estimated distribution and the later is the actual/expected distribution. The &lt;strong&gt;KL&lt;/strong&gt; divergence is defined as&lt;/p&gt;
$$
\begin{aligned}
D_{KL}(P||Q) &amp;amp; = \mathbb{E}_{x \sim P} [\log \frac{P(x)}{Q(x)}]\\
             &amp;amp; = \mathbb{E}_{x \sim P}[\log P(x)] - \mathbb{E}_{x \sim P}[\log Q(x)]
\end{aligned}
$$&lt;p&gt;For discrete random distribution&lt;/p&gt;
$$
D_{KL}(P||Q) = \sum_{i} P(x_i)\log \frac{P(x_i)}{Q(x_i)}
$$&lt;p&gt;And for continuous random variable&lt;/p&gt;
$$
D_{KL}(p||q) = \int_{x} p(x) \log \frac{p(x)}{q(x)}
$$&lt;p&gt;KL divergence between $P$ and $Q$ tells how much information we lose when trying to approximate data given by $P$ with $Q$. It is non-negative $D_{KL}(P\mid \mid Q) \geq 0$ and  $0$ if $P$ and $Q$ are the same (distribution discrete) or equal almost anywhere in the case of continuous distribution. Apart from that KL divergence is not symmetric $D_{KL}(P\mid \mid Q) \neq D_{KL}(P\mid \mid Q)$ because of this it is not a true distance measure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relation between KL divergence and Cross Entropy&lt;/strong&gt;&lt;/p&gt;
$$
\begin{aligned}
D_{KL}(P||Q) &amp;amp; = \mathbb{E}_{x \sim P} [\log \frac{P(x)}{Q(x)}]\\
             &amp;amp; = \mathbb{E}_{x \sim P}[\log P(x)] - \mathbb{E}_{x \sim P}[\log Q(x)]\\
             &amp;amp; = H(P) - H(P, Q)\\
\end{aligned}
$$&lt;p&gt;where $\mathbb{E}_{x \sim P}[\log P(x)] = H(P)$$ and $$\mathbb{E}_{x \sim P}[\log Q(x)] = H(P, Q)$. Thus  $H(P,Q) = H(P) - D_{KL}(P||Q)$. This implies that minimizing cross entropy with respect to $Q$ is equivalent to minimizing the KL divergence. KL divergence is used in unsupervised machine learning technique like variational auto-encoder. The KL divergence is also used  as objective function in variational bayesian method to find optimal value for approximating distribution.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sambaiga.github.io/sambaiga/images/probabilities.jpg" /><media:content medium="image" url="https://sambaiga.github.io/sambaiga/images/probabilities.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>